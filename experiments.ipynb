{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from RAG.librarian import Librarian \n",
    "\n",
    "librarian = Librarian(librarian_LLM_model = \"GEMINI\")\n",
    "\n",
    "# SELECT SPECIALIST DATABASE\n",
    "librarian.select_specialist(specialist = \"traveller\", specialist_LLM_model = \"GEMINI\", )\n",
    "\n",
    "# Ask librarian to get acquinted with the specialist database\n",
    "librarian.Traveller.load_data_model(reembed = False,\n",
    "                                    embed_id = 0,\n",
    "                                    data_model_keys = {\"TEST - CLIENT\":\"CLIENT ID\",\n",
    "                                                        \"TEST - CLIENT REQUEST\":\"CLIENT ID\",\n",
    "                                                        \"TEST - FLIGHTS\":\"FLIGHT ID\",\n",
    "                                                        \"TEST - ACCOMODATIONS\":\"ACCOMODATION ID\",\n",
    "                                                        \"TEST - ACTIVITIES\":\"ACTIVITY ID\",\n",
    "                                                        \"TEST - SERVICES\":\"SERVICE ID\",\n",
    "                                                        },\n",
    "                                    reembed_table = {\"TEST - CLIENT\":True,\n",
    "                                                    \"TEST - CLIENT REQUEST\":True,\n",
    "                                                    \"TEST - FLIGHTS\":True,\n",
    "                                                    \"TEST - ACCOMODATIONS\":True,\n",
    "                                                    \"TEST - ACTIVITIES\":True,\n",
    "                                                    \"TEST - SERVICES\":True,\n",
    "                                                    }\n",
    "                                                    \n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _*Generate*_ : travel package from customer/agent prompt  + inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token size of the prompt for cl100k_base ~ 3745\n",
      "**Summary**\n",
      "\n",
      "Immerse yourself in the beauty of Bali on this 4-day beach getaway designed to soothe your soul and rejuvenate your senses. Dive into the pristine waters, bask in the warm sun, indulge in luxurious spa treatments, and explore the island's stunning natural wonders. This trip_recommendation offers a perfect blend of relaxation, adventure, and cultural experiences, ensuring an unforgettable escape to paradise.\n",
      "\n",
      "**Journey Highlights**\n",
      "\n",
      "* Chill out on the white-sand beaches of Bali\n",
      "* Pamper yourself with a rejuvenating spa treatment\n",
      "* Embark on a scenic hiking and cliff-hunting tour\n",
      "* Savor delicious meals at local restaurants\n",
      "* Capture the beauty of Bali's rice fields\n",
      "* Your journey takes you to: Kuala Lumpur - Denpasar Bali - Kuala Lumpur\n",
      "\n",
      "**Itinerary**\n",
      "\n",
      "**Day 1**\n",
      "\n",
      "* 0900hrs: Depart from Kuala Lumpur International Airport (FLIGHT ID: 1)\n",
      "* 1100hrs: Arrive at Denpasar Bali Airport (DPS)\n",
      "* 1200hrs: Transfer to Hard Rock Hotel Bali (ACCOMODATION ID: 4)\n",
      "* 1400hrs: Check in and relax at the hotel\n",
      "* 1600hrs: Spend the afternoon at Kuta Beach, enjoying the sun and sand\n",
      "* 1900hrs: Dinner at Jimbaran Bay (SERVICE ID: 2)\n",
      "\n",
      "**Day 2**\n",
      "\n",
      "* 0800hrs: Breakfast at the hotel\n",
      "* 0900hrs: Embark on a Bali Beach and scenic cliff hunting tour (ACTIVITY ID: 5)\n",
      "* 1500hrs: Lunch at a local restaurant\n",
      "* 1700hrs: Return to the hotel and relax\n",
      "* 1900hrs: Dinner at the hotel restaurant\n",
      "\n",
      "**Day 3**\n",
      "\n",
      "* 0800hrs: Breakfast at the hotel\n",
      "* 0900hrs: Visit Ubud rice fields group tours and activities (ACTIVITY ID: 6)\n",
      "* 1200hrs: Lunch at a local restaurant\n",
      "* 1400hrs: Enjoy a rejuvenating spa treatment at a local spa (ACTIVITY ID: 1)\n",
      "* 1900hrs: Farewell dinner at a traditional Balinese restaurant\n",
      "\n",
      "**Day 4**\n",
      "\n",
      "* 0800hrs: Breakfast at the hotel\n",
      "* 1000hrs: Check out of the hotel and depart for Denpasar Bali Airport (DPS)\n",
      "* 1200hrs: Depart for Kuala Lumpur International Airport (FLIGHT ID: 19)\n",
      "\n",
      "**Highlights and Inclusions**\n",
      "\n",
      "* Round-trip flights from Kuala Lumpur to Bali\n",
      "* 3 nights' accommodation at Hard Rock Hotel Bali\n",
      "* Breakfast daily\n",
      "* Bali Beach and scenic cliff hunting tour\n",
      "* Ubud rice fields group tours and activities\n",
      "* Spa treatment\n",
      "* Airport transfers\n",
      "* Services of a local tour guide and translator\n",
      "* Fairprice Travel Insurance (SERVICE ID: 35)\n",
      "\n",
      "**Pricing**\n",
      "\n",
      "* Flights (FLIGHT ID: 1, 19): $257 x 2 = $514\n",
      "* Accommodation (ACCOMODATION ID: 4): $400 x 3 = $1200\n",
      "* Bali Beach and scenic cliff hunting tour (ACTIVITY ID: 5): $4500 x 2 = $9000\n",
      "* Ubud rice fields group tours and activities (ACTIVITY ID: 6): $300 x 2 = $600\n",
      "* Spa treatment (ACTIVITY ID: 1): $62 x 2 = $124\n",
      "* Airport transfers (SERVICE ID: 2): $400 x 2 = $800\n",
      "* Tour guide and translator (SERVICE ID: 1): $100 x 3 = $300\n",
      "* Fairprice Travel Insurance (SERVICE ID: 35): $50 x 2 = $100\n",
      "\n",
      "**Total Cost: $12,988**\n"
     ]
    }
   ],
   "source": [
    "# Ask Traveller to generate a travel package\n",
    "initial_query = \"Hafeez from Kuala Lumpur wants to go Bali for 4 days for 2 pax. Budget: $2000. I want to chill at the beach\"\n",
    "\n",
    "convo_package = librarian.Traveller.III_generate_travel_package(initial_query = initial_query,\n",
    "                                                                 topN = 6, \n",
    "                                                                 model_name = \"gemini-pro\",\n",
    "                                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regenerate : travel package from followup prompt + inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token size of the prompt for cl100k_base ~ 3857\n",
      "**Refined Travel Package**\n",
      "\n",
      "**Summary**\n",
      "\n",
      "Immerse yourself in the beauty of Bali on this tailored 4-day beach getaway designed to soothe your soul and rejuvenate your senses. Dive into the pristine waters, bask in the warm sun, indulge in luxurious spa treatments, and explore the island's stunning natural wonders. This trip offers a perfect blend of relaxation, adventure, and cultural experiences, ensuring an unforgettable escape to paradise.\n",
      "\n",
      "**Journey Highlights**\n",
      "\n",
      "* Chill out on the white-sand beaches of Bali\n",
      "* Pamper yourself with a rejuvenating spa treatment\n",
      "* Embark on a scenic hiking and cliff-hunting tour\n",
      "* Savor delicious meals at local restaurants\n",
      "* Capture the beauty of Bali's rice fields\n",
      "* Your journey takes you to: Kuala Lumpur - Denpasar Bali - Kuala Lumpur\n",
      "\n",
      "**Itinerary**\n",
      "\n",
      "**Day 1**\n",
      "\n",
      "* 0900hrs: Depart from Kuala Lumpur International Airport (FLIGHT ID: 1)\n",
      "* 1100hrs: Arrive at Denpasar Bali Airport (DPS)\n",
      "* 1200hrs: Transfer to Hard Rock Hotel Bali (ACCOMODATION ID: 4)\n",
      "* 1400hrs: Check in and relax at the hotel\n",
      "* 1600hrs: Spend the afternoon at Kuta Beach, enjoying the sun and sand\n",
      "* 1900hrs: Dinner at Jimbaran Bay (SERVICE ID: 2)\n",
      "\n",
      "**Day 2**\n",
      "\n",
      "* 0800hrs: Breakfast at the hotel\n",
      "* 0900hrs: Embark on a Bali Beach and cliff hiking tour (ACTIVITY ID: 5)\n",
      "* 1500hrs: Lunch at a local restaurant\n",
      "* 1700hrs: Return to the hotel and relax\n",
      "* 1900hrs: Dinner at the hotel restaurant\n",
      "\n",
      "**Day 3**\n",
      "\n",
      "* 0800hrs: Breakfast at the hotel\n",
      "* 0900hrs: Visit Ubud rice fields group tours and activities (ACTIVITY ID: 6)\n",
      "* 1200hrs: Lunch at a local restaurant\n",
      "* 1400hrs: Enjoy a rejuvenating spa treatment at a local spa (ACTIVITY ID: 1)\n",
      "* 1900hrs: Farewell dinner at a traditional Balinese restaurant\n",
      "\n",
      "**Day 4**\n",
      "\n",
      "* 0800hrs: Breakfast at the hotel\n",
      "* 1000hrs: Check out of the hotel and depart for Denpasar Bali Airport (DPS)\n",
      "* 1200hrs: Depart for Kuala Lumpur International Airport (FLIGHT ID: 19)\n",
      "\n",
      "**Highlights and Inclusions**\n",
      "\n",
      "* Round-trip flights from Kuala Lumpur to Bali\n",
      "* 3 nights' accommodation at Hard Rock Hotel Bali\n",
      "* Breakfast daily\n",
      "* Bali Beach and cliff hiking tour (ACTIVITY ID: 5)\n",
      "* Ubud rice fields group tours and activities (ACTIVITY ID: 6)\n",
      "* Spa treatment (ACTIVITY ID: 1)\n",
      "* Airport transfers\n",
      "\n",
      "**Pricing**\n",
      "\n",
      "* Flights (FLIGHT ID: 1, 19): $257 x 2 = $514\n",
      "* Accommodation (ACCOMODATION ID: 4): $400 x 3 = $1200\n",
      "* Bali Beach and cliff hiking tour (ACTIVITY ID: 5): $4500 x 2 = $9000\n",
      "* Ubud rice fields group tours and activities (ACTIVITY ID: 6): $300 x 2 = $600\n",
      "* Spa treatment (ACTIVITY ID: 1): $62 x 2 = $124\n",
      "* Airport transfers (SERVICE ID: 2): $400 x 2 = $800\n",
      "\n",
      "**Total Cost: $11,638**\n",
      "**Revised Pricing:**\n",
      "* Flights (FLIGHT ID: 1, 19): $257 x 2 = $514\n",
      "* Accommodation (ACCOMODATION ID: 4): $400 x 3 = $1200\n",
      "* Bali Beach and cliff hiking tour (ACTIVITY ID: 4): $200 x 2 = $400\n",
      "* Ubud rice fields group tours and activities (ACTIVITY ID: 6): $300 x 2 = $600\n",
      "* Spa treatment (ACTIVITY ID: 1): $62 x 2 = $124\n",
      "* Airport transfers (SERVICE ID: 2): $400 x 2 = $800\n",
      "**Revised Total Cost: $3038**\n"
     ]
    }
   ],
   "source": [
    "followup_query = \"Give me the cheapest bali activities instead of that 4.5K hunting tour\"\n",
    "\n",
    "convo_package = librarian.Traveller.III_generate_travel_package(initial_query = \"\",\n",
    "                                                                followup_query = followup_query,\n",
    "                                                                 topN = 6, \n",
    "                                                                 model_name = \"gemini-pro\",\n",
    "                                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR : extract text from image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) EasyOCR(image: jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text: exil Ieo @ su-nig {6v.m4 One- no } | 03 2114 5414 Ajeat 54nia Trave( \"V^+ I(tuc SMART Travel Agency :Sedunia Travel Services WeRLD Package Kuala Lumpur Tour Date 05 January 2024 Kuala Lumpur Tour No. of Pax 04 pax (02 adult & 02 child) Date 05 January 2024 11.OOAM 7 OOPM Full Day (8 Hours) Time Pick up Alila Hotel in Bangsar Wilayah Persekutuan Kuala Lumpur Location 58, Jalan Ang Seng, Brickfields, 50470 Kualo Lumpur, Drop off Alila Hotel in Bangsar Wilayah Persekutuan Kuala Lumpur Location 58, Jalan Ang Seng, Brickfields, 50470 Kuala Lumpur, Type of Mini Van Vehicle Tour Guide English Speaking Entrance Fee Not included Meals Adult (MYR 50.00) Child (MYR 25.00) etarian Meal for Indian family (MYR) MYR 550.00 (transport/tour) MYR 300 (Tour guide) MYR 300 (meals) MYR3OO Price (markup) Highlight Gaze at the beautiful city of Kuala Lumpur from the Observation Deck of the Petronas Twin Towers Climb Batu Caves to explore Southeast Asia'$ most visited Hindu Temple Capture your memories with your camera at all these major landmarks_ Shop in Central Market, one of the oldest markets in Kuala Lumpur Places To Petronas Twin Towers Visit Menara Kuala Lumpur Petaling Street (shopping) Istana Negara (the King' Palace) Old Train Station Negara (National Monument) Batu Cave Restaurant (Lunch and Dinner) Budget per pax: Adult (MYR 50.00) Child (MYR 25.00) \"Vegetarian Meal for Indian family = TOTAL 1,450.00 Price for 4 pax 04\" dc{ tvp Day Day \"Vege Tugu\n",
      "Average Probability: 0.7584124356817936\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "\n",
    "def extract_text_from_image(image_file):\n",
    "  \"\"\"\n",
    "  Extracts text and calculates average probability from an image using EasyOCR.\n",
    "\n",
    "  Args:\n",
    "      image_file: Path to the image file.\n",
    "\n",
    "  Returns:\n",
    "      A tuple containing:\n",
    "          * Extracted text as a single string concatenated from all blocks.\n",
    "          * Raw output from EasyOCR (list of tuples with text, probability, and bounding box).\n",
    "          * Average probability of all text blocks.\n",
    "  \"\"\"\n",
    "\n",
    "  # Create a reader for the languages you want to support\n",
    "  reader = easyocr.Reader(['en'])\n",
    "\n",
    "  # Use the reader to read the text from the image\n",
    "  raw = reader.readtext(image_file)\n",
    "\n",
    "  # Extract text and probabilities\n",
    "  text = ' '.join([block[1] for block in raw])\n",
    "  probabilities = [block[2] for block in raw]\n",
    "\n",
    "  # Calculate average probability (handling potential division by zero)\n",
    "  average_probability = sum(probabilities) / len(probabilities) if probabilities else 0.0\n",
    "\n",
    "  return text, raw, average_probability\n",
    "\n",
    "# Example usage\n",
    "# image_file = './database/travel/Ingest/Cust_req_1.jpg'\n",
    "image_path = './database/travel/Ingest/TP_Cxxx_20240105_KL1.jpg'\n",
    "text, raw, avg_prob = extract_text_from_image(image_path)\n",
    "print(\"Extracted Text:\", text)\n",
    "# print(\"Raw Data:\", raw)\n",
    "print(\"Average Probability:\", avg_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exil Ieo @ su-nig {6v.m4 One- no } | 03 2114 5414 Ajeat 54nia Trave( \"V^+ I(tuc SMART Travel Agency :Sedunia Travel Services WeRLD Package Kuala Lumpur Tour Date 05 January 2024 Kuala Lumpur Tour No. of Pax 04 pax (02 adult & 02 child) Date 05 January 2024 11.OOAM 7 OOPM Full Day (8 Hours) Time Pick up Alila Hotel in Bangsar Wilayah Persekutuan Kuala Lumpur Location 58, Jalan Ang Seng, Brickfields, 50470 Kualo Lumpur, Drop off Alila Hotel in Bangsar Wilayah Persekutuan Kuala Lumpur Location 58, Jalan Ang Seng, Brickfields, 50470 Kuala Lumpur, Type of Mini Van Vehicle Tour Guide English Speaking Entrance Fee Not included Meals Adult (MYR 50.00) Child (MYR 25.00) etarian Meal for Indian family (MYR) MYR 550.00 (transport/tour) MYR 300 (Tour guide) MYR 300 (meals) MYR3OO Price (markup) Highlight Gaze at the beautiful city of Kuala Lumpur from the Observation Deck of the Petronas Twin Towers Climb Batu Caves to explore Southeast Asia\\'$ most visited Hindu Temple Capture your memories with your camera at all these major landmarks_ Shop in Central Market, one of the oldest markets in Kuala Lumpur Places To Petronas Twin Towers Visit Menara Kuala Lumpur Petaling Street (shopping) Istana Negara (the King\\' Palace) Old Train Station Negara (National Monument) Batu Cave Restaurant (Lunch and Dinner) Budget per pax: Adult (MYR 50.00) Child (MYR 25.00) \"Vegetarian Meal for Indian family = TOTAL 1,450.00 Price for 4 pax 04\" dc{ tvp Day Day \"Vege Tugu'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) LLM(image: jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading specialist: TRAVELLER ...\n",
      "TRAVELLER embedding: TEST - CLIENT - LOADED\n",
      "TRAVELLER embedding: TEST - CLIENT REQUEST - LOADED\n",
      "TRAVELLER embedding: TEST - FLIGHTS - LOADED\n",
      "TRAVELLER embedding: TEST - ACCOMODATIONS - LOADED\n",
      "TRAVELLER embedding: TEST - ACTIVITIES - LOADED\n",
      "TRAVELLER embedding: TEST - SERVICES - LOADED\n",
      "TRAVELLER loaded\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Could not find image: database\\travel\\Ingest\\partners - johor - theme park_legoland.pdf",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 29\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Ask librarian to get acquinted with the specialist database\u001b[39;00m\n\u001b[0;32m     11\u001b[0m librarian\u001b[38;5;241m.\u001b[39mTraveller\u001b[38;5;241m.\u001b[39mload_data_model(reembed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     12\u001b[0m                                     embed_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     13\u001b[0m                                     data_model_keys \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTEST - CLIENT\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIENT ID\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m                                                     \n\u001b[0;32m     28\u001b[0m                                     )\n\u001b[1;32m---> 29\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mlibrarian\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTraveller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_specialist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mprompt_1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprompt_1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mprompt_2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemini-pro-vision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\PycharmProjects\\smart-travels\\llm_handler\\GHandler.py:102\u001b[0m, in \u001b[0;36mGHandler.prompt_image\u001b[1;34m(self, image_path, prompt_1, prompt_2, model_name)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Validate that an image is present\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (img \u001b[38;5;241m:=\u001b[39m Path(image_path))\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    104\u001b[0m image_parts \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    105\u001b[0m     {\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmime_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage/jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: Path(image_path)\u001b[38;5;241m.\u001b[39mread_bytes()\n\u001b[0;32m    108\u001b[0m     },\n\u001b[0;32m    109\u001b[0m ]\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompt_2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Could not find image: database\\travel\\Ingest\\partners - johor - theme park_legoland.pdf"
     ]
    }
   ],
   "source": [
    "image_path = './database/travel/Ingest/partners - johor - theme park_legoland.pdf'\n",
    "prompt_1 = \"\"\"The image is a travel itinerary that i want to digitise so that i can populate a database. \n",
    "Help analyse the content and reproduce it in a structured format such that it is easily extractable\n",
    "\"\"\"\n",
    "from RAG.librarian import Librarian\n",
    "librarian = Librarian(librarian_LLM_model = \"GEMINI\")\n",
    "# SELECT SPECIALIST DATABASE\n",
    "librarian.select_specialist(specialist = \"traveller\", specialist_LLM_model = \"GEMINI\", )\n",
    "\n",
    "# Ask librarian to get acquinted with the specialist database\n",
    "librarian.Traveller.load_data_model(reembed = False,\n",
    "                                    embed_id = 0,\n",
    "                                    data_model_keys = {\"TEST - CLIENT\":\"CLIENT ID\",\n",
    "                                                        \"TEST - CLIENT REQUEST\":\"CLIENT ID\",\n",
    "                                                        \"TEST - FLIGHTS\":\"FLIGHT ID\",\n",
    "                                                        \"TEST - ACCOMODATIONS\":\"ACCOMODATION ID\",\n",
    "                                                        \"TEST - ACTIVITIES\":\"ACTIVITY ID\",\n",
    "                                                        \"TEST - SERVICES\":\"SERVICE ID\",\n",
    "                                                        },\n",
    "                                    reembed_table = {\"TEST - CLIENT\":True,\n",
    "                                                    \"TEST - CLIENT REQUEST\":True,\n",
    "                                                    \"TEST - FLIGHTS\":True,\n",
    "                                                    \"TEST - ACCOMODATIONS\":True,\n",
    "                                                    \"TEST - ACTIVITIES\":True,\n",
    "                                                    \"TEST - SERVICES\":True,\n",
    "                                                    }\n",
    "                                                    \n",
    "                                    )\n",
    "response = librarian.Traveller.model_specialist.prompt_image(image_path = image_path,\n",
    "                                                  prompt_1 = prompt_1,\n",
    "                                                  prompt_2 = None,\n",
    "                                                  model_name = \"gemini-pro-vision\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (1.26.4)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pandas in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (2.2.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: openpyxl in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (3.1.2)\n",
      "Requirement already satisfied: google-generativeai in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from -r requirements.txt (line 6)) (0.4.1)\n",
      "Requirement already satisfied: google-cloud-aiplatform in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from -r requirements.txt (line 7)) (1.48.0)\n",
      "Requirement already satisfied: cohere in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from -r requirements.txt (line 8)) (5.3.2)\n",
      "Requirement already satisfied: anthropic in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from -r requirements.txt (line 9)) (0.20.0)\n",
      "Requirement already satisfied: python-telegram-bot in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from -r requirements.txt (line 11)) (21.1.1)\n",
      "Requirement already satisfied: gspread in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from -r requirements.txt (line 12)) (6.1.0)\n",
      "Requirement already satisfied: PyPDF2 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from -r requirements.txt (line 13)) (3.0.1)\n",
      "Requirement already satisfied: Pillow in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from -r requirements.txt (line 14)) (10.3.0)\n",
      "Requirement already satisfied: Flask in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from -r requirements.txt (line 15)) (3.0.3)\n",
      "Requirement already satisfied: tiktoken in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from -r requirements.txt (line 17)) (0.6.0)\n",
      "Requirement already satisfied: tqdm in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from -r requirements.txt (line 18)) (4.66.2)\n",
      "Requirement already satisfied: wikipedia in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from -r requirements.txt (line 19)) (1.4.0)\n",
      "Requirement already satisfied: llama-index in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from -r requirements.txt (line 20)) (0.10.35)\n",
      "Collecting Flask-CORS (from -r requirements.txt (line 21))\n",
      "  Downloading Flask_Cors-4.0.1-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: et-xmlfile in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from openpyxl->-r requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.4.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-generativeai->-r requirements.txt (line 6)) (0.4.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-generativeai->-r requirements.txt (line 6)) (2.28.2)\n",
      "Requirement already satisfied: google-api-core in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-generativeai->-r requirements.txt (line 6)) (2.17.1)\n",
      "Requirement already satisfied: protobuf in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-generativeai->-r requirements.txt (line 6)) (4.25.3)\n",
      "Requirement already satisfied: pydantic in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-generativeai->-r requirements.txt (line 6)) (2.6.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-generativeai->-r requirements.txt (line 6)) (4.10.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-ai-generativelanguage==0.4.0->google-generativeai->-r requirements.txt (line 6)) (1.23.0)\n",
      "Requirement already satisfied: packaging>=14.3 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-cloud-aiplatform->-r requirements.txt (line 7)) (24.0)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-cloud-aiplatform->-r requirements.txt (line 7)) (2.16.0)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-cloud-aiplatform->-r requirements.txt (line 7)) (3.20.1)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-cloud-aiplatform->-r requirements.txt (line 7)) (1.12.3)\n",
      "Requirement already satisfied: shapely<3.0.0dev in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-cloud-aiplatform->-r requirements.txt (line 7)) (2.0.4)\n",
      "Requirement already satisfied: docstring-parser<1 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-cloud-aiplatform->-r requirements.txt (line 7)) (0.16)\n",
      "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from cohere->-r requirements.txt (line 8)) (1.9.4)\n",
      "Requirement already satisfied: httpx>=0.21.2 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from cohere->-r requirements.txt (line 8)) (0.27.0)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from cohere->-r requirements.txt (line 8)) (0.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from cohere->-r requirements.txt (line 8)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.16.0,>=0.15.2 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from cohere->-r requirements.txt (line 8)) (0.15.2)\n",
      "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from cohere->-r requirements.txt (line 8)) (2.31.0.20240406)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from anthropic->-r requirements.txt (line 9)) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from anthropic->-r requirements.txt (line 9)) (1.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from anthropic->-r requirements.txt (line 9)) (1.3.1)\n",
      "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from gspread->-r requirements.txt (line 12)) (1.2.0)\n",
      "Requirement already satisfied: StrEnum==0.4.15 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from gspread->-r requirements.txt (line 12)) (0.4.15)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from Flask->-r requirements.txt (line 15)) (3.0.2)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from Flask->-r requirements.txt (line 15)) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from Flask->-r requirements.txt (line 15)) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from Flask->-r requirements.txt (line 15)) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from Flask->-r requirements.txt (line 15)) (1.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from tiktoken->-r requirements.txt (line 17)) (2023.12.25)\n",
      "Requirement already satisfied: colorama in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from tqdm->-r requirements.txt (line 18)) (0.4.6)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from wikipedia->-r requirements.txt (line 19)) (4.12.3)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index->-r requirements.txt (line 20)) (0.2.4)\n",
      "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index->-r requirements.txt (line 20)) (0.1.12)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.35 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index->-r requirements.txt (line 20)) (0.10.35.post1)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index->-r requirements.txt (line 20)) (0.1.9)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index->-r requirements.txt (line 20)) (0.1.6)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index->-r requirements.txt (line 20)) (0.9.48)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.13 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index->-r requirements.txt (line 20)) (0.1.18)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index->-r requirements.txt (line 20)) (0.1.5)\n",
      "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index->-r requirements.txt (line 20)) (0.1.6)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index->-r requirements.txt (line 20)) (0.1.3)\n",
      "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index->-r requirements.txt (line 20)) (0.1.22)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index->-r requirements.txt (line 20)) (0.1.4)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->anthropic->-r requirements.txt (line 9)) (3.6)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-core->google-generativeai->-r requirements.txt (line 6)) (1.63.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.4.0->google-generativeai->-r requirements.txt (line 6)) (1.62.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.4.0->google-generativeai->-r requirements.txt (line 6)) (1.62.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai->-r requirements.txt (line 6)) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai->-r requirements.txt (line 6)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai->-r requirements.txt (line 6)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-auth-oauthlib>=0.4.1->gspread->-r requirements.txt (line 12)) (2.0.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 7)) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 7)) (2.7.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform->-r requirements.txt (line 7)) (0.13.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r requirements.txt (line 7)) (1.5.0)\n",
      "Requirement already satisfied: certifi in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from httpx>=0.21.2->cohere->-r requirements.txt (line 8)) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from httpx>=0.21.2->cohere->-r requirements.txt (line 8)) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.21.2->cohere->-r requirements.txt (line 8)) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from Jinja2>=3.1.2->Flask->-r requirements.txt (line 15)) (2.1.5)\n",
      "Requirement already satisfied: openai>=1.14.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-agent-openai<0.3.0,>=0.1.4->llama-index->-r requirements.txt (line 20)) (1.27.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 20)) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 20)) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 20)) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 20)) (0.6.5)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 20)) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 20)) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 20)) (2024.3.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 20)) (0.1.19)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 20)) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 20)) (3.3)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 20)) (3.8.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 20)) (8.3.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 20)) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 20)) (1.16.0)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index->-r requirements.txt (line 20)) (4.2.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index->-r requirements.txt (line 20)) (0.0.26)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from beautifulsoup4->wikipedia->-r requirements.txt (line 19)) (2.5)\n",
      "Requirement already satisfied: llama-parse<0.5.0,>=0.4.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index->-r requirements.txt (line 20)) (0.4.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from pydantic->google-generativeai->-r requirements.txt (line 6)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from pydantic->google-generativeai->-r requirements.txt (line 6)) (2.16.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->cohere->-r requirements.txt (line 8)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->cohere->-r requirements.txt (line 8)) (1.26.18)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from tokenizers<0.16.0,>=0.15.2->cohere->-r requirements.txt (line 8)) (0.21.4)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.0.0->cohere->-r requirements.txt (line 8))\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 20)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 20)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 20)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 20)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 20)) (1.9.4)\n",
      "Requirement already satisfied: filelock in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16.0,>=0.15.2->cohere->-r requirements.txt (line 8)) (3.13.1)\n",
      "Requirement already satisfied: joblib in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 20)) (1.4.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai->-r requirements.txt (line 6)) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread->-r requirements.txt (line 12)) (3.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 20)) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 20)) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 20)) (3.21.2)\n",
      "Downloading Flask_Cors-4.0.1-py2.py3-none-any.whl (14 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: urllib3, Flask-CORS\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.18\n",
      "    Uninstalling urllib3-1.26.18:\n",
      "      Successfully uninstalled urllib3-1.26.18\n",
      "Successfully installed Flask-CORS-4.0.1 urllib3-2.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gpsoauth 1.1.0 requires urllib3<2.0, but you have urllib3 2.2.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\PycharmProjects\\smart-travels\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "menara taming sari.pdf\n",
      "processing: menara taming sari.pdf\n"
     ]
    },
    {
     "ename": "PDFInfoNotInstalledError",
     "evalue": "Unable to get page count. Is poppler installed and in PATH?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32md:\\PycharmProjects\\smart-travels\\.venv\\Lib\\site-packages\\pdf2image\\pdf2image.py:581\u001b[0m, in \u001b[0;36mpdfinfo_from_path\u001b[1;34m(pdf_path, userpw, ownerpw, poppler_path, rawdates, timeout, first_page, last_page)\u001b[0m\n\u001b[0;32m    580\u001b[0m     env[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLD_LIBRARY_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m poppler_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m env\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLD_LIBRARY_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 581\u001b[0m proc \u001b[38;5;241m=\u001b[39m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Programs\\Python\\Python312\\Lib\\subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[0;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Programs\\Python\\Python312\\Lib\\subprocess.py:1538\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[0m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1538\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1540\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1547\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPDFInfoNotInstalledError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m pdf_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(pdf_dir, filename)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Convert PDF to image\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m responses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# image.show()\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# save image to file\u001b[39;00m\n",
      "File \u001b[1;32md:\\PycharmProjects\\smart-travels\\.venv\\Lib\\site-packages\\pdf2image\\pdf2image.py:127\u001b[0m, in \u001b[0;36mconvert_from_path\u001b[1;34m(pdf_path, dpi, output_folder, first_page, last_page, fmt, jpegopt, thread_count, userpw, ownerpw, use_cropbox, strict, transparent, single_file, output_file, poppler_path, grayscale, size, paths_only, use_pdftocairo, timeout, hide_annotations)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(poppler_path, PurePath):\n\u001b[0;32m    125\u001b[0m     poppler_path \u001b[38;5;241m=\u001b[39m poppler_path\u001b[38;5;241m.\u001b[39mas_posix()\n\u001b[1;32m--> 127\u001b[0m page_count \u001b[38;5;241m=\u001b[39m \u001b[43mpdfinfo_from_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserpw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mownerpw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoppler_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpoppler_path\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# We start by getting the output format, the buffer processing function and if we need pdftocairo\u001b[39;00m\n\u001b[0;32m    132\u001b[0m parsed_fmt, final_extension, parse_buffer_func, use_pdfcairo_format \u001b[38;5;241m=\u001b[39m _parse_format(\n\u001b[0;32m    133\u001b[0m     fmt, grayscale\n\u001b[0;32m    134\u001b[0m )\n",
      "File \u001b[1;32md:\\PycharmProjects\\smart-travels\\.venv\\Lib\\site-packages\\pdf2image\\pdf2image.py:607\u001b[0m, in \u001b[0;36mpdfinfo_from_path\u001b[1;34m(pdf_path, userpw, ownerpw, poppler_path, rawdates, timeout, first_page, last_page)\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m d\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PDFInfoNotInstalledError(\n\u001b[0;32m    608\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to get page count. Is poppler installed and in PATH?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    609\u001b[0m     )\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    611\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PDFPageCountError(\n\u001b[0;32m    612\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to get page count.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m     )\n",
      "\u001b[1;31mPDFInfoNotInstalledError\u001b[0m: Unable to get page count. Is poppler installed and in PATH?"
     ]
    }
   ],
   "source": [
    "# import pytesseract\n",
    "\n",
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "\n",
    "from settings import GEMINI_API_KEY\n",
    "from llm_handler.GHandler import GHandler\n",
    "\n",
    "ghandler = GHandler(GEMINI_API_KEY,                  \n",
    "                    generation_config = {\"temperature\": 0.9,\n",
    "                                        \"top_p\": 0.95,\n",
    "                                        \"top_k\": 40,\n",
    "                                        \"max_output_tokens\": 40000,\n",
    "                                        },\n",
    "                    block_threshold=\"BLOCK_NONE\",\n",
    "                    )\n",
    "\n",
    "prompt_1 = \"You are an OCR bot. Extract ALL the text from the image as raw text. Ensure all pricing, phone numbers and emails are extracted ACCURATELY. OCR text output is sometimes wrong, so correct it where needed.\"\n",
    "# Specify the directory containing the PDF files\n",
    "pdf_dir = './downloaded_pdfs/partners - malacca - attractions'#partners - johor - theme park_legoland.pdf'\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(pdf_dir):\n",
    "    print(filename)\n",
    "    if filename.endswith('.pdf'):\n",
    "        # Open the PDF file\n",
    "        print('processing:', filename)\n",
    "        pdf_file = os.path.join(pdf_dir, filename)\n",
    "        \n",
    "        # Convert PDF to image\n",
    "        images = convert_from_path(pdf_file)\n",
    "        responses = []\n",
    "        for image in images:\n",
    "            # image.show()\n",
    "            # save image to file\n",
    "            image_path = './database/Ingest/' + filename.replace('.pdf', '.jpg')\n",
    "            image.save(image_path, 'JPEG')\n",
    "            print('processing:', image_path)\n",
    "            response = ghandler.prompt_image(image_path = image_path,\n",
    "                                                  prompt_1 = prompt_1,\n",
    "                                                  prompt_2 = None,\n",
    "                                                  model_name = \"gemini-pro-vision\",)\n",
    "            print(response)\n",
    "            responses.append(response.text)\n",
    "            # use ghandler image analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading PyMuPDF-1.24.4-cp312-none-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting PyMuPDFb==1.24.3 (from PyMuPDF)\n",
      "  Downloading PyMuPDFb-1.24.3-py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Downloading PyMuPDF-1.24.4-cp312-none-win_amd64.whl (3.2 MB)\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/3.2 MB 991.0 kB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 1.4/3.2 MB 18.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.2/3.2 MB 25.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.2/3.2 MB 22.7 MB/s eta 0:00:00\n",
      "Downloading PyMuPDFb-1.24.3-py3-none-win_amd64.whl (12.4 MB)\n",
      "   ---------------------------------------- 0.0/12.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 2.3/12.4 MB 48.8 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.7/12.4 MB 50.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.7/12.4 MB 48.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.4/12.4 MB 44.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.8/12.4 MB 46.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.4/12.4 MB 46.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.4/12.4 MB 40.9 MB/s eta 0:00:00\n",
      "Installing collected packages: PyMuPDFb, PyMuPDF\n",
      "Successfully installed PyMuPDF-1.24.4 PyMuPDFb-1.24.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # Import PyMuPDF\n",
    "pdf_file = './downloaded_pdfs/partners - malacca - attractions/menara taming sari.pdf' #'./database/Ingest/partners - johor - theme park_legoland.pdf'\n",
    "def gemini_ocr(pdf_file):\n",
    "    \"\"\"Performs OCR on the given PDF using Gemini and returns extracted text.\"\"\"\n",
    "    ghandler = GHandler(GEMINI_API_KEY, generation_config={\"temperature\": 0.9, \"top_p\": 0.95, \"top_k\": 40, \"max_output_tokens\": 40000}, block_threshold=\"BLOCK_NONE\")\n",
    "    prompt = \"You are an OCR bot. Extract ALL the text from the image as raw text. Ensure all pricing, phone numbers, and emails are extracted ACCURATELY. OCR text output is sometimes wrong, so correct it where needed.\"\n",
    "\n",
    "    doc = fitz.open(pdf_file)\n",
    "    extracted_text = \"\"\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc.load_page(page_num)\n",
    "        pix = page.get_pixmap(matrix=fitz.Matrix(300/72, 300/72))  # Increase resolution for better OCR\n",
    "        image_path = pdf_file.replace('.pdf', f'_page_{page_num + 1}.jpg')\n",
    "        pix.save(image_path)  # Save each page as an image\n",
    "\n",
    "        response = ghandler.prompt_image(image_path=image_path, prompt_1=prompt, prompt_2=None, model_name=\"gemini-pro-vision\")\n",
    "        extracted_text += response.text\n",
    "        os.remove(image_path)  # Clean up the image file\n",
    "\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = gemini_ocr(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MENARA TAMING SARI\n",
      "澄沙里塔\n",
      "தாமிரிசாரி கோபுரம்\n",
      "TAMING SARI TOWER\n",
      "MELAKA\n",
      "\n",
      "Taming Sari Tower\n",
      "Bird's-Eye View of Melaka\n",
      "www.menaratamingsari.com\n",
      "\n",
      "4th Tallest Revolving Tower In Malaysia\n",
      "Revolving at 90 meters above sea level\n",
      "360-degree view of the historical city of Melaka\n",
      "\n",
      "MENARA TAMING SARI SDN. BHD. (744024-H)\n",
      "Wisma Taming Sari, Jalan Merdeka, 75000 Melaka, Malaysia.\n",
      "Tel: 06-281 6888 Fax: 06-281 6066\n",
      "Email: ticketing@menaratamingsari.com\n",
      "\n",
      "Adult: RM50.00\n",
      "Child: RM25.00\n",
      "Family Package (2 adults + 2 children): RM100.00\n",
      "(All prices are inclusive of 6% Service Tax) A Breathtaking View From Above\n",
      "\n",
      "A 360° viewing tower that provides a breath-taking view from a height of 80 meters in the air. The 7-minute ride enables you to view spectacular and panoramic sights of Melaka, UNESCO World Heritage City such as St. Paul's Hill, Independence Memorial Building, Samudra Museum (Flor De La Mar), Melaka River, Dataran Pahlawan, Selat Melaka Mosque, Pulau Besar and the Straits of Melaka.\n",
      "\n",
      "While enjoying the exhilarating sightseeing in the air, the ride also enables you to witness the rapid development taking place in Melaka.\n",
      "\n",
      "With an overall tower height of 110 meters from the ground to the top and built using Swiss technology, the revolving glass cabin has a passenger capacity of 66 people per viewing session.\n",
      "\n",
      "Join the ride and make your visit to Melaka a memorable one. **KEINDAHAN PANORAMA DARI KETINGGIAN 80 METER**\n",
      "\n",
      "Setiap menaiki tinjau yang memberikan anda menikmati pemandangan indah dari ketinggian 80 meter di udara selama 7 menit ini, selain dapat melihat berbagai tempat tempat menarik di sekitar Melaka Bandaraya Warisan Dunia UNESCO seperti Bukit St. Paul, Bangunan Memorial Kemerdekaan, Muzium Samudera di Jalan Parameswara, Pulau Besar dan Selat Melaka.\n",
      "\n",
      "Disamping menikmati panorama indah pulau Melaka juga berpeluang melihat pembangunan yang pesat di negeri Melaka.\n",
      "\n",
      "Dengan ketinggian keseluruhan dari tapak ke puncak setinggi 110 meter serta penggunaan teknologi terkini dari Switzerland, kabin kaca yang berbentuk ini mampu menampung seramai 66 orang bagi setiap lawatan.\n",
      "\n",
      "Rasai pengalaman menakjubkan dari pelantar pemandangan Menara Taming Sari.\n",
      "\n",
      "**Waktu Operasi 10.00 pagi - 11.00 malam / Setiap hari**\n",
      "\n",
      "Harga Tiket (Semua harga termasuk harga untuk pemegang MyCard)\n",
      "Dewasa RM23.00 (Potongan harga untuk pemegang MyCard)\n",
      "Kanak-kanak RM15.00 (Tidak termasuk pada tema dan syarat)\n",
      "\n",
      "**Bird's Eye View Of Melaka**\n",
      "\n",
      "_The Malaysia Book Of Records_ TOWER SPECIFICATIONS\n",
      "SPESIFIKASI MENARA\n",
      "\n",
      "NAME / NAMA\n",
      "Menara Taming Sari (360° viewing tower)\n",
      "Menara Taming Sari (menara tinjau 360°)\n",
      "\n",
      "HEIGHT / TINGGI\n",
      "110 meters\n",
      "110 meter\n",
      "\n",
      "PASSENGER CAPACITY / KAPASITI PENUMPANG\n",
      "66 passengers\n",
      "66 penumpang\n",
      "\n",
      "MADE / TECHNOLOGY / BUATAN / TEKNOLOGI\n",
      "Switzerland\n",
      "Switzerland\n",
      "\n",
      "RIDE DURATION / DURASI\n",
      "7 minutes\n",
      "7 minit\n",
      "\n",
      "SEARCH & LOCATE US:\n",
      "Latitude - 2.1909\n",
      "Longitude - 102.247\n",
      "\n",
      "FOR FURTHER ENQUIRY PLEASE CONTACT\n",
      "MELAKA TAMING SARI BERHAD\n",
      "Jalan Merdeka, Bandar Hilir, 75000 Melaka, Malaysia\n",
      "No. 8-3, Aras 3, Bangunan Kota Cemerlang\n",
      "75450 Ayer Keroh, Melaka.\n",
      "Tel: +60 6 - 288 1100\n",
      "Fax: +60 6 - 231 1633\n",
      "Email: menara@tammingsari.com.my\n",
      "Website: www.menaratamingsari.com.my\n",
      "\n",
      "MELAKA\n",
      "\n",
      "Touch 'n Go\n",
      "Boost\n",
      "Alipay\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'conda-forge'\n"
     ]
    }
   ],
   "source": [
    "pip install -c conda-forge poppler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n",
      " MATTA FAIR EXCLUSIVE\n",
      "2D1N Room with Breakfast @ LEGOLAND® Hotel\n",
      "Stay Period: 22 Mar - 30 Sept 2024\n",
      "\n",
      "* Each room sleeps up to 5 persons (2 adults & 3 children or 3 adults & 2 children)\n",
      "* Offer valid for Themed Room and Premium Themed Room category. Applicable for Pirate, Kingdom and Adventure Theme\n",
      "\n",
      "Themed Room \n",
      "UP: RM950\n",
      "RM650 nett** per night\n",
      "\n",
      "Premium Themed Room \n",
      "UP: RM1,100\n",
      "RM750 nett** per night\n",
      "\n",
      "**Surcharge apply: RM50 (Fridays & Saturdays)\n",
      "RM150 (24 May - 23 June, 15 Jul - 23 Aug, 31 Aug - 8 Sept, 13 - 21 Sept 2024)\n",
      "\n",
      "STAY & PLAY PACKAGE FOR A FAMILY OF 4 FROM RM1,762**\n",
      "Save up to RM500\n",
      "\n",
      "2D1N** @ LEGOLAND® Hotel\n",
      "Enjoy a unique LEGO® overnight experience\n",
      "\n",
      "BREAKFAST at Bricks Family Restaurant\n",
      "Buffet breakfast in a child-friendly dining environment for up to 5 pax\n",
      "\n",
      "TRIPLE-PARK ANNUAL PASS*\n",
      "2 Adults & 2 Children\n",
      "1 year unlimited access to theme park, Water Park and Aquarium\n",
      "\n",
      "FREE RM100 cash voucher\n",
      "\n",
      "*Collection of Annual Pass voucher on the date of arrival. **Price based on 2 Adults and 2 Children with Themed Room.\n",
      "\n",
      "LEGOLAND® MALAYSIA RESORT  www.LEGOLAND.my\n",
      "\n",
      "LEGO, the LEGO logo, the Brick and Knob configurations, the Minifigure and LEGOLAND are trademarks of the LEGO Group. ©2023 The LEGO Group. LEGOLAND® Malaysia is a part of THE MERLIN ENTERTAINMENTS PLC\n"
     ]
    }
   ],
   "source": [
    "print(len(responses[1]))\n",
    "print(responses[1])\n",
    "#  count number of characters in the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) FM RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RAG.cohere_RAG import RAG\n",
    "query = \"i want to go to a negeri sembilan cultural tour, give me a 3 days itinerary\"\n",
    "rag = RAG()\n",
    "result = rag.rag_pipeline(query = query, sheet_name = \"Master Database\", worksheet_name=\"inventory\", reembed= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's an itinerary for a cultural tour of Negeri Sembilan, a state in Malaysia, for three days. \n",
      "\n",
      "## Day 1: Cruise Tasik Putrajaya\n",
      "Start your morning with a 45-minute cruise on Tasik Putrajaya, or Lake Putrajaya. This cruise will take you beyond the city limits and offer a unique perspective of the surrounding areas. You'll drift past 17 interesting locations, including mosques, bridges and monuments.\n",
      "\n",
      "**Snippet**:\n",
      "Cruise beyond your imagination...\n",
      "\n",
      "**Price**:\n",
      "RM30 per adult \n",
      "\n",
      "**Vendor**:\n",
      "MARINA PUTRAJAYA SDN BHD\n",
      "Managed by: PULSE GROUP\n",
      "Phone: 03-8881 0648\n",
      "Email: [email protected]\n",
      "Website: www.cruisetasikputrajaya.com\n",
      "\n",
      "## Day 2: Perbadanan Putrajaya\n",
      "For your second day, how about a trip to the Putrajaya Botanical Garden, which boasts an impressive collection of local and exotic flora, or take a stroll down the beautiful Putrajaya Waterfront, lined with cafes and restaurants, and admire the architectural marvels of the city skyline. If you want to explore both, there's a convenient package deal for RM30 that will give you access to these two locations and more. \n",
      "\n",
      "**Snippet** (for both options):\n",
      "Discover the beauty of Putrajaya!\n",
      "\n",
      "**Price** (Perbadanan Putrajaya Botanical Garden):\n",
      "RM10\n",
      "\n",
      "**Price** (Putrajaya Waterfront):\n",
      "RM15\n",
      "\n",
      "**Vendor** (for both options):\n",
      "PULSE GROUP\n",
      "Phone: 03-8890 7776\n",
      "Email: [email protected]\n",
      "Website: www.pulsegroup.com.my\n",
      "\n",
      "## Day 3: Menara Taming Sari, Melaka\n",
      "End your trip with a visit to the historic city of Melaka and ascend the Menara Taming Sari, a 360° viewing tower that offers breathtaking views of the city. The 7-minute ride brings you 80 meters up in the air and is a great way to conclude your visit, taking in the sights of this UNESCO World Heritage City. \n",
      "\n",
      "**Snippet**:\n",
      "A 360° viewing tower that provides a breath-taking view from a height of 80 meters in the air.\n",
      "\n",
      "**Price**:\n",
      "RM22 per adult, RM15 per child\n",
      "\n",
      "**Vendor**:\n",
      "MELAKA TAMINNG SARI BERHAD\n",
      "Jalan Merdeka, Bandar Hilir, 75000 Melaka, Malaysia\n",
      "Phone: +606 - 288 1100\n",
      "Email: menaraatamingsari.official@gmail.com\n",
      "Website: menaraatamingsari.com.my\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index\n",
      "  Downloading llama_index-0.10.35-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index)\n",
      "  Downloading llama_index_agent_openai-0.2.4-py3-none-any.whl.metadata (678 bytes)\n",
      "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Downloading llama_index_cli-0.1.12-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.11.0,>=0.10.35 (from llama-index)\n",
      "  Downloading llama_index_core-0.10.35.post1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index)\n",
      "  Downloading llama_index_embeddings_openai-0.1.9-py3-none-any.whl.metadata (603 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
      "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index)\n",
      "  Downloading llama_index_llms_openai-0.1.18-py3-none-any.whl.metadata (559 bytes)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.1.5-py3-none-any.whl.metadata (677 bytes)\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index)\n",
      "  Downloading llama_index_program_openai-0.1.6-py3-none-any.whl.metadata (715 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index)\n",
      "  Downloading llama_index_readers_file-0.1.22-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.3.0,>=0.1.4->llama-index)\n",
      "  Downloading openai-1.27.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (6.0.1)\n",
      "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.35->llama-index)\n",
      "  Downloading SQLAlchemy-2.0.30-cp312-cp312-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.6 (from llama-index-core<0.11.0,>=0.10.35->llama-index)\n",
      "  Downloading aiohttp-3.9.5-cp312-cp312-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.35->llama-index)\n",
      "  Downloading dataclasses_json-0.6.5-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.35->llama-index)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.35->llama-index)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (2024.3.0)\n",
      "Requirement already satisfied: httpx in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (0.27.0)\n",
      "Collecting llamaindex-py-client<0.2.0,>=0.1.18 (from llama-index-core<0.11.0,>=0.10.35->llama-index)\n",
      "  Downloading llamaindex_py_client-0.1.19-py3-none-any.whl.metadata (760 bytes)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (1.6.0)\n",
      "Collecting networkx>=3.0 (from llama-index-core<0.11.0,>=0.10.35->llama-index)\n",
      "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting nltk<4.0.0,>=3.8.1 (from llama-index-core<0.11.0,>=0.10.35->llama-index)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (2.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (10.3.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (2.31.0)\n",
      "Collecting tenacity<9.0.0,>=8.2.0 (from llama-index-core<0.11.0,>=0.10.35->llama-index)\n",
      "  Downloading tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (4.10.0)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.35->llama-index)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core<0.11.0,>=0.10.35->llama-index)\n",
      "  Downloading wrapt-1.16.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
      "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Downloading pypdf-4.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse<0.5.0,>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading llama_parse-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index)\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index)\n",
      "  Downloading frozenlist-1.4.1-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index)\n",
      "  Downloading multidict-6.0.5-cp312-cp312-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index)\n",
      "  Downloading yarl-1.9.4-cp312-cp312-win_amd64.whl.metadata (32 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
      "Requirement already satisfied: pydantic>=1.10 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.6.4)\n",
      "Requirement already satisfied: anyio in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (4.3.0)\n",
      "Requirement already satisfied: certifi in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.0.4)\n",
      "Requirement already satisfied: idna in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (3.6)\n",
      "Requirement already satisfied: sniffio in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (0.14.0)\n",
      "Requirement already satisfied: click in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (8.1.7)\n",
      "Collecting joblib (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.35->llama-index)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (2023.12.25)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama-index) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.35->llama-index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.2.1)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.35->llama-index)\n",
      "  Downloading greenlet-3.0.3-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: colorama in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (0.4.6)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.35->llama-index)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.35->llama-index)\n",
      "  Downloading marshmallow-3.21.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.35->llama-index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.35->llama-index) (2024.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.35->llama-index) (24.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.35->llama-index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.16.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.16.0)\n",
      "Downloading llama_index-0.10.35-py3-none-any.whl (6.9 kB)\n",
      "Downloading llama_index_agent_openai-0.2.4-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\n",
      "Downloading llama_index_core-0.10.35.post1-py3-none-any.whl (15.4 MB)\n",
      "   ---------------------------------------- 0.0/15.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.4/15.4 MB 13.9 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.2/15.4 MB 14.7 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 2.1/15.4 MB 15.0 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 3.3/15.4 MB 17.4 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 4.8/15.4 MB 20.3 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 6.5/15.4 MB 23.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 8.2/15.4 MB 24.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 9.9/15.4 MB 27.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 11.9/15.4 MB 32.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.0/15.4 MB 38.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.4/15.4 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.4/15.4 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.4/15.4 MB 29.7 MB/s eta 0:00:00\n",
      "Downloading llama_index_embeddings_openai-0.1.9-py3-none-any.whl (6.0 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl (6.7 kB)\n",
      "Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------  2.0/2.0 MB 41.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 31.3 MB/s eta 0:00:00\n",
      "Downloading llama_index_llms_openai-0.1.18-py3-none-any.whl (11 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.1.5-py3-none-any.whl (5.8 kB)\n",
      "Downloading llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
      "Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_file-0.1.22-py3-none-any.whl (36 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl (2.5 kB)\n",
      "Downloading aiohttp-3.9.5-cp312-cp312-win_amd64.whl (369 kB)\n",
      "   ---------------------------------------- 0.0/369.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 369.0/369.0 kB 11.6 MB/s eta 0:00:00\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading llama_parse-0.4.2-py3-none-any.whl (7.6 kB)\n",
      "Downloading llamaindex_py_client-0.1.19-py3-none-any.whl (141 kB)\n",
      "   ---------------------------------------- 0.0/141.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 141.9/141.9 kB 8.2 MB/s eta 0:00:00\n",
      "Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.7/1.7 MB 54.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 27.3 MB/s eta 0:00:00\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.5/1.5 MB 99.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 31.9 MB/s eta 0:00:00\n",
      "Downloading openai-1.27.0-py3-none-any.whl (314 kB)\n",
      "   ---------------------------------------- 0.0/314.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 314.1/314.1 kB 19.0 MB/s eta 0:00:00\n",
      "Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
      "   ---------------------------------------- 0.0/290.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 290.4/290.4 kB 17.5 MB/s eta 0:00:00\n",
      "Downloading SQLAlchemy-2.0.30-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 1.9/2.1 MB 60.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 33.3 MB/s eta 0:00:00\n",
      "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading wrapt-1.16.0-cp312-cp312-win_amd64.whl (37 kB)\n",
      "Downloading dataclasses_json-0.6.5-py3-none-any.whl (28 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "   ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 60.8/60.8 kB 3.2 MB/s eta 0:00:00\n",
      "Downloading frozenlist-1.4.1-cp312-cp312-win_amd64.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.5/50.5 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading greenlet-3.0.3-cp312-cp312-win_amd64.whl (293 kB)\n",
      "   ---------------------------------------- 0.0/293.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 293.6/293.6 kB 17.7 MB/s eta 0:00:00\n",
      "Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
      "   ---------------------------------------- 0.0/49.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 49.3/49.3 kB 2.6 MB/s eta 0:00:00\n",
      "Downloading multidict-6.0.5-cp312-cp312-win_amd64.whl (27 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading yarl-1.9.4-cp312-cp312-win_amd64.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.4/76.4 kB 4.4 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "   ---------------------------------------- 0.0/301.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 301.8/301.8 kB 18.2 MB/s eta 0:00:00\n",
      "Installing collected packages: striprtf, dirtyjson, wrapt, tenacity, pypdf, networkx, mypy-extensions, multidict, marshmallow, joblib, greenlet, frozenlist, attrs, yarl, typing-inspect, SQLAlchemy, nltk, deprecated, aiosignal, openai, llamaindex-py-client, dataclasses-json, aiohttp, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
      "Successfully installed SQLAlchemy-2.0.30 aiohttp-3.9.5 aiosignal-1.3.1 attrs-23.2.0 dataclasses-json-0.6.5 deprecated-1.2.14 dirtyjson-1.0.8 frozenlist-1.4.1 greenlet-3.0.3 joblib-1.4.2 llama-index-0.10.35 llama-index-agent-openai-0.2.4 llama-index-cli-0.1.12 llama-index-core-0.10.35.post1 llama-index-embeddings-openai-0.1.9 llama-index-indices-managed-llama-cloud-0.1.6 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.18 llama-index-multi-modal-llms-openai-0.1.5 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.22 llama-index-readers-llama-parse-0.1.4 llama-parse-0.4.2 llamaindex-py-client-0.1.19 marshmallow-3.21.2 multidict-6.0.5 mypy-extensions-1.0.0 networkx-3.3 nltk-3.8.1 openai-1.27.0 pypdf-4.2.0 striprtf-0.0.26 tenacity-8.3.0 typing-inspect-0.9.0 wrapt-1.16.0 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-readers-google\n",
      "  Downloading llama_index_readers_google-0.2.4-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting gkeepapi<0.16.0,>=0.15.1 (from llama-index-readers-google)\n",
      "  Downloading gkeepapi-0.15.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: google-api-python-client<3.0.0,>=2.115.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-readers-google) (2.126.0)\n",
      "Requirement already satisfied: google-auth-httplib2<0.3.0,>=0.2.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-readers-google) (0.2.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2.0.0,>=1.2.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-readers-google) (1.2.0)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.11.post1 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-readers-google) (0.10.35.post1)\n",
      "Collecting pydrive<2.0.0,>=1.3.1 (from llama-index-readers-google)\n",
      "  Downloading PyDrive-1.3.1.tar.gz (987 kB)\n",
      "     ---------------------------------------- 0.0/987.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 10.2/987.4 kB ? eta -:--:--\n",
      "     - ----------------------------------- 51.2/987.4 kB 650.2 kB/s eta 0:00:02\n",
      "     ---------------------- --------------- 593.9/987.4 kB 5.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 987.4/987.4 kB 6.3 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting gpsoauth>=1.0.3 (from gkeepapi<0.16.0,>=0.15.1->llama-index-readers-google)\n",
      "  Downloading gpsoauth-1.1.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting future>=0.16.0 (from gkeepapi<0.16.0,>=0.15.1->llama-index-readers-google)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-python-client<3.0.0,>=2.115.0->llama-index-readers-google) (0.22.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-python-client<3.0.0,>=2.115.0->llama-index-readers-google) (2.28.2)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-python-client<3.0.0,>=2.115.0->llama-index-readers-google) (2.17.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-python-client<3.0.0,>=2.115.0->llama-index-readers-google) (4.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-auth-oauthlib<2.0.0,>=1.2.0->llama-index-readers-google) (2.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (0.6.5)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (2024.3.0)\n",
      "Requirement already satisfied: httpx in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (0.1.19)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (3.3)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (1.27.0)\n",
      "Requirement already satisfied: pandas in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (2.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (10.3.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (8.3.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (4.10.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (1.16.0)\n",
      "Collecting oauth2client>=4.0.0 (from pydrive<2.0.0,>=1.3.1->llama-index-readers-google)\n",
      "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (1.9.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client<3.0.0,>=2.115.0->llama-index-readers-google) (1.63.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client<3.0.0,>=2.115.0->llama-index-readers-google) (4.25.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client<3.0.0,>=2.115.0->llama-index-readers-google) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client<3.0.0,>=2.115.0->llama-index-readers-google) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client<3.0.0,>=2.115.0->llama-index-readers-google) (4.9)\n",
      "Collecting pycryptodomex>=3.0 (from gpsoauth>=1.0.3->gkeepapi<0.16.0,>=0.15.1->llama-index-readers-google)\n",
      "  Downloading pycryptodomex-3.20.0-cp35-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting urllib3<2.0 (from gpsoauth>=1.0.3->gkeepapi<0.16.0,>=0.15.1->llama-index-readers-google)\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "     ---------------------------------------- 0.0/48.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 48.9/48.9 kB 2.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client<3.0.0,>=2.115.0->llama-index-readers-google) (3.1.2)\n",
      "Requirement already satisfied: pydantic>=1.10 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (2.6.4)\n",
      "Requirement already satisfied: anyio in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (4.3.0)\n",
      "Requirement already satisfied: certifi in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (1.0.4)\n",
      "Requirement already satisfied: idna in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (3.6)\n",
      "Requirement already satisfied: sniffio in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (0.14.0)\n",
      "Requirement already satisfied: click in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (2023.12.25)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from oauth2client>=4.0.0->pydrive<2.0.0,>=1.3.1->llama-index-readers-google) (0.5.1)\n",
      "Requirement already satisfied: six>=1.6.1 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from oauth2client>=4.0.0->pydrive<2.0.0,>=1.3.1->llama-index-readers-google) (1.16.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (3.3.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2.0.0,>=1.2.0->llama-index-readers-google) (3.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (3.21.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (2024.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (24.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-readers-google) (2.16.3)\n",
      "Downloading llama_index_readers_google-0.2.4-py3-none-any.whl (23 kB)\n",
      "Downloading gkeepapi-0.15.1-py3-none-any.whl (22 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "   ---------------------------------------- 0.0/491.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 491.3/491.3 kB 15.5 MB/s eta 0:00:00\n",
      "Downloading gpsoauth-1.1.0-py3-none-any.whl (7.1 kB)\n",
      "Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "   ---------------------------------------- 0.0/98.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 98.2/98.2 kB 5.5 MB/s eta 0:00:00\n",
      "Downloading pycryptodomex-3.20.0-cp35-abi3-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.9/1.8 MB 28.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.8/1.8 MB 27.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 18.6 MB/s eta 0:00:00\n",
      "Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "   ---------------------------------------- 0.0/143.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 143.8/143.8 kB 4.3 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: pydrive\n",
      "  Building wheel for pydrive (pyproject.toml): started\n",
      "  Building wheel for pydrive (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pydrive: filename=PyDrive-1.3.1-py3-none-any.whl size=27462 sha256=7266a7fd9f5780c80f3e67c21e6b0dd199b9d047b36e823ee0cd4ac09e0c0f69\n",
      "  Stored in directory: c:\\users\\shaik\\appdata\\local\\pip\\cache\\wheels\\6c\\10\\da\\a5b513f5b3916fc391c20ee7b4633e5cf3396d570cdd74970f\n",
      "Successfully built pydrive\n",
      "Installing collected packages: urllib3, pycryptodomex, future, oauth2client, gpsoauth, gkeepapi, pydrive, llama-index-readers-google\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.1\n",
      "    Uninstalling urllib3-2.2.1:\n",
      "      Successfully uninstalled urllib3-2.2.1\n",
      "Successfully installed future-1.0.0 gkeepapi-0.15.1 gpsoauth-1.1.0 llama-index-readers-google-0.2.4 oauth2client-4.1.3 pycryptodomex-3.20.0 pydrive-1.3.1 urllib3-1.26.18\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "types-requests 2.31.0.20240406 requires urllib3>=2, but you have urllib3 1.26.18 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index-readers-google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.google import GoogleDriveReader\n",
    "loader = GoogleDriveReader(credentials_path='./gdrive/phrasal-ability-419201-d527372ace3b.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Client secrets must be for a web or installed app.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\PycharmProjects\\smart-travels\\.venv\\Lib\\site-packages\\llama_index\\readers\\google\\drive\\base.py:511\u001b[0m, in \u001b[0;36mGoogleDriveReader.load_data\u001b[1;34m(self, drive_id, folder_id, file_ids, mime_types, query_string)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    492\u001b[0m     drive_id: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    496\u001b[0m     query_string: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    497\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    498\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data from the folder id or file ids.\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \n\u001b[0;32m    500\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;124;03m        List[Document]: A list of documents.\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_creds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_credentials\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;66;03m# If no arguments are provided to load_data, default to the object attributes\u001b[39;00m\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drive_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\PycharmProjects\\smart-travels\\.venv\\Lib\\site-packages\\llama_index\\readers\\google\\drive\\base.py:170\u001b[0m, in \u001b[0;36mGoogleDriveReader._get_credentials\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m     creds\u001b[38;5;241m.\u001b[39mrefresh(Request())\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m     flow \u001b[38;5;241m=\u001b[39m \u001b[43mInstalledAppFlow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_client_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSCOPES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m     creds \u001b[38;5;241m=\u001b[39m flow\u001b[38;5;241m.\u001b[39mrun_local_server(port\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# Save the credentials for the next run\u001b[39;00m\n",
      "File \u001b[1;32mc:\\PycharmProjects\\smart-travels\\.venv\\Lib\\site-packages\\google_auth_oauthlib\\flow.py:159\u001b[0m, in \u001b[0;36mFlow.from_client_config\u001b[1;34m(cls, client_config, scopes, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m     client_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstalled\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClient secrets must be for a web or installed app.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# these args cannot be passed to requests_oauthlib.OAuth2Session\u001b[39;00m\n\u001b[0;32m    162\u001b[0m code_verifier \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_verifier\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: Client secrets must be for a web or installed app."
     ]
    }
   ],
   "source": [
    "loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "\n",
    "class GspreadHandler:\n",
    "    def __init__(self, credentials_filepath):\n",
    "        self.credentials_filepath = credentials_filepath\n",
    "\n",
    "        # Initialize both gspread (for sheets) and Drive API (for other files)\n",
    "        self.gc = gspread.service_account(filename=self.credentials_filepath)\n",
    "        self.creds = service_account.Credentials.from_service_account_file(\n",
    "            self.credentials_filepath, scopes=['https://www.googleapis.com/auth/drive']\n",
    "        )\n",
    "        self.drive_service = build('drive', 'v3', credentials=self.creds)\n",
    "\n",
    "    # ... (Your existing sheet-related methods remain unchanged) ...\n",
    "\n",
    "    def get_file_content(self, file_id):\n",
    "        \"\"\"Downloads and returns the content of a file from Google Drive.\n",
    "\n",
    "        Args:\n",
    "            file_id (str): The ID of the file in Google Drive.\n",
    "\n",
    "        Returns:\n",
    "            bytes: The content of the file (exact format depends on file type).\n",
    "        \"\"\"\n",
    "        request = self.drive_service.files().get_media(fileId=file_id)\n",
    "        file_content = request.execute()\n",
    "        return file_content\n",
    "\n",
    "    def list_files(self, query=None):\n",
    "        \"\"\"Lists files in Google Drive based on an optional query.\n",
    "\n",
    "        Args:\n",
    "            query (str, optional): A search query to filter the results (see Drive API docs).\n",
    "\n",
    "        Returns:\n",
    "            list: A list of file metadata dictionaries (name, id, etc.).\n",
    "        \"\"\"\n",
    "        results = self.drive_service.files().list(q=query).execute()\n",
    "        items = results.get('files', [])\n",
    "        return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'your_credentials.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m handler \u001b[38;5;241m=\u001b[39m \u001b[43mGspreadHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myour_credentials.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Get a spreadsheet as a DataFrame\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m handler\u001b[38;5;241m.\u001b[39mget_sheet_as_df(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYourSheetName\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYourWorksheetName\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m, in \u001b[0;36mGspreadHandler.__init__\u001b[1;34m(self, credentials_filepath)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcredentials_filepath \u001b[38;5;241m=\u001b[39m credentials_filepath\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Initialize both gspread (for sheets) and Drive API (for other files)\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgc \u001b[38;5;241m=\u001b[39m \u001b[43mgspread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice_account\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcredentials_filepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreds \u001b[38;5;241m=\u001b[39m service_account\u001b[38;5;241m.\u001b[39mCredentials\u001b[38;5;241m.\u001b[39mfrom_service_account_file(\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcredentials_filepath, scopes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.googleapis.com/auth/drive\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrive_service \u001b[38;5;241m=\u001b[39m build(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrive\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv3\u001b[39m\u001b[38;5;124m'\u001b[39m, credentials\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreds)\n",
      "File \u001b[1;32md:\\PycharmProjects\\smart-travels\\.venv\\Lib\\site-packages\\gspread\\auth.py:324\u001b[0m, in \u001b[0;36mservice_account\u001b[1;34m(filename, scopes, http_client)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mservice_account\u001b[39m(\n\u001b[0;32m    296\u001b[0m     filename: Union[Path, \u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m DEFAULT_SERVICE_ACCOUNT_FILENAME,\n\u001b[0;32m    297\u001b[0m     scopes: Iterable[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m DEFAULT_SCOPES,\n\u001b[0;32m    298\u001b[0m     http_client: HTTPClientType \u001b[38;5;241m=\u001b[39m HTTPClient,\n\u001b[0;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Client:\n\u001b[0;32m    300\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Authenticate using a service account.\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \n\u001b[0;32m    302\u001b[0m \u001b[38;5;124;03m    ``scopes`` parameter defaults to read/write scope available in\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;124;03m    :rtype: :class:`gspread.client.Client`\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 324\u001b[0m     creds \u001b[38;5;241m=\u001b[39m \u001b[43mSACredentials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_service_account_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscopes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Client(auth\u001b[38;5;241m=\u001b[39mcreds, http_client\u001b[38;5;241m=\u001b[39mhttp_client)\n",
      "File \u001b[1;32md:\\PycharmProjects\\smart-travels\\.venv\\Lib\\site-packages\\google\\oauth2\\service_account.py:258\u001b[0m, in \u001b[0;36mCredentials.from_service_account_file\u001b[1;34m(cls, filename, **kwargs)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_service_account_file\u001b[39m(\u001b[38;5;28mcls\u001b[39m, filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    248\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a Credentials instance from a service account json file.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03m            credentials.\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m     info, signer \u001b[38;5;241m=\u001b[39m \u001b[43m_service_account_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_filename\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequire\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclient_email\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken_uri\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_signer_and_info(signer, info, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\PycharmProjects\\smart-travels\\.venv\\Lib\\site-packages\\google\\auth\\_service_account_info.py:78\u001b[0m, in \u001b[0;36mfrom_filename\u001b[1;34m(filename, require, use_rsa_signer)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_filename\u001b[39m(filename, require\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, use_rsa_signer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reads a Google service account JSON file and returns its parsed info.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m            info and a signer instance.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[0;32m     79\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(json_file)\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data, from_dict(data, require\u001b[38;5;241m=\u001b[39mrequire, use_rsa_signer\u001b[38;5;241m=\u001b[39muse_rsa_signer)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_credentials.json'"
     ]
    }
   ],
   "source": [
    "handler = GspreadHandler('your_credentials.json')\n",
    "\n",
    "# Get a spreadsheet as a DataFrame\n",
    "df = handler.get_sheet_as_df(\"YourSheetName\", \"YourWorksheetName\")\n",
    "print(df)\n",
    "\n",
    "# List files in a specific folder\n",
    "folder_id = 'your_folder_id'\n",
    "files = handler.list_files(f\"'{folder_id}' in parents\")\n",
    "for file in files:\n",
    "    print(f\"File Name: {file['name']}, ID: {file['id']}\")\n",
    "\n",
    "# Download and process a file\n",
    "file_id = 'your_file_id'\n",
    "file_content = handler.get_file_content(file_id)\n",
    "# ... (Handle the file content based on its type, e.g., CSV, image, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCP drive handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (2.126.0)\n",
      "Requirement already satisfied: google-auth-httplib2 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-python-client) (2.29.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-python-client) (2.18.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-auth-oauthlib) (2.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.63.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.23.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.1.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyDrive2\n",
      "  Downloading PyDrive2-1.19.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: google-api-python-client>=1.12.5 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from PyDrive2) (2.126.0)\n",
      "Requirement already satisfied: oauth2client>=4.0.0 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from PyDrive2) (4.1.3)\n",
      "Requirement already satisfied: PyYAML>=3.0 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from PyDrive2) (6.0.1)\n",
      "Collecting pyOpenSSL>=19.1.0 (from PyDrive2)\n",
      "  Downloading pyOpenSSL-24.1.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-python-client>=1.12.5->PyDrive2) (0.22.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-python-client>=1.12.5->PyDrive2) (2.29.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-python-client>=1.12.5->PyDrive2) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-python-client>=1.12.5->PyDrive2) (2.18.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-python-client>=1.12.5->PyDrive2) (4.1.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from oauth2client>=4.0.0->PyDrive2) (0.6.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from oauth2client>=4.0.0->PyDrive2) (0.4.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from oauth2client>=4.0.0->PyDrive2) (4.9)\n",
      "Requirement already satisfied: six>=1.6.1 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from oauth2client>=4.0.0->PyDrive2) (1.16.0)\n",
      "Collecting cryptography<43,>=41.0.5 (from pyOpenSSL>=19.1.0->PyDrive2)\n",
      "  Downloading cryptography-42.0.7-cp39-abi3-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cffi>=1.12 (from cryptography<43,>=41.0.5->pyOpenSSL>=19.1.0->PyDrive2)\n",
      "  Using cached cffi-1.16.0-cp312-cp312-win_amd64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.5->PyDrive2) (1.63.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.5->PyDrive2) (4.25.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.5->PyDrive2) (1.23.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.5->PyDrive2) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.12.5->PyDrive2) (5.3.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client>=1.12.5->PyDrive2) (3.1.2)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography<43,>=41.0.5->pyOpenSSL>=19.1.0->PyDrive2)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.5->PyDrive2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.5->PyDrive2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.5->PyDrive2) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.5->PyDrive2) (2024.2.2)\n",
      "Downloading PyDrive2-1.19.0-py3-none-any.whl (46 kB)\n",
      "   ---------------------------------------- 0.0/46.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 46.7/46.7 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading pyOpenSSL-24.1.0-py3-none-any.whl (56 kB)\n",
      "   ---------------------------------------- 0.0/56.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 56.9/56.9 kB ? eta 0:00:00\n",
      "Downloading cryptography-42.0.7-cp39-abi3-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.9/2.9 MB 28.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.5/2.9 MB 31.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 30.6 MB/s eta 0:00:00\n",
      "Using cached cffi-1.16.0-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "   ---------------------------------------- 0.0/117.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 117.6/117.6 kB ? eta 0:00:00\n",
      "Installing collected packages: pycparser, cffi, cryptography, pyOpenSSL, PyDrive2\n",
      "Successfully installed PyDrive2-1.19.0 cffi-1.16.0 cryptography-42.0.7 pyOpenSSL-24.1.0 pycparser-2.22\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyDrive2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Oauth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=995857605983-onp1ieqbr9dr12ivrui5igins8trst4d.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=online&response_type=code\n",
      "\n",
      "Authentication successful.\n"
     ]
    }
   ],
   "source": [
    "from pydrive2.auth import GoogleAuth\n",
    "from pydrive2.drive import GoogleDrive\n",
    "\n",
    "gauth = GoogleAuth()\n",
    "gauth.settings['client_config_file'] = r'./client_secrets.json'\n",
    "gauth.LocalWebserverAuth()\n",
    "\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: SW Backend Docs, id: 1U2jWotVN5a3AKoPCt7a-9j-Y3dTj8bmF\n",
      "title: Consultant Agreement - CL and Shaik 16 May 2024, id: 1s0julYLUV0Slyv0nsWIVtg-NvKt-oU_SAqAGt7F5CSo\n",
      "title: give me the arabic script for the following dua i..., id: 1Tz_9NvMw0cEHq7MJe6CP7-cM9SKGCxS70yj8fzOWm5w\n",
      "title: Meeting Summary Proposal, id: 1H22V48Pz6a49umVnboyEwspW9Hfh0Ce9fQWhtJIB8MQ\n",
      "title: Freedom Phoenix Inc x AI Proposal , id: 1dNP4G0O3QyVduLIDerJwmt3lq8oWUN7wGul8S58Ho_4\n",
      "title: CLPL Contract 2, id: 1LIe2-mRgohEUvkeDRkliDU0hOHLx_PNRzekAB_lcYMY\n",
      "title: State of the Data 2, id: 12tp9L6SZlVRmeIKCwfo3PjDvyqXNCJkL2tiSO9kls8I\n",
      "title: Singtel Data Scientist Cover Letter, id: 1Zfh0z9bjWlxFbwN0NvbskJzGNI9hSTGiIHZHCMwx-WI\n",
      "title: Consultant Agreement - CL and Shaik 2.docx, id: 1333kHUyQgVO0oLZhG06L9SXiuvlN88sg\n",
      "title: Invoice No: 002, id: 1nH6Mrakv8DKTDGb4O5rS0PdwffV1lwb_h1oG-tjEJuE\n",
      "title: Untitled document, id: 1lxUZiSj9yaokcUB-gQQx-Dq17zisqA_ZYdQR7s1pjnk\n",
      "title: STATE OF THE DATA 2  Steps forward, id: 1BPcCUNqfYujhECTgjGazkkeyTRyvuOHbvNMHsnoxfXg\n",
      "title: so now Sue says she wants to \"We need to start wi..., id: 1_n2QkfPTqa7vNGwCHVng49idCOmWF30o5pNhHxCjfsw\n",
      "title: SHAIK REZA SHAFIQ, id: 14S3hsahPPhYFNl8peFz7bCZ2i4qQ51-9hphXvAMKQHQ\n",
      "title: ​***(I) Revised RAG Proposals***, id: 1NCJmY_yJFuKvaaLLi1RPXdH7bbJi8mqo5kKJ2dHEp6k\n",
      "title: Untitled document, id: 1F4PiFUttPvsxWHRLVxBqO_ZaN9pDxeSW6X-97ZEg9rQ\n",
      "title: JustBento Jurong Food Photoshoot, id: 1Pjc4xGVymT1acPLw4ytmjAViJdDZjwXA\n",
      "title: please help break down the longer portions of the..., id: 1u48TSlvBX8WaSIKv4hBY-u7AbA1WFsUNzjOhPpjwsUk\n",
      "title: You are an organiser of duas, that help build a t..., id: 1BY3nPH4o33nyX8pqYJc4swNyjLF2k5Lg2-_YF-L_fpo\n",
      "title: SWTT DB, id: 11GQ4pLXNffuEZH7r2d0LzBA4rjX7NviR7u-pbwjoFVY\n",
      "title: help tabulate the following conversation into a t..., id: 1g4ayFTNnB7jq0EWjO667d5zaCgGYPu_Gi19dnUuhPEo\n",
      "title: transpose this table. such that the arabic is the..., id: 1vCDzumj-4nZPHsYaiGrm1ukEpVnCKkEonm-bNxkxZ5Q\n",
      "title: AI Consultancy, id: 1KtywmNa1GX1EYLZRVyLQrb8fgC2otfq0\n",
      "title: make it into more rows, (for each comma), id: 1fK0ylS9-kcf6jxL7zCFyULdSRldoRuGDf2Jzcabl0Gw\n",
      "title: help me write a table: consisting of columns: ara..., id: 1mWvrkOXjSzb_pqLSpsyG6XI-JEOBoG0KX6NfmKjbU34\n",
      "title: Hisn Al Muslim, id: 17ljfB64kSVyQ6X4lkxP-ntuIW-PA6UoxpUeK8FujOrM\n",
      "title: bukhari, id: 1DcEzi6i3314UqD0EEu_46TkQySOiW8AXMqoKOFuKFbA\n",
      "title: Summary of Data Audit - Smart World Travels, id: 1achhUajrebWWkUYc0Ig22RYlkYwcLKu_Mk6U-JA9Dkg\n",
      "title: Untitled document, id: 1-MurY-TbkhGrdUI1ta1spHiVF_z5y2BGXoNxLP8wlP4\n",
      "title: STATE OF THE DATA A high-level overview of current state of data, id: 1klkHlfw2tgPAJg-VB6lrKXARv5TlMNZeFFmn17J0Ops\n",
      "title: Untitled document, id: 1kzsNNRyidVX86Fcso4lVeYGGUVoq0x0DWn4zEsaoFm8\n",
      "title: TN, id: 1tDGkgeb-2Vh628iMpgBq9cOlmcmiq52T\n",
      "title: White Paper -  SmartWorldTravel, id: 1u6eH9lkSfwFtuHAFhjAl26EHdqImwcjnDg2ZypFFgok\n",
      "title: Untitled document, id: 12-7XB6Na-EuxUX5p6htcW04M2NHmQrKUP-kl8Zftm8c\n",
      "title: White Paper -  Odela, id: 16JcD4D08onDKlW5GJbKQiJHzhyc5C8Hm3Tw8ec60CPM\n",
      "title: Google AI Studio, id: 1mloXMfoGhl8sJOmcviIiXe3AbabM5yN3\n",
      "title: The Mid G, id: 1P3TjlqwYUTFrEoYmkElVgIAaNep8E75O\n",
      "title: Im a budding creator with 4k views over a week- im producing dawah, islam, and self improvement videos in the style of mohammed hijab or andrew tate-  give me a table of rows: facebook, youtube, insta, tiktok with colums: best time range(in SGT) to post to maximise views given my creator profile, major country with social media presence, id: 1dChTYMy6nEdrIJHwOfX0Prj-MMd--84TKv_AshCUapo\n",
      "title: Shaiks photos, id: 1-jyCRDe18xHwKJ8VeFALh-6aKkd0-nDz\n",
      "title: Buku Wirid & Doa PHM 2018.pdf, id: 1w9sx1y0mAbvs5z9_OrfnDGB4Z5WRQusP\n",
      "title: Hello, please give me in excel table format, the target_age, target_annual_income, target_gender of all the shops in the list given here:\n",
      "Name:\n",
      "EACH A CUP\n",
      "ECCO\n",
      "FENDI\n",
      "FRESH\n",
      "GIORGIO ARMANI BEAUTY\n",
      "LAC\n",
      "GUARDIAN B4\n",
      "GUCCI\n",
      "HARRY WINSTON\n",
      "HOSHINO COFFEE\n",
      "HYSSES\n",
      "ISLAND CREAMERY\n",
      "JUMBO SEAFOOD\n",
      "KANSHOKU RAMEN BAR\n",
      "KOI THE\n",
      "LEE HWA DIAMOND PROMENADE\n",
      "CHANEL\n",
      "LORO PIANA, id: 1zzz2vsq1DCFYiRMnH8GpUgXHBbRU9_ubzuYD0k6wMig\n",
      "title: Hello, please give me in excel table format, the target_age, target_annual_income, target_gender of all the shops in the list given here:\n",
      "Name:\n",
      "LETAO / TOKYO MILK CHEESE FACTORY\n",
      "BIMBA Y LOLA\n",
      "DESIGUAL\n",
      "V-ZUG\n",
      "H&M\n",
      "FITFLOP\n",
      "APM MONACO\n",
      "DIOR 01-06\n",
      "SINGAPORE POST\n",
      "I-PRIMO\n",
      "CROCS KIOSK\n",
      "SMILE MARTABAK\n",
      "JD SPORTS\n",
      "LE MATIN PATISSERIE\n",
      "AESOP\n",
      "OWNDAYS\n",
      "DEVIALET\n",
      "7-ELEVEN\n",
      "A. LANGE & SOHNE\n",
      "ADIDAS\n",
      "AESTHETIC BAY\n",
      "BENGAWAN SOLO\n",
      "BVLGARI\n",
      "CALVIN KLEIN\n",
      "CHRISTIAN DIOR PARIS\n",
      "CLARINS\n",
      "COS\n",
      "COTTON ON BODY\n",
      "CROCS\n",
      "DAISO\n",
      "DIOR HOMME\n",
      "EACH A CUP\n",
      "ECCO\n",
      "FENDI\n",
      "FRESH\n",
      "GIORGIO ARMANI BEAUTY\n",
      "LAC\n",
      "GUARDIAN B4\n",
      "GUCCI\n",
      "HARRY WINSTON\n",
      "HOSHINO COFFEE\n",
      "HYSSES\n",
      "ISLAND CREAMERY\n",
      "JUMBO SEAFOOD\n",
      "KANSHOKU RAMEN BAR\n",
      "KOI THE\n",
      "LEE HWA DIAMOND PROMENADE\n",
      "CHANEL\n",
      "LORO PIANA, id: 1uEz33JEOEObazwOELrmNtNCxjSAm6-G6kS_UdklLVCI\n",
      "title: Hello, please give me in excel table format, the target_age, target_annual_income, target_gender of all the shops in the list given here:\n",
      "Name:\n",
      "G2000\n",
      "SUNNY STEP\n",
      "JERIC SALON\n",
      "CHANEL PRIVE\n",
      "NB 1906\n",
      "GUCCI BEAUTY\n",
      "VACHERON CONSTANTIN\n",
      "ROYAL SELANGOR\n",
      "CARTIER\n",
      "STELLAR\n",
      "MIZU AESTHETIC CLINIC + AERAS\n",
      "FILA\n",
      "NAILZ TREATS\n",
      "PUZZLE COFFEE\n",
      "THE ONITSUKA\n",
      "LEICA\n",
      "NESPRESSO\n",
      "G-SHOCK CASIO\n",
      "CLARKS\n",
      "MASSIMO DUTTI\n",
      "GOLDHEART JEWEL GALLERIA\n",
      "SURREY HILLS GROCER\n",
      "GUZMAN Y GOMEZ MEXICAN KITCHEN\n",
      "HAIR ATELIER BY SHUNJI MATSUO\n",
      "LA SENZA, id: 1Yoj7rKfJ0aLUKRfkd98RZuiAVCf6qjAlwmgyaN3uH2M\n",
      "title: Hello, please give me in excel table format, the target_age, target_annual_income, target_gender of all the shops in the list given here:\n",
      "Name:\n",
      "SANOOK KITCHEN\n",
      "PICOTA NAIL SPA\n",
      "KLARRA\n",
      "PINKO\n",
      "FIVE GUYS\n",
      "PLAIN VANILLA\n",
      "FRANCK MULLER\n",
      "PUTIEN\n",
      "Citibank ATM\n",
      "COULISSE HEIR\n",
      "RATIO CAFE + GASTROBAR\n",
      "DOLCE & GABBANA\n",
      "LUCKY DUMPLINGS\n",
      "GUERLAIN\n",
      "& OTHER STORIES\n",
      "SALLY'S\n",
      "YUMMY TUMMY\n",
      "COCOMI\n",
      "TUMI\n",
      "SULWHASOO\n",
      "YUN NANS RESTAURANT\n",
      "SPA ELEMENTS\n",
      "VENCHI\n",
      "COTTON ON\n",
      "FRED PERRY\n",
      "2XU, id: 1YevJk25osrDHh2OaA-wyT1OvsO0rrPP0L3N8QWPDoHg\n",
      "title: Hello, please give me in excel table format, the target_age, target_annual_income, target_gender of all the shops in the list given here:\n",
      "Name:\n",
      "GLANCE\n",
      "L'OCCITANE\n",
      "SAMSUNG\n",
      "LAB SERIES\n",
      "SWEATY BETTY LONDON\n",
      "SUPERGURL\n",
      "RISIS\n",
      "LEGO\n",
      "SPACIO TCM WELLNESS\n",
      "LA PRAIRIE\n",
      "LEVI'S\n",
      "MONICA VINADER\n",
      "DIOR PRESTIGE LA SUITE\n",
      "MONSTER CURRY\n",
      "PARIS BAGUETTE\n",
      "CHALLENGER\n",
      "LAVENDER\n",
      "CATH KIDSTON\n",
      "BY INVITE ONLY\n",
      "NIKU KAPPO BY WATAMI\n",
      "PANERAI\n",
      "OMEGA\n",
      "OPERA GALLERY\n",
      "ORIGINS\n",
      "SUNDAY FOLKS\n",
      "PEDRO\n",
      "TWO LIPS\n",
      "PENHALIGON'S\n",
      "KEE WAH BAKERY\n",
      "PEZZO\n",
      "SANOOK KITCHEN\n",
      "PICOTA NAIL SPA\n",
      "KLARRA\n",
      "PINKO\n",
      "FIVE GUYS\n",
      "PLAIN VANILLA\n",
      "FRANCK MULLER\n",
      "PUTIEN\n",
      "Citibank ATM\n",
      "COULISSE HEIR\n",
      "RATIO CAFE + GASTROBAR\n",
      "DOLCE & GABBANA\n",
      "LUCKY DUMPLINGS\n",
      "GUERLAIN\n",
      "& OTHER STORIES\n",
      "SALLY'S\n",
      "YUMMY TUMMY\n",
      "COCOMI\n",
      "TUMI\n",
      "SULWHASOO\n",
      "YUN NANS RESTAURANT\n",
      "SPA ELEMENTS\n",
      "VENCHI\n",
      "COTTON ON\n",
      "FRED PERRY\n",
      "2XU\n",
      "G2000\n",
      "SUNNY STEP\n",
      "JERIC SALON\n",
      "CHANEL PRIVE\n",
      "NB 1906\n",
      "GUCCI BEAUTY\n",
      "VACHERON CONSTANTIN\n",
      "ROYAL SELANGOR\n",
      "CARTIER\n",
      "STELLAR\n",
      "MIZU AESTHETIC CLINIC + AERAS\n",
      "FILA\n",
      "NAILZ TREATS\n",
      "PUZZLE COFFEE\n",
      "THE ONITSUKA\n",
      "LEICA\n",
      "NESPRESSO\n",
      "G-SHOCK CASIO\n",
      "CLARKS\n",
      "MASSIMO DUTTI\n",
      "GOLDHEART JEWEL GALLERIA\n",
      "SURREY HILLS GROCER\n",
      "GUZMAN Y GOMEZ MEXICAN KITCHEN\n",
      "HAIR ATELIER BY SHUNJI MATSUO\n",
      "LA SENZA\n",
      "LETAO / TOKYO MILK CHEESE FACTORY\n",
      "BIMBA Y LOLA\n",
      "DESIGUAL\n",
      "V-ZUG\n",
      "H&M\n",
      "FITFLOP\n",
      "APM MONACO\n",
      "DIOR 01-06\n",
      "SINGAPORE POST\n",
      "I-PRIMO\n",
      "CROCS KIOSK\n",
      "SMILE MARTABAK\n",
      "JD SPORTS\n",
      "LE MATIN PATISSERIE\n",
      "AESOP\n",
      "OWNDAYS\n",
      "DEVIALET\n",
      "7-ELEVEN\n",
      "A. LANGE & SOHNE\n",
      "ADIDAS\n",
      "AESTHETIC BAY\n",
      "BENGAWAN SOLO\n",
      "BVLGARI\n",
      "CALVIN KLEIN\n",
      "CHRISTIAN DIOR PARIS\n",
      "CLARINS\n",
      "COS\n",
      "COTTON ON BODY\n",
      "CROCS\n",
      "DAISO\n",
      "DIOR HOMME\n",
      "EACH A CUP\n",
      "ECCO\n",
      "FENDI\n",
      "FRESH\n",
      "GIORGIO ARMANI BEAUTY\n",
      "LAC\n",
      "GUARDIAN B4\n",
      "GUCCI\n",
      "HARRY WINSTON\n",
      "HOSHINO COFFEE\n",
      "HYSSES\n",
      "ISLAND CREAMERY\n",
      "JUMBO SEAFOOD\n",
      "KANSHOKU RAMEN BAR\n",
      "KOI THE\n",
      "LEE HWA DIAMOND PROMENADE\n",
      "CHANEL\n",
      "LORO PIANA, id: 1J0PPhF8_n0h43CF9OtJyC2ROPPaooMaoO56JVDVEK14\n",
      "title: Hello, please give me in excel table format, the target_age, target_annual_income, target_gender of all the shops in the list given here:\n",
      "Name:\n",
      "LOVE, BONITO\n",
      "GENTLE MONSTER\n",
      "SEN-RYO\n",
      "PARFUMS CHRISTIAN DIOR\n",
      "GRAF VON FABER-CASTELL\n",
      "GRAFF\n",
      "AMERICAN VINTAGE\n",
      "RIVE GAUCHE PATISSERIE\n",
      "SHU UEMURA\n",
      "LADY M\n",
      "HEYTEA 喜茶\n",
      "HUGO\n",
      "IMPERIAL TREASURE FINE TEOCHEW CUISINE\n",
      "ELEMIS LONDON\n",
      "IMPERIAL TREASURE STEAMBOAT RESTAURANT\n",
      "IN GOOD COMPANY\n",
      "HAWKERS' STREET\n",
      "BEYOND THE VINES\n",
      "ITACHO SUSHI\n",
      "NATURELAND SPA. PREMIUM\n",
      "IWC SCHAFFHAUSEN\n",
      "TEMPURA TENDON TENYA\n",
      "JENNIFER GREEN\n",
      "JO MALONE LONDON\n",
      "THE HISTORY OF WHOO\n",
      "KATE SPADE\n",
      "KIEHL'S SINCE 1851\n",
      "HUBLOT\n",
      "SU:M37\n",
      "L'ATELIER BY THE HOUR GLASS\n",
      "GLANCE\n",
      "L'OCCITANE\n",
      "SAMSUNG\n",
      "LAB SERIES\n",
      "SWEATY BETTY LONDON\n",
      "SUPERGURL\n",
      "RISIS\n",
      "LEGO\n",
      "SPACIO TCM WELLNESS\n",
      "LA PRAIRIE\n",
      "LEVI'S\n",
      "MONICA VINADER\n",
      "DIOR PRESTIGE LA SUITE\n",
      "MONSTER CURRY\n",
      "PARIS BAGUETTE\n",
      "CHALLENGER\n",
      "LAVENDER\n",
      "CATH KIDSTON\n",
      "BY INVITE ONLY\n",
      "NIKU KAPPO BY WATAMI\n",
      "PANERAI\n",
      "OMEGA\n",
      "OPERA GALLERY\n",
      "ORIGINS\n",
      "SUNDAY FOLKS\n",
      "PEDRO\n",
      "TWO LIPS\n",
      "PENHALIGON'S\n",
      "KEE WAH BAKERY\n",
      "PEZZO\n",
      "SANOOK KITCHEN\n",
      "PICOTA NAIL SPA\n",
      "KLARRA\n",
      "PINKO\n",
      "FIVE GUYS\n",
      "PLAIN VANILLA\n",
      "FRANCK MULLER\n",
      "PUTIEN\n",
      "Citibank ATM\n",
      "COULISSE HEIR\n",
      "RATIO CAFE + GASTROBAR\n",
      "DOLCE & GABBANA\n",
      "LUCKY DUMPLINGS\n",
      "GUERLAIN\n",
      "& OTHER STORIES\n",
      "SALLY'S\n",
      "YUMMY TUMMY\n",
      "COCOMI\n",
      "TUMI\n",
      "SULWHASOO\n",
      "YUN NANS RESTAURANT\n",
      "SPA ELEMENTS\n",
      "VENCHI\n",
      "COTTON ON\n",
      "FRED PERRY\n",
      "2XU\n",
      "G2000\n",
      "SUNNY STEP\n",
      "JERIC SALON\n",
      "CHANEL PRIVE\n",
      "NB 1906\n",
      "GUCCI BEAUTY\n",
      "VACHERON CONSTANTIN\n",
      "ROYAL SELANGOR\n",
      "CARTIER\n",
      "STELLAR\n",
      "MIZU AESTHETIC CLINIC + AERAS\n",
      "FILA\n",
      "NAILZ TREATS\n",
      "PUZZLE COFFEE\n",
      "THE ONITSUKA\n",
      "LEICA\n",
      "NESPRESSO\n",
      "G-SHOCK CASIO\n",
      "CLARKS\n",
      "MASSIMO DUTTI\n",
      "GOLDHEART JEWEL GALLERIA\n",
      "SURREY HILLS GROCER\n",
      "GUZMAN Y GOMEZ MEXICAN KITCHEN\n",
      "HAIR ATELIER BY SHUNJI MATSUO\n",
      "LA SENZA\n",
      "LETAO / TOKYO MILK CHEESE FACTORY\n",
      "BIMBA Y LOLA\n",
      "DESIGUAL\n",
      "V-ZUG\n",
      "H&M\n",
      "FITFLOP\n",
      "APM MONACO\n",
      "DIOR 01-06\n",
      "SINGAPORE POST\n",
      "I-PRIMO\n",
      "CROCS KIOSK\n",
      "SMILE MARTABAK\n",
      "JD SPORTS\n",
      "LE MATIN PATISSERIE\n",
      "AESOP\n",
      "OWNDAYS\n",
      "DEVIALET\n",
      "7-ELEVEN\n",
      "A. LANGE & SOHNE\n",
      "ADIDAS\n",
      "AESTHETIC BAY\n",
      "BENGAWAN SOLO\n",
      "BVLGARI\n",
      "CALVIN KLEIN\n",
      "CHRISTIAN DIOR PARIS\n",
      "CLARINS\n",
      "COS\n",
      "COTTON ON BODY\n",
      "CROCS\n",
      "DAISO\n",
      "DIOR HOMME\n",
      "EACH A CUP\n",
      "ECCO\n",
      "FENDI\n",
      "FRESH\n",
      "GIORGIO ARMANI BEAUTY\n",
      "LAC\n",
      "GUARDIAN B4\n",
      "GUCCI\n",
      "HARRY WINSTON\n",
      "HOSHINO COFFEE\n",
      "HYSSES\n",
      "ISLAND CREAMERY\n",
      "JUMBO SEAFOOD\n",
      "KANSHOKU RAMEN BAR\n",
      "KOI THE\n",
      "LEE HWA DIAMOND PROMENADE\n",
      "CHANEL\n",
      "LORO PIANA, id: 141mkTxsVrxTwC3yUJRN4Rx43BROCb4XX1qPZ7637EGc\n",
      "title: Hello, please give me in excel table format, the target_age, target_annual_income, target_gender of all the shops in the list given here:\n",
      "Name:\n",
      "SEPHORA\n",
      "CLARINS SKIN SPA\n",
      "SANDRO\n",
      "SHISEIDO\n",
      "SAMSONITE\n",
      "BATH & BODY WORKS\n",
      "ESCENTIALS\n",
      "JASONS DELI\n",
      "ROLEX\n",
      "RED ARMY WATCHES\n",
      "CONVERSE\n",
      "ALEXANDER MCQUEEN\n",
      "CRATE & BARREL\n",
      "JOY LUCK TEAHOUSE\n",
      "DIOR\n",
      "ESTEE LAUDER\n",
      "ROYCE'\n",
      "DUNKIN' DONUTS\n",
      "LOEWE\n",
      "THE BODY SHOP\n",
      "ENVIE DE POIS\n",
      "TUDOR\n",
      "EU YAN SANG\n",
      "HANG HEUNG HONG KONG\n",
      "108 MATCHA SARO\n",
      "CLUB MONACO\n",
      "FOOD OPERA\n",
      "SIGNATURE KOI\n",
      "FOSSIL\n",
      "1-ATICO by 1-ALTITUDE\n",
      "LOVE, BONITO\n",
      "GENTLE MONSTER\n",
      "SEN-RYO\n",
      "PARFUMS CHRISTIAN DIOR\n",
      "GRAF VON FABER-CASTELL\n",
      "GRAFF\n",
      "AMERICAN VINTAGE\n",
      "RIVE GAUCHE PATISSERIE\n",
      "SHU UEMURA\n",
      "LADY M\n",
      "HEYTEA 喜茶\n",
      "HUGO\n",
      "IMPERIAL TREASURE FINE TEOCHEW CUISINE\n",
      "ELEMIS LONDON\n",
      "IMPERIAL TREASURE STEAMBOAT RESTAURANT\n",
      "IN GOOD COMPANY\n",
      "HAWKERS' STREET\n",
      "BEYOND THE VINES\n",
      "ITACHO SUSHI\n",
      "NATURELAND SPA. PREMIUM\n",
      "IWC SCHAFFHAUSEN\n",
      "TEMPURA TENDON TENYA\n",
      "JENNIFER GREEN\n",
      "JO MALONE LONDON\n",
      "THE HISTORY OF WHOO\n",
      "KATE SPADE\n",
      "KIEHL'S SINCE 1851\n",
      "HUBLOT\n",
      "SU:M37\n",
      "L'ATELIER BY THE HOUR GLASS\n",
      "GLANCE\n",
      "L'OCCITANE\n",
      "SAMSUNG\n",
      "LAB SERIES\n",
      "SWEATY BETTY LONDON\n",
      "SUPERGURL\n",
      "RISIS\n",
      "LEGO\n",
      "SPACIO TCM WELLNESS\n",
      "LA PRAIRIE\n",
      "LEVI'S\n",
      "MONICA VINADER\n",
      "DIOR PRESTIGE LA SUITE\n",
      "MONSTER CURRY\n",
      "PARIS BAGUETTE\n",
      "CHALLENGER\n",
      "LAVENDER\n",
      "CATH KIDSTON\n",
      "BY INVITE ONLY\n",
      "NIKU KAPPO BY WATAMI\n",
      "PANERAI\n",
      "OMEGA\n",
      "OPERA GALLERY\n",
      "ORIGINS\n",
      "SUNDAY FOLKS\n",
      "PEDRO\n",
      "TWO LIPS\n",
      "PENHALIGON'S\n",
      "KEE WAH BAKERY\n",
      "PEZZO\n",
      "SANOOK KITCHEN\n",
      "PICOTA NAIL SPA\n",
      "KLARRA\n",
      "PINKO\n",
      "FIVE GUYS\n",
      "PLAIN VANILLA\n",
      "FRANCK MULLER\n",
      "PUTIEN\n",
      "Citibank ATM\n",
      "COULISSE HEIR\n",
      "RATIO CAFE + GASTROBAR\n",
      "DOLCE & GABBANA\n",
      "LUCKY DUMPLINGS\n",
      "GUERLAIN\n",
      "& OTHER STORIES\n",
      "SALLY'S\n",
      "YUMMY TUMMY\n",
      "COCOMI\n",
      "TUMI\n",
      "SULWHASOO\n",
      "YUN NANS RESTAURANT\n",
      "SPA ELEMENTS\n",
      "VENCHI\n",
      "COTTON ON\n",
      "FRED PERRY\n",
      "2XU\n",
      "G2000\n",
      "SUNNY STEP\n",
      "JERIC SALON\n",
      "CHANEL PRIVE\n",
      "NB 1906\n",
      "GUCCI BEAUTY\n",
      "VACHERON CONSTANTIN\n",
      "ROYAL SELANGOR\n",
      "CARTIER\n",
      "STELLAR\n",
      "MIZU AESTHETIC CLINIC + AERAS\n",
      "FILA\n",
      "NAILZ TREATS\n",
      "PUZZLE COFFEE\n",
      "THE ONITSUKA\n",
      "LEICA\n",
      "NESPRESSO\n",
      "G-SHOCK CASIO\n",
      "CLARKS\n",
      "MASSIMO DUTTI\n",
      "GOLDHEART JEWEL GALLERIA\n",
      "SURREY HILLS GROCER\n",
      "GUZMAN Y GOMEZ MEXICAN KITCHEN\n",
      "HAIR ATELIER BY SHUNJI MATSUO\n",
      "LA SENZA\n",
      "LETAO / TOKYO MILK CHEESE FACTORY\n",
      "BIMBA Y LOLA\n",
      "DESIGUAL\n",
      "V-ZUG\n",
      "H&M\n",
      "FITFLOP\n",
      "APM MONACO\n",
      "DIOR 01-06\n",
      "SINGAPORE POST\n",
      "I-PRIMO\n",
      "CROCS KIOSK\n",
      "SMILE MARTABAK\n",
      "JD SPORTS\n",
      "LE MATIN PATISSERIE\n",
      "AESOP\n",
      "OWNDAYS\n",
      "DEVIALET\n",
      "7-ELEVEN\n",
      "A. LANGE & SOHNE\n",
      "ADIDAS\n",
      "AESTHETIC BAY\n",
      "BENGAWAN SOLO\n",
      "BVLGARI\n",
      "CALVIN KLEIN\n",
      "CHRISTIAN DIOR PARIS\n",
      "CLARINS\n",
      "COS\n",
      "COTTON ON BODY\n",
      "CROCS\n",
      "DAISO\n",
      "DIOR HOMME\n",
      "EACH A CUP\n",
      "ECCO\n",
      "FENDI\n",
      "FRESH\n",
      "GIORGIO ARMANI BEAUTY\n",
      "LAC\n",
      "GUARDIAN B4\n",
      "GUCCI\n",
      "HARRY WINSTON\n",
      "HOSHINO COFFEE\n",
      "HYSSES\n",
      "ISLAND CREAMERY\n",
      "JUMBO SEAFOOD\n",
      "KANSHOKU RAMEN BAR\n",
      "KOI THE\n",
      "LEE HWA DIAMOND PROMENADE\n",
      "CHANEL\n",
      "LORO PIANA, id: 1TDoLJ3J-yn4Ab_9qTi1oNKTvTx3vK_vofgaTeluZCZs\n",
      "title: Hello, please give me in excel table format, the target_age, target_annual_income, target_gender of all the shops in the list given here:\n",
      "Name:\n",
      "THE WHISKY DISTILLERY\n",
      "RUBI SHOES\n",
      "THE OAKS CELLARS\n",
      "BREITLING\n",
      "THE NORTH FACE\n",
      "MAJE\n",
      "RABEANCO\n",
      "LEE WEE & BROTHERS\n",
      "ZARA\n",
      "RAY-BAN\n",
      "THE 1872 CLIPPER TEA CO.\n",
      "RAMEN NAGI\n",
      "TED BAKER LONDON\n",
      "VALENTINO\n",
      "TASTE PARADISE\n",
      "BACHA COFFEE\n",
      "TAG HEUER\n",
      "ROGER DUBUIS\n",
      "TAKA JEWELLERY TREASURES\n",
      "SWAROVSKI\n",
      "SUITSUPPLY\n",
      "GUARDIAN B2\n",
      "STEINWAY & SONS\n",
      "STARBUCKS COFFEE\n",
      "EVISU\n",
      "POULET + BRASSERIE\n",
      "SKIN INC SUPPLEMENT BAR\n",
      "N.CAT\n",
      "SINGAPORE AIRLINES SERVICE CENTRE\n",
      "YVES SAINT LAURENT BEAUTE\n",
      "SEPHORA\n",
      "CLARINS SKIN SPA\n",
      "SANDRO\n",
      "SHISEIDO\n",
      "SAMSONITE\n",
      "BATH & BODY WORKS\n",
      "ESCENTIALS\n",
      "JASONS DELI\n",
      "ROLEX\n",
      "RED ARMY WATCHES\n",
      "CONVERSE\n",
      "ALEXANDER MCQUEEN\n",
      "CRATE & BARREL\n",
      "JOY LUCK TEAHOUSE\n",
      "DIOR\n",
      "ESTEE LAUDER\n",
      "ROYCE'\n",
      "DUNKIN' DONUTS\n",
      "LOEWE\n",
      "THE BODY SHOP\n",
      "ENVIE DE POIS\n",
      "TUDOR\n",
      "EU YAN SANG\n",
      "HANG HEUNG HONG KONG\n",
      "108 MATCHA SARO\n",
      "CLUB MONACO\n",
      "FOOD OPERA\n",
      "SIGNATURE KOI\n",
      "FOSSIL\n",
      "1-ATICO by 1-ALTITUDE\n",
      "LOVE, BONITO\n",
      "GENTLE MONSTER\n",
      "SEN-RYO\n",
      "PARFUMS CHRISTIAN DIOR\n",
      "GRAF VON FABER-CASTELL\n",
      "GRAFF\n",
      "AMERICAN VINTAGE\n",
      "RIVE GAUCHE PATISSERIE\n",
      "SHU UEMURA\n",
      "LADY M\n",
      "HEYTEA 喜茶\n",
      "HUGO\n",
      "IMPERIAL TREASURE FINE TEOCHEW CUISINE\n",
      "ELEMIS LONDON\n",
      "IMPERIAL TREASURE STEAMBOAT RESTAURANT\n",
      "IN GOOD COMPANY\n",
      "HAWKERS' STREET\n",
      "BEYOND THE VINES\n",
      "ITACHO SUSHI\n",
      "NATURELAND SPA. PREMIUM\n",
      "IWC SCHAFFHAUSEN\n",
      "TEMPURA TENDON TENYA\n",
      "JENNIFER GREEN\n",
      "JO MALONE LONDON\n",
      "THE HISTORY OF WHOO\n",
      "KATE SPADE\n",
      "KIEHL'S SINCE 1851\n",
      "HUBLOT\n",
      "SU:M37\n",
      "L'ATELIER BY THE HOUR GLASS\n",
      "GLANCE\n",
      "L'OCCITANE\n",
      "SAMSUNG\n",
      "LAB SERIES\n",
      "SWEATY BETTY LONDON\n",
      "SUPERGURL\n",
      "RISIS\n",
      "LEGO\n",
      "SPACIO TCM WELLNESS\n",
      "LA PRAIRIE\n",
      "LEVI'S\n",
      "MONICA VINADER\n",
      "DIOR PRESTIGE LA SUITE\n",
      "MONSTER CURRY\n",
      "PARIS BAGUETTE\n",
      "CHALLENGER\n",
      "LAVENDER\n",
      "CATH KIDSTON\n",
      "BY INVITE ONLY\n",
      "NIKU KAPPO BY WATAMI\n",
      "PANERAI\n",
      "OMEGA\n",
      "OPERA GALLERY\n",
      "ORIGINS\n",
      "SUNDAY FOLKS\n",
      "PEDRO\n",
      "TWO LIPS\n",
      "PENHALIGON'S\n",
      "KEE WAH BAKERY\n",
      "PEZZO\n",
      "SANOOK KITCHEN\n",
      "PICOTA NAIL SPA\n",
      "KLARRA\n",
      "PINKO\n",
      "FIVE GUYS\n",
      "PLAIN VANILLA\n",
      "FRANCK MULLER\n",
      "PUTIEN\n",
      "Citibank ATM\n",
      "COULISSE HEIR\n",
      "RATIO CAFE + GASTROBAR\n",
      "DOLCE & GABBANA\n",
      "LUCKY DUMPLINGS\n",
      "GUERLAIN\n",
      "& OTHER STORIES\n",
      "SALLY'S\n",
      "YUMMY TUMMY\n",
      "COCOMI\n",
      "TUMI\n",
      "SULWHASOO\n",
      "YUN NANS RESTAURANT\n",
      "SPA ELEMENTS\n",
      "VENCHI\n",
      "COTTON ON\n",
      "FRED PERRY\n",
      "2XU\n",
      "G2000\n",
      "SUNNY STEP\n",
      "JERIC SALON\n",
      "CHANEL PRIVE\n",
      "NB 1906\n",
      "GUCCI BEAUTY\n",
      "VACHERON CONSTANTIN\n",
      "ROYAL SELANGOR\n",
      "CARTIER\n",
      "STELLAR\n",
      "MIZU AESTHETIC CLINIC + AERAS\n",
      "FILA\n",
      "NAILZ TREATS\n",
      "PUZZLE COFFEE\n",
      "THE ONITSUKA\n",
      "LEICA\n",
      "NESPRESSO\n",
      "G-SHOCK CASIO\n",
      "CLARKS\n",
      "MASSIMO DUTTI\n",
      "GOLDHEART JEWEL GALLERIA\n",
      "SURREY HILLS GROCER\n",
      "GUZMAN Y GOMEZ MEXICAN KITCHEN\n",
      "HAIR ATELIER BY SHUNJI MATSUO\n",
      "LA SENZA\n",
      "LETAO / TOKYO MILK CHEESE FACTORY\n",
      "BIMBA Y LOLA\n",
      "DESIGUAL\n",
      "V-ZUG\n",
      "H&M\n",
      "FITFLOP\n",
      "APM MONACO\n",
      "DIOR 01-06\n",
      "SINGAPORE POST\n",
      "I-PRIMO\n",
      "CROCS KIOSK\n",
      "SMILE MARTABAK\n",
      "JD SPORTS\n",
      "LE MATIN PATISSERIE\n",
      "AESOP\n",
      "OWNDAYS\n",
      "DEVIALET\n",
      "7-ELEVEN\n",
      "A. LANGE & SOHNE\n",
      "ADIDAS\n",
      "AESTHETIC BAY\n",
      "BENGAWAN SOLO\n",
      "BVLGARI\n",
      "CALVIN KLEIN\n",
      "CHRISTIAN DIOR PARIS\n",
      "CLARINS\n",
      "COS\n",
      "COTTON ON BODY\n",
      "CROCS\n",
      "DAISO\n",
      "DIOR HOMME\n",
      "EACH A CUP\n",
      "ECCO\n",
      "FENDI\n",
      "FRESH\n",
      "GIORGIO ARMANI BEAUTY\n",
      "LAC\n",
      "GUARDIAN B4\n",
      "GUCCI\n",
      "HARRY WINSTON\n",
      "HOSHINO COFFEE\n",
      "HYSSES\n",
      "ISLAND CREAMERY\n",
      "JUMBO SEAFOOD\n",
      "KANSHOKU RAMEN BAR\n",
      "KOI THE\n",
      "LEE HWA DIAMOND PROMENADE\n",
      "CHANEL\n",
      "LORO PIANA, id: 1MWTiP4NZvuaLU1w5qbn2YlE338jBrNX-Ay3SjMkaxOg\n",
      "title: Hello, please give me in excel table format, the target_age, target_annual_income, target_gender of all the shops in the list given here:\n",
      "Name:\n",
      "THE WHISKY DISTILLERY\n",
      "RUBI SHOES\n",
      "THE OAKS CELLARS\n",
      "BREITLING\n",
      "THE NORTH FACE\n",
      "MAJE\n",
      "RABEANCO\n",
      "LEE WEE & BROTHERS\n",
      "ZARA\n",
      "RAY-BAN\n",
      "THE 1872 CLIPPER TEA CO.\n",
      "RAMEN NAGI\n",
      "TED BAKER LONDON\n",
      "VALENTINO\n",
      "TASTE PARADISE\n",
      "BACHA COFFEE\n",
      "TAG HEUER\n",
      "ROGER DUBUIS\n",
      "TAKA JEWELLERY TREASURES\n",
      "SWAROVSKI\n",
      "SUITSUPPLY\n",
      "GUARDIAN B2\n",
      "STEINWAY & SONS\n",
      "STARBUCKS COFFEE\n",
      "EVISU\n",
      "POULET + BRASSERIE\n",
      "SKIN INC SUPPLEMENT BAR\n",
      "N.CAT\n",
      "SINGAPORE AIRLINES SERVICE CENTRE\n",
      "YVES SAINT LAURENT BEAUTE\n",
      "SEPHORA\n",
      "CLARINS SKIN SPA\n",
      "SANDRO\n",
      "SHISEIDO\n",
      "SAMSONITE\n",
      "BATH & BODY WORKS\n",
      "ESCENTIALS\n",
      "JASONS DELI\n",
      "ROLEX\n",
      "RED ARMY WATCHES\n",
      "CONVERSE\n",
      "ALEXANDER MCQUEEN\n",
      "CRATE & BARREL\n",
      "JOY LUCK TEAHOUSE\n",
      "DIOR\n",
      "ESTEE LAUDER\n",
      "ROYCE'\n",
      "DUNKIN' DONUTS\n",
      "LOEWE\n",
      "THE BODY SHOP\n",
      "ENVIE DE POIS\n",
      "TUDOR\n",
      "EU YAN SANG\n",
      "HANG HEUNG HONG KONG\n",
      "108 MATCHA SARO\n",
      "CLUB MONACO\n",
      "FOOD OPERA\n",
      "SIGNATURE KOI\n",
      "FOSSIL\n",
      "1-ATICO by 1-ALTITUDE\n",
      "LOVE, BONITO\n",
      "GENTLE MONSTER\n",
      "SEN-RYO\n",
      "PARFUMS CHRISTIAN DIOR\n",
      "GRAF VON FABER-CASTELL\n",
      "GRAFF\n",
      "AMERICAN VINTAGE\n",
      "RIVE GAUCHE PATISSERIE\n",
      "SHU UEMURA\n",
      "LADY M\n",
      "HEYTEA 喜茶\n",
      "HUGO\n",
      "IMPERIAL TREASURE FINE TEOCHEW CUISINE\n",
      "ELEMIS LONDON\n",
      "IMPERIAL TREASURE STEAMBOAT RESTAURANT\n",
      "IN GOOD COMPANY\n",
      "HAWKERS' STREET\n",
      "BEYOND THE VINES\n",
      "ITACHO SUSHI\n",
      "NATURELAND SPA. PREMIUM\n",
      "IWC SCHAFFHAUSEN\n",
      "TEMPURA TENDON TENYA\n",
      "JENNIFER GREEN\n",
      "JO MALONE LONDON\n",
      "THE HISTORY OF WHOO\n",
      "KATE SPADE\n",
      "KIEHL'S SINCE 1851\n",
      "HUBLOT\n",
      "SU:M37\n",
      "L'ATELIER BY THE HOUR GLASS\n",
      "GLANCE\n",
      "L'OCCITANE\n",
      "SAMSUNG\n",
      "LAB SERIES\n",
      "SWEATY BETTY LONDON\n",
      "SUPERGURL\n",
      "RISIS\n",
      "LEGO\n",
      "SPACIO TCM WELLNESS\n",
      "LA PRAIRIE\n",
      "LEVI'S\n",
      "MONICA VINADER\n",
      "DIOR PRESTIGE LA SUITE\n",
      "MONSTER CURRY\n",
      "PARIS BAGUETTE\n",
      "CHALLENGER\n",
      "LAVENDER\n",
      "CATH KIDSTON\n",
      "BY INVITE ONLY\n",
      "NIKU KAPPO BY WATAMI\n",
      "PANERAI\n",
      "OMEGA\n",
      "OPERA GALLERY\n",
      "ORIGINS\n",
      "SUNDAY FOLKS\n",
      "PEDRO\n",
      "TWO LIPS\n",
      "PENHALIGON'S\n",
      "KEE WAH BAKERY\n",
      "PEZZO\n",
      "SANOOK KITCHEN\n",
      "PICOTA NAIL SPA\n",
      "KLARRA\n",
      "PINKO\n",
      "FIVE GUYS\n",
      "PLAIN VANILLA\n",
      "FRANCK MULLER\n",
      "PUTIEN\n",
      "Citibank ATM\n",
      "COULISSE HEIR\n",
      "RATIO CAFE + GASTROBAR\n",
      "DOLCE & GABBANA\n",
      "LUCKY DUMPLINGS\n",
      "GUERLAIN\n",
      "& OTHER STORIES\n",
      "SALLY'S\n",
      "YUMMY TUMMY\n",
      "COCOMI\n",
      "TUMI\n",
      "SULWHASOO\n",
      "YUN NANS RESTAURANT\n",
      "SPA ELEMENTS\n",
      "VENCHI\n",
      "COTTON ON\n",
      "FRED PERRY\n",
      "2XU\n",
      "G2000\n",
      "SUNNY STEP\n",
      "JERIC SALON\n",
      "CHANEL PRIVE\n",
      "NB 1906\n",
      "GUCCI BEAUTY\n",
      "VACHERON CONSTANTIN\n",
      "ROYAL SELANGOR\n",
      "CARTIER\n",
      "STELLAR\n",
      "MIZU AESTHETIC CLINIC + AERAS\n",
      "FILA\n",
      "NAILZ TREATS\n",
      "PUZZLE COFFEE\n",
      "THE ONITSUKA\n",
      "LEICA\n",
      "NESPRESSO\n",
      "G-SHOCK CASIO\n",
      "CLARKS\n",
      "MASSIMO DUTTI\n",
      "GOLDHEART JEWEL GALLERIA\n",
      "SURREY HILLS GROCER\n",
      "GUZMAN Y GOMEZ MEXICAN KITCHEN\n",
      "HAIR ATELIER BY SHUNJI MATSUO\n",
      "LA SENZA\n",
      "LETAO / TOKYO MILK CHEESE FACTORY\n",
      "BIMBA Y LOLA\n",
      "DESIGUAL\n",
      "V-ZUG\n",
      "H&M\n",
      "FITFLOP\n",
      "APM MONACO\n",
      "DIOR 01-06\n",
      "SINGAPORE POST\n",
      "I-PRIMO\n",
      "CROCS KIOSK\n",
      "SMILE MARTABAK\n",
      "JD SPORTS\n",
      "LE MATIN PATISSERIE\n",
      "AESOP\n",
      "OWNDAYS\n",
      "DEVIALET\n",
      "7-ELEVEN\n",
      "A. LANGE & SOHNE\n",
      "ADIDAS\n",
      "AESTHETIC BAY\n",
      "BENGAWAN SOLO\n",
      "BVLGARI\n",
      "CALVIN KLEIN\n",
      "CHRISTIAN DIOR PARIS\n",
      "CLARINS\n",
      "COS\n",
      "COTTON ON BODY\n",
      "CROCS\n",
      "DAISO\n",
      "DIOR HOMME\n",
      "EACH A CUP\n",
      "ECCO\n",
      "FENDI\n",
      "FRESH\n",
      "GIORGIO ARMANI BEAUTY\n",
      "LAC\n",
      "GUARDIAN B4\n",
      "GUCCI\n",
      "HARRY WINSTON\n",
      "HOSHINO COFFEE\n",
      "HYSSES\n",
      "ISLAND CREAMERY\n",
      "JUMBO SEAFOOD\n",
      "KANSHOKU RAMEN BAR\n",
      "KOI THE\n",
      "LEE HWA DIAMOND PROMENADE\n",
      "CHANEL\n",
      "LORO PIANA, id: 1fKlYldLHueV_NHLcZ7rG-vK1jWWE2ib0r2U0Uk7jw-8\n",
      "title: Hello, please give me in excel table format, the target_age, target_annual_income, target_gender of all the shops in the list given here:\n",
      "Name:\n",
      "BOBBI BROWN\n",
      "SWATCH\n",
      "LIM CHEE GUAN\n",
      "LONGCHAMP\n",
      "TEPPEI SYOKUDO\n",
      "THE MARMALADE PANTRY\n",
      "TORI-Q\n",
      "TYPO\n",
      "BRAUN BÜFFEL\n",
      "UNIQLO\n",
      "BUBEN & ZORWEG\n",
      "VAN CLEEF & ARPELS\n",
      "VANS\n",
      "BYND ARTISAN\n",
      "VIOLET OON SINGAPORE\n",
      "VOM FASS\n",
      "CHATERAISE\n",
      "WATSONS\n",
      "CHOW TAI FOOK\n",
      "YA KUN KAYA TOAST\n",
      "TWG 1837 TEA\n",
      "FLOWER MATTERS\n",
      "TRIUMPH\n",
      "MASTER-FIX SERVICES\n",
      "PRESTO DRYCLEANERS\n",
      "TOMMY HILFIGER\n",
      "TOAST BOX\n",
      "TISSOT\n",
      "OCBC ATM\n",
      "TIFFANY & CO.\n",
      "OCBC BANK\n",
      "LULULEMON\n",
      "THE WHISKY DISTILLERY\n",
      "RUBI SHOES\n",
      "THE OAKS CELLARS\n",
      "BREITLING\n",
      "THE NORTH FACE\n",
      "MAJE\n",
      "RABEANCO\n",
      "LEE WEE & BROTHERS\n",
      "ZARA\n",
      "RAY-BAN\n",
      "THE 1872 CLIPPER TEA CO.\n",
      "RAMEN NAGI\n",
      "TED BAKER LONDON\n",
      "VALENTINO\n",
      "TASTE PARADISE\n",
      "BACHA COFFEE\n",
      "TAG HEUER\n",
      "ROGER DUBUIS\n",
      "TAKA JEWELLERY TREASURES\n",
      "SWAROVSKI\n",
      "SUITSUPPLY\n",
      "GUARDIAN B2\n",
      "STEINWAY & SONS\n",
      "STARBUCKS COFFEE\n",
      "EVISU\n",
      "POULET + BRASSERIE\n",
      "SKIN INC SUPPLEMENT BAR\n",
      "N.CAT\n",
      "SINGAPORE AIRLINES SERVICE CENTRE\n",
      "YVES SAINT LAURENT BEAUTE\n",
      "SEPHORA\n",
      "CLARINS SKIN SPA\n",
      "SANDRO\n",
      "SHISEIDO\n",
      "SAMSONITE\n",
      "BATH & BODY WORKS\n",
      "ESCENTIALS\n",
      "JASONS DELI\n",
      "ROLEX\n",
      "RED ARMY WATCHES\n",
      "CONVERSE\n",
      "ALEXANDER MCQUEEN\n",
      "CRATE & BARREL\n",
      "JOY LUCK TEAHOUSE\n",
      "DIOR\n",
      "ESTEE LAUDER\n",
      "ROYCE'\n",
      "DUNKIN' DONUTS\n",
      "LOEWE\n",
      "THE BODY SHOP\n",
      "ENVIE DE POIS\n",
      "TUDOR\n",
      "EU YAN SANG\n",
      "HANG HEUNG HONG KONG\n",
      "108 MATCHA SARO\n",
      "CLUB MONACO\n",
      "FOOD OPERA\n",
      "SIGNATURE KOI\n",
      "FOSSIL\n",
      "1-ATICO by 1-ALTITUDE\n",
      "LOVE, BONITO\n",
      "GENTLE MONSTER\n",
      "SEN-RYO\n",
      "PARFUMS CHRISTIAN DIOR\n",
      "GRAF VON FABER-CASTELL\n",
      "GRAFF\n",
      "AMERICAN VINTAGE\n",
      "RIVE GAUCHE PATISSERIE\n",
      "SHU UEMURA\n",
      "LADY M\n",
      "HEYTEA 喜茶\n",
      "HUGO\n",
      "IMPERIAL TREASURE FINE TEOCHEW CUISINE\n",
      "ELEMIS LONDON\n",
      "IMPERIAL TREASURE STEAMBOAT RESTAURANT\n",
      "IN GOOD COMPANY\n",
      "HAWKERS' STREET\n",
      "BEYOND THE VINES\n",
      "ITACHO SUSHI\n",
      "NATURELAND SPA. PREMIUM\n",
      "IWC SCHAFFHAUSEN\n",
      "TEMPURA TENDON TENYA\n",
      "JENNIFER GREEN\n",
      "JO MALONE LONDON\n",
      "THE HISTORY OF WHOO\n",
      "KATE SPADE\n",
      "KIEHL'S SINCE 1851\n",
      "HUBLOT\n",
      "SU:M37\n",
      "L'ATELIER BY THE HOUR GLASS\n",
      "GLANCE\n",
      "L'OCCITANE\n",
      "SAMSUNG\n",
      "LAB SERIES\n",
      "SWEATY BETTY LONDON\n",
      "SUPERGURL\n",
      "RISIS\n",
      "LEGO\n",
      "SPACIO TCM WELLNESS\n",
      "LA PRAIRIE\n",
      "LEVI'S\n",
      "MONICA VINADER\n",
      "DIOR PRESTIGE LA SUITE\n",
      "MONSTER CURRY\n",
      "PARIS BAGUETTE\n",
      "CHALLENGER\n",
      "LAVENDER\n",
      "CATH KIDSTON\n",
      "BY INVITE ONLY\n",
      "NIKU KAPPO BY WATAMI\n",
      "PANERAI\n",
      "OMEGA\n",
      "OPERA GALLERY\n",
      "ORIGINS\n",
      "SUNDAY FOLKS\n",
      "PEDRO\n",
      "TWO LIPS\n",
      "PENHALIGON'S\n",
      "KEE WAH BAKERY\n",
      "PEZZO\n",
      "SANOOK KITCHEN\n",
      "PICOTA NAIL SPA\n",
      "KLARRA\n",
      "PINKO\n",
      "FIVE GUYS\n",
      "PLAIN VANILLA\n",
      "FRANCK MULLER\n",
      "PUTIEN\n",
      "Citibank ATM\n",
      "COULISSE HEIR\n",
      "RATIO CAFE + GASTROBAR\n",
      "DOLCE & GABBANA\n",
      "LUCKY DUMPLINGS\n",
      "GUERLAIN\n",
      "& OTHER STORIES\n",
      "SALLY'S\n",
      "YUMMY TUMMY\n",
      "COCOMI\n",
      "TUMI\n",
      "SULWHASOO\n",
      "YUN NANS RESTAURANT\n",
      "SPA ELEMENTS\n",
      "VENCHI\n",
      "COTTON ON\n",
      "FRED PERRY\n",
      "2XU\n",
      "G2000\n",
      "SUNNY STEP\n",
      "JERIC SALON\n",
      "CHANEL PRIVE\n",
      "NB 1906\n",
      "GUCCI BEAUTY\n",
      "VACHERON CONSTANTIN\n",
      "ROYAL SELANGOR\n",
      "CARTIER\n",
      "STELLAR\n",
      "MIZU AESTHETIC CLINIC + AERAS\n",
      "FILA\n",
      "NAILZ TREATS\n",
      "PUZZLE COFFEE\n",
      "THE ONITSUKA\n",
      "LEICA\n",
      "NESPRESSO\n",
      "G-SHOCK CASIO\n",
      "CLARKS\n",
      "MASSIMO DUTTI\n",
      "GOLDHEART JEWEL GALLERIA\n",
      "SURREY HILLS GROCER\n",
      "GUZMAN Y GOMEZ MEXICAN KITCHEN\n",
      "HAIR ATELIER BY SHUNJI MATSUO\n",
      "LA SENZA\n",
      "LETAO / TOKYO MILK CHEESE FACTORY\n",
      "BIMBA Y LOLA\n",
      "DESIGUAL\n",
      "V-ZUG\n",
      "H&M\n",
      "FITFLOP\n",
      "APM MONACO\n",
      "DIOR 01-06\n",
      "SINGAPORE POST\n",
      "I-PRIMO\n",
      "CROCS KIOSK\n",
      "SMILE MARTABAK\n",
      "JD SPORTS\n",
      "LE MATIN PATISSERIE\n",
      "AESOP\n",
      "OWNDAYS\n",
      "DEVIALET\n",
      "7-ELEVEN\n",
      "A. LANGE & SOHNE\n",
      "ADIDAS\n",
      "AESTHETIC BAY\n",
      "BENGAWAN SOLO\n",
      "BVLGARI\n",
      "CALVIN KLEIN\n",
      "CHRISTIAN DIOR PARIS\n",
      "CLARINS\n",
      "COS\n",
      "COTTON ON BODY\n",
      "CROCS\n",
      "DAISO\n",
      "DIOR HOMME\n",
      "EACH A CUP\n",
      "ECCO\n",
      "FENDI\n",
      "FRESH\n",
      "GIORGIO ARMANI BEAUTY\n",
      "LAC\n",
      "GUARDIAN B4\n",
      "GUCCI\n",
      "HARRY WINSTON\n",
      "HOSHINO COFFEE\n",
      "HYSSES\n",
      "ISLAND CREAMERY\n",
      "JUMBO SEAFOOD\n",
      "KANSHOKU RAMEN BAR\n",
      "KOI THE\n",
      "LEE HWA DIAMOND PROMENADE\n",
      "CHANEL\n",
      "LORO PIANA, id: 1VD77bPEfVoS5sRdc_ZEVV2MOSXHXkpmTjKXo0Llbgc4\n",
      "title: for the following list of shops, give me a table of their target age of shoppers, target annual income, target gender of shoppers. \n",
      "\n",
      "LOUIS VUITTON\n",
      "LOVE & CO.\n",
      "LOVISA\n",
      "MANGO\n",
      "CHRISTIAN LOUBOUTIN\n",
      "MUJI 无印良品\n",
      "OLD CHANG KEE\n",
      "MOS BURGER\n",
      "OPTICAL 88* PREMIER\n",
      "ALICE AND OLIVIA\n",
      "PANDORA\n",
      "AWFULLY CHOCOLATE\n",
      "PATEK PHILIPPE\n",
      "BOOST &\n",
      "BOTTEGA VENETA\n",
      "PRADA\n",
      "BREADTALK B4\n",
      "PROOF LIVING\n",
      "BREADTALK B2\n",
      "PULL&BEAR\n",
      "PUMA\n",
      "CHARLES & KEITH\n",
      "ROYAL SPORTING HOUSE\n",
      "CHAUMET\n",
      "SAINT LAURENT\n",
      "CHOMEL\n",
      "SINGTEL EXCLUSIVE RETAILER\n",
      "CRUMPLER\n",
      "SMIGGLE\n",
      "ZOFF\n",
      "SK JEWELLERY\n",
      "STRIP & BROWHAUS\n",
      "BOBBI BROWN\n",
      "SWATCH\n",
      "LIM CHEE GUAN\n",
      "LONGCHAMP\n",
      "TEPPEI SYOKUDO\n",
      "THE MARMALADE PANTRY\n",
      "TORI-Q\n",
      "TYPO\n",
      "BRAUN BÜFFEL\n",
      "UNIQLO\n",
      "BUBEN & ZORWEG\n",
      "VAN CLEEF & ARPELS\n",
      "VANS\n",
      "BYND ARTISAN\n",
      "VIOLET OON SINGAPORE\n",
      "VOM FASS\n",
      "CHATERAISE\n",
      "WATSONS\n",
      "CHOW TAI FOOK\n",
      "YA KUN KAYA TOAST\n",
      "TWG 1837 TEA\n",
      "FLOWER MATTERS\n",
      "TRIUMPH\n",
      "MASTER-FIX SERVICES\n",
      "PRESTO DRYCLEANERS\n",
      "TOMMY HILFIGER\n",
      "TOAST BOX\n",
      "TISSOT\n",
      "OCBC ATM\n",
      "TIFFANY & CO.\n",
      "OCBC BANK\n",
      "LULULEMON\n",
      "THE WHISKY DISTILLERY\n",
      "RUBI SHOES\n",
      "THE OAKS CELLARS\n",
      "BREITLING\n",
      "THE NORTH FACE\n",
      "MAJE\n",
      "RABEANCO\n",
      "LEE WEE & BROTHERS\n",
      "ZARA\n",
      "RAY-BAN\n",
      "THE 1872 CLIPPER TEA CO.\n",
      "RAMEN NAGI\n",
      "TED BAKER LONDON\n",
      "VALENTINO\n",
      "TASTE PARADISE\n",
      "BACHA COFFEE\n",
      "TAG HEUER\n",
      "ROGER DUBUIS\n",
      "TAKA JEWELLERY TREASURES\n",
      "SWAROVSKI\n",
      "SUITSUPPLY\n",
      "GUARDIAN B2\n",
      "STEINWAY & SONS\n",
      "STARBUCKS COFFEE\n",
      "EVISU\n",
      "POULET + BRASSERIE\n",
      "SKIN INC SUPPLEMENT BAR\n",
      "N.CAT\n",
      "SINGAPORE AIRLINES SERVICE CENTRE\n",
      "YVES SAINT LAURENT BEAUTE\n",
      "SEPHORA\n",
      "CLARINS SKIN SPA\n",
      "SANDRO\n",
      "SHISEIDO\n",
      "SAMSONITE\n",
      "BATH & BODY WORKS\n",
      "ESCENTIALS\n",
      "JASONS DELI\n",
      "ROLEX\n",
      "RED ARMY WATCHES\n",
      "CONVERSE\n",
      "ALEXANDER MCQUEEN\n",
      "CRATE & BARREL\n",
      "JOY LUCK TEAHOUSE\n",
      "DIOR\n",
      "ESTEE LAUDER\n",
      "ROYCE'\n",
      "DUNKIN' DONUTS\n",
      "LOEWE\n",
      "THE BODY SHOP\n",
      "ENVIE DE POIS\n",
      "TUDOR\n",
      "EU YAN SANG\n",
      "HANG HEUNG HONG KONG\n",
      "108 MATCHA SARO\n",
      "CLUB MONACO\n",
      "FOOD OPERA\n",
      "SIGNATURE KOI\n",
      "FOSSIL\n",
      "1-ATICO by 1-ALTITUDE\n",
      "LOVE, BONITO\n",
      "GENTLE MONSTER\n",
      "SEN-RYO\n",
      "PARFUMS CHRISTIAN DIOR\n",
      "GRAF VON FABER-CASTELL\n",
      "GRAFF\n",
      "AMERICAN VINTAGE\n",
      "RIVE GAUCHE PATISSERIE\n",
      "SHU UEMURA\n",
      "LADY M\n",
      "HEYTEA 喜茶\n",
      "HUGO\n",
      "IMPERIAL TREASURE FINE TEOCHEW CUISINE\n",
      "ELEMIS LONDON\n",
      "IMPERIAL TREASURE STEAMBOAT RESTAURANT\n",
      "IN GOOD COMPANY\n",
      "HAWKERS' STREET\n",
      "BEYOND THE VINES\n",
      "ITACHO SUSHI\n",
      "NATURELAND SPA. PREMIUM\n",
      "IWC SCHAFFHAUSEN\n",
      "TEMPURA TENDON TENYA\n",
      "JENNIFER GREEN\n",
      "JO MALONE LONDON\n",
      "THE HISTORY OF WHOO\n",
      "KATE SPADE\n",
      "KIEHL'S SINCE 1851\n",
      "HUBLOT\n",
      "SU:M37\n",
      "L'ATELIER BY THE HOUR GLASS\n",
      "GLANCE\n",
      "L'OCCITANE\n",
      "SAMSUNG\n",
      "LAB SERIES\n",
      "SWEATY BETTY LONDON\n",
      "SUPERGURL\n",
      "RISIS\n",
      "LEGO\n",
      "SPACIO TCM WELLNESS\n",
      "LA PRAIRIE\n",
      "LEVI'S\n",
      "MONICA VINADER\n",
      "DIOR PRESTIGE LA SUITE\n",
      "MONSTER CURRY\n",
      "PARIS BAGUETTE\n",
      "CHALLENGER\n",
      "LAVENDER\n",
      "CATH KIDSTON\n",
      "BY INVITE ONLY\n",
      "NIKU KAPPO BY WATAMI\n",
      "PANERAI\n",
      "OMEGA\n",
      "OPERA GALLERY\n",
      "ORIGINS\n",
      "SUNDAY FOLKS\n",
      "PEDRO\n",
      "TWO LIPS\n",
      "PENHALIGON'S\n",
      "KEE WAH BAKERY\n",
      "PEZZO\n",
      "SANOOK KITCHEN\n",
      "PICOTA NAIL SPA\n",
      "KLARRA\n",
      "PINKO\n",
      "FIVE GUYS\n",
      "PLAIN VANILLA\n",
      "FRANCK MULLER\n",
      "PUTIEN\n",
      "Citibank ATM\n",
      "COULISSE HEIR\n",
      "RATIO CAFE + GASTROBAR\n",
      "DOLCE & GABBANA\n",
      "LUCKY DUMPLINGS\n",
      "GUERLAIN\n",
      "& OTHER STORIES\n",
      "SALLY'S\n",
      "YUMMY TUMMY\n",
      "COCOMI\n",
      "TUMI\n",
      "SULWHASOO\n",
      "YUN NANS RESTAURANT\n",
      "SPA ELEMENTS\n",
      "VENCHI\n",
      "COTTON ON\n",
      "FRED PERRY\n",
      "2XU\n",
      "G2000\n",
      "SUNNY STEP\n",
      "JERIC SALON\n",
      "CHANEL PRIVE\n",
      "NB 1906\n",
      "GUCCI BEAUTY\n",
      "VACHERON CONSTANTIN\n",
      "ROYAL SELANGOR\n",
      "CARTIER\n",
      "STELLAR\n",
      "MIZU AESTHETIC CLINIC + AERAS\n",
      "FILA\n",
      "NAILZ TREATS\n",
      "PUZZLE COFFEE\n",
      "THE ONITSUKA\n",
      "LEICA\n",
      "NESPRESSO\n",
      "G-SHOCK CASIO\n",
      "CLARKS\n",
      "MASSIMO DUTTI\n",
      "GOLDHEART JEWEL GALLERIA\n",
      "SURREY HILLS GROCER\n",
      "GUZMAN Y GOMEZ MEXICAN KITCHEN\n",
      "HAIR ATELIER BY SHUNJI MATSUO\n",
      "LA SENZA\n",
      "LETAO / TOKYO MILK CHEESE FACTORY\n",
      "BIMBA Y LOLA\n",
      "DESIGUAL\n",
      "V-ZUG\n",
      "H&M\n",
      "FITFLOP\n",
      "APM MONACO\n",
      "DIOR 01-06\n",
      "SINGAPORE POST\n",
      "I-PRIMO\n",
      "CROCS KIOSK\n",
      "SMILE MARTABAK\n",
      "JD SPORTS\n",
      "LE MATIN PATISSERIE\n",
      "AESOP\n",
      "OWNDAYS\n",
      "DEVIALET\n",
      "7-ELEVEN\n",
      "A. LANGE & SOHNE\n",
      "ADIDAS\n",
      "AESTHETIC BAY\n",
      "BENGAWAN SOLO\n",
      "BVLGARI\n",
      "CALVIN KLEIN\n",
      "CHRISTIAN DIOR PARIS\n",
      "CLARINS\n",
      "COS\n",
      "COTTON ON BODY\n",
      "CROCS\n",
      "DAISO\n",
      "DIOR HOMME\n",
      "EACH A CUP\n",
      "ECCO\n",
      "FENDI\n",
      "FRESH\n",
      "GIORGIO ARMANI BEAUTY\n",
      "LAC\n",
      "GUARDIAN B4\n",
      "GUCCI\n",
      "HARRY WINSTON\n",
      "HOSHINO COFFEE\n",
      "HYSSES\n",
      "ISLAND CREAMERY\n",
      "JUMBO SEAFOOD\n",
      "KANSHOKU RAMEN BAR\n",
      "KOI THE\n",
      "LEE HWA DIAMOND PROMENADE\n",
      "CHANEL\n",
      "LORO PIANA, id: 1jNzbCLHNN0bU6cP9vEt4PrT2QsFGsvkXkHORyuTo-fs\n",
      "title: Hello, please give me in excel table format, the target_age, target_annual_income, target_gender of all the shops in the list given here: name\n",
      "LOUIS VUITTON\n",
      "LOVE & CO.\n",
      "LOVISA\n",
      "MANGO\n",
      "CHRISTIAN LOUBOUTIN\n",
      "MUJI 无印良品\n",
      "OLD CHANG KEE\n",
      "MOS BURGER\n",
      "OPTICAL 88* PREMIER\n",
      "ALICE AND OLIVIA\n",
      "PANDORA\n",
      "AWFULLY CHOCOLATE\n",
      "PATEK PHILIPPE\n",
      "BOOST &\n",
      "BOTTEGA VENETA\n",
      "PRADA\n",
      "BREADTALK B4\n",
      "PROOF LIVING\n",
      "BREADTALK B2\n",
      "PULL&BEAR\n",
      "PUMA\n",
      "CHARLES & KEITH\n",
      "ROYAL SPORTING HOUSE\n",
      "CHAUMET\n",
      "SAINT LAURENT\n",
      "CHOMEL\n",
      "SINGTEL EXCLUSIVE RETAILER\n",
      "CRUMPLER\n",
      "SMIGGLE\n",
      "ZOFF\n",
      "SK JEWELLERY\n",
      "STRIP & BROWHAUS\n",
      "BOBBI BROWN\n",
      "SWATCH\n",
      "LIM CHEE GUAN\n",
      "LONGCHAMP\n",
      "TEPPEI SYOKUDO\n",
      "THE MARMALADE PANTRY\n",
      "TORI-Q\n",
      "TYPO\n",
      "BRAUN BÜFFEL\n",
      "UNIQLO\n",
      "BUBEN & ZORWEG\n",
      "VAN CLEEF & ARPELS\n",
      "VANS\n",
      "BYND ARTISAN\n",
      "VIOLET OON SINGAPORE\n",
      "VOM FASS\n",
      "CHATERAISE\n",
      "WATSONS\n",
      "CHOW TAI FOOK\n",
      "YA KUN KAYA TOAST\n",
      "TWG 1837 TEA\n",
      "FLOWER MATTERS\n",
      "TRIUMPH\n",
      "MASTER-FIX SERVICES\n",
      "PRESTO DRYCLEANERS\n",
      "TOMMY HILFIGER\n",
      "TOAST BOX\n",
      "TISSOT\n",
      "OCBC ATM\n",
      "TIFFANY & CO.\n",
      "OCBC BANK\n",
      "LULULEMON\n",
      "THE WHISKY DISTILLERY\n",
      "RUBI SHOES\n",
      "THE OAKS CELLARS\n",
      "BREITLING\n",
      "THE NORTH FACE\n",
      "MAJE\n",
      "RABEANCO\n",
      "LEE WEE & BROTHERS\n",
      "ZARA\n",
      "RAY-BAN\n",
      "THE 1872 CLIPPER TEA CO.\n",
      "RAMEN NAGI\n",
      "TED BAKER LONDON\n",
      "VALENTINO\n",
      "TASTE PARADISE\n",
      "BACHA COFFEE\n",
      "TAG HEUER\n",
      "ROGER DUBUIS\n",
      "TAKA JEWELLERY TREASURES\n",
      "SWAROVSKI\n",
      "SUITSUPPLY\n",
      "GUARDIAN B2\n",
      "STEINWAY & SONS\n",
      "STARBUCKS COFFEE\n",
      "EVISU\n",
      "POULET + BRASSERIE\n",
      "SKIN INC SUPPLEMENT BAR\n",
      "N.CAT\n",
      "SINGAPORE AIRLINES SERVICE CENTRE\n",
      "YVES SAINT LAURENT BEAUTE\n",
      "SEPHORA\n",
      "CLARINS SKIN SPA\n",
      "SANDRO\n",
      "SHISEIDO\n",
      "SAMSONITE\n",
      "BATH & BODY WORKS\n",
      "ESCENTIALS\n",
      "JASONS DELI\n",
      "ROLEX\n",
      "RED ARMY WATCHES\n",
      "CONVERSE\n",
      "ALEXANDER MCQUEEN\n",
      "CRATE & BARREL\n",
      "JOY LUCK TEAHOUSE\n",
      "DIOR\n",
      "ESTEE LAUDER\n",
      "ROYCE'\n",
      "DUNKIN' DONUTS\n",
      "LOEWE\n",
      "THE BODY SHOP\n",
      "ENVIE DE POIS\n",
      "TUDOR\n",
      "EU YAN SANG\n",
      "HANG HEUNG HONG KONG\n",
      "108 MATCHA SARO\n",
      "CLUB MONACO\n",
      "FOOD OPERA\n",
      "SIGNATURE KOI\n",
      "FOSSIL\n",
      "1-ATICO by 1-ALTITUDE\n",
      "LOVE, BONITO\n",
      "GENTLE MONSTER\n",
      "SEN-RYO\n",
      "PARFUMS CHRISTIAN DIOR\n",
      "GRAF VON FABER-CASTELL\n",
      "GRAFF\n",
      "AMERICAN VINTAGE\n",
      "RIVE GAUCHE PATISSERIE\n",
      "SHU UEMURA\n",
      "LADY M\n",
      "HEYTEA 喜茶\n",
      "HUGO\n",
      "IMPERIAL TREASURE FINE TEOCHEW CUISINE\n",
      "ELEMIS LONDON\n",
      "IMPERIAL TREASURE STEAMBOAT RESTAURANT\n",
      "IN GOOD COMPANY\n",
      "HAWKERS' STREET\n",
      "BEYOND THE VINES\n",
      "ITACHO SUSHI\n",
      "NATURELAND SPA. PREMIUM\n",
      "IWC SCHAFFHAUSEN\n",
      "TEMPURA TENDON TENYA\n",
      "JENNIFER GREEN\n",
      "JO MALONE LONDON\n",
      "THE HISTORY OF WHOO\n",
      "KATE SPADE\n",
      "KIEHL'S SINCE 1851\n",
      "HUBLOT\n",
      "SU:M37\n",
      "L'ATELIER BY THE HOUR GLASS\n",
      "GLANCE\n",
      "L'OCCITANE\n",
      "SAMSUNG\n",
      "LAB SERIES\n",
      "SWEATY BETTY LONDON\n",
      "SUPERGURL\n",
      "RISIS\n",
      "LEGO\n",
      "SPACIO TCM WELLNESS\n",
      "LA PRAIRIE\n",
      "LEVI'S\n",
      "MONICA VINADER\n",
      "DIOR PRESTIGE LA SUITE\n",
      "MONSTER CURRY\n",
      "PARIS BAGUETTE\n",
      "CHALLENGER\n",
      "LAVENDER\n",
      "CATH KIDSTON\n",
      "BY INVITE ONLY\n",
      "NIKU KAPPO BY WATAMI\n",
      "PANERAI\n",
      "OMEGA\n",
      "OPERA GALLERY\n",
      "ORIGINS\n",
      "SUNDAY FOLKS\n",
      "PEDRO\n",
      "TWO LIPS\n",
      "PENHALIGON'S\n",
      "KEE WAH BAKERY\n",
      "PEZZO\n",
      "SANOOK KITCHEN\n",
      "PICOTA NAIL SPA\n",
      "KLARRA\n",
      "PINKO\n",
      "FIVE GUYS\n",
      "PLAIN VANILLA\n",
      "FRANCK MULLER\n",
      "PUTIEN\n",
      "Citibank ATM\n",
      "COULISSE HEIR\n",
      "RATIO CAFE + GASTROBAR\n",
      "DOLCE & GABBANA\n",
      "LUCKY DUMPLINGS\n",
      "GUERLAIN\n",
      "& OTHER STORIES\n",
      "SALLY'S\n",
      "YUMMY TUMMY\n",
      "COCOMI\n",
      "TUMI\n",
      "SULWHASOO\n",
      "YUN NANS RESTAURANT\n",
      "SPA ELEMENTS\n",
      "VENCHI\n",
      "COTTON ON\n",
      "FRED PERRY\n",
      "2XU\n",
      "G2000\n",
      "SUNNY STEP\n",
      "JERIC SALON\n",
      "CHANEL PRIVE\n",
      "NB 1906\n",
      "GUCCI BEAUTY\n",
      "VACHERON CONSTANTIN\n",
      "ROYAL SELANGOR\n",
      "CARTIER\n",
      "STELLAR\n",
      "MIZU AESTHETIC CLINIC + AERAS\n",
      "FILA\n",
      "NAILZ TREATS\n",
      "PUZZLE COFFEE\n",
      "THE ONITSUKA\n",
      "LEICA\n",
      "NESPRESSO\n",
      "G-SHOCK CASIO\n",
      "CLARKS\n",
      "MASSIMO DUTTI\n",
      "GOLDHEART JEWEL GALLERIA\n",
      "SURREY HILLS GROCER\n",
      "GUZMAN Y GOMEZ MEXICAN KITCHEN\n",
      "HAIR ATELIER BY SHUNJI MATSUO\n",
      "LA SENZA\n",
      "LETAO / TOKYO MILK CHEESE FACTORY\n",
      "BIMBA Y LOLA\n",
      "DESIGUAL\n",
      "V-ZUG\n",
      "H&M\n",
      "FITFLOP\n",
      "APM MONACO\n",
      "DIOR 01-06\n",
      "SINGAPORE POST\n",
      "I-PRIMO\n",
      "CROCS KIOSK\n",
      "SMILE MARTABAK\n",
      "JD SPORTS\n",
      "LE MATIN PATISSERIE\n",
      "AESOP\n",
      "OWNDAYS\n",
      "DEVIALET\n",
      "7-ELEVEN\n",
      "A. LANGE & SOHNE\n",
      "ADIDAS\n",
      "AESTHETIC BAY\n",
      "BENGAWAN SOLO\n",
      "BVLGARI\n",
      "CALVIN KLEIN\n",
      "CHRISTIAN DIOR PARIS\n",
      "CLARINS\n",
      "COS\n",
      "COTTON ON BODY\n",
      "CROCS\n",
      "DAISO\n",
      "DIOR HOMME\n",
      "EACH A CUP\n",
      "ECCO\n",
      "FENDI\n",
      "FRESH\n",
      "GIORGIO ARMANI BEAUTY\n",
      "LAC\n",
      "GUARDIAN B4\n",
      "GUCCI\n",
      "HARRY WINSTON\n",
      "HOSHINO COFFEE\n",
      "HYSSES\n",
      "ISLAND CREAMERY\n",
      "JUMBO SEAFOOD\n",
      "KANSHOKU RAMEN BAR\n",
      "KOI THE\n",
      "LEE HWA DIAMOND PROMENADE\n",
      "CHANEL\n",
      "LORO PIANA, id: 1srCVwVVHM1wIxTcR7UFRw1Jw6SMlAMB8ADtCLi8TU7M\n",
      "title: Hello, please give me in excel table format, the target_age, target_annual_income, target_gender of all the shops in the list given here: name\n",
      "LOUIS VUITTON\n",
      "LOVE & CO.\n",
      "LOVISA\n",
      "MANGO\n",
      "CHRISTIAN LOUBOUTIN\n",
      "MUJI 无印良品\n",
      "OLD CHANG KEE\n",
      "MOS BURGER\n",
      "OPTICAL 88* PREMIER\n",
      "ALICE AND OLIVIA\n",
      "PANDORA\n",
      "AWFULLY CHOCOLATE\n",
      "PATEK PHILIPPE\n",
      "BOOST &\n",
      "BOTTEGA VENETA\n",
      "PRADA\n",
      "BREADTALK B4\n",
      "PROOF LIVING\n",
      "BREADTALK B2\n",
      "PULL&BEAR\n",
      "PUMA\n",
      "CHARLES & KEITH\n",
      "ROYAL SPORTING HOUSE\n",
      "CHAUMET\n",
      "SAINT LAURENT\n",
      "CHOMEL\n",
      "SINGTEL EXCLUSIVE RETAILER\n",
      "CRUMPLER\n",
      "SMIGGLE\n",
      "ZOFF\n",
      "SK JEWELLERY\n",
      "STRIP & BROWHAUS\n",
      "BOBBI BROWN\n",
      "SWATCH\n",
      "LIM CHEE GUAN\n",
      "LONGCHAMP\n",
      "TEPPEI SYOKUDO\n",
      "THE MARMALADE PANTRY\n",
      "TORI-Q\n",
      "TYPO\n",
      "BRAUN BÜFFEL\n",
      "UNIQLO\n",
      "BUBEN & ZORWEG\n",
      "VAN CLEEF & ARPELS\n",
      "VANS\n",
      "BYND ARTISAN\n",
      "VIOLET OON SINGAPORE\n",
      "VOM FASS\n",
      "CHATERAISE\n",
      "WATSONS\n",
      "CHOW TAI FOOK\n",
      "YA KUN KAYA TOAST\n",
      "TWG 1837 TEA\n",
      "FLOWER MATTERS\n",
      "TRIUMPH\n",
      "MASTER-FIX SERVICES\n",
      "PRESTO DRYCLEANERS\n",
      "TOMMY HILFIGER\n",
      "TOAST BOX\n",
      "TISSOT\n",
      "OCBC ATM\n",
      "TIFFANY & CO.\n",
      "OCBC BANK\n",
      "LULULEMON\n",
      "THE WHISKY DISTILLERY\n",
      "RUBI SHOES\n",
      "THE OAKS CELLARS\n",
      "BREITLING\n",
      "THE NORTH FACE\n",
      "MAJE\n",
      "RABEANCO\n",
      "LEE WEE & BROTHERS\n",
      "ZARA\n",
      "RAY-BAN\n",
      "THE 1872 CLIPPER TEA CO.\n",
      "RAMEN NAGI\n",
      "TED BAKER LONDON\n",
      "VALENTINO\n",
      "TASTE PARADISE\n",
      "BACHA COFFEE\n",
      "TAG HEUER\n",
      "ROGER DUBUIS\n",
      "TAKA JEWELLERY TREASURES\n",
      "SWAROVSKI\n",
      "SUITSUPPLY\n",
      "GUARDIAN B2\n",
      "STEINWAY & SONS\n",
      "STARBUCKS COFFEE\n",
      "EVISU\n",
      "POULET + BRASSERIE\n",
      "SKIN INC SUPPLEMENT BAR\n",
      "N.CAT\n",
      "SINGAPORE AIRLINES SERVICE CENTRE\n",
      "YVES SAINT LAURENT BEAUTE\n",
      "SEPHORA\n",
      "CLARINS SKIN SPA\n",
      "SANDRO\n",
      "SHISEIDO\n",
      "SAMSONITE\n",
      "BATH & BODY WORKS\n",
      "ESCENTIALS\n",
      "JASONS DELI\n",
      "ROLEX\n",
      "RED ARMY WATCHES\n",
      "CONVERSE\n",
      "ALEXANDER MCQUEEN\n",
      "CRATE & BARREL\n",
      "JOY LUCK TEAHOUSE\n",
      "DIOR\n",
      "ESTEE LAUDER\n",
      "ROYCE'\n",
      "DUNKIN' DONUTS\n",
      "LOEWE\n",
      "THE BODY SHOP\n",
      "ENVIE DE POIS\n",
      "TUDOR\n",
      "EU YAN SANG\n",
      "HANG HEUNG HONG KONG\n",
      "108 MATCHA SARO\n",
      "CLUB MONACO\n",
      "FOOD OPERA\n",
      "SIGNATURE KOI\n",
      "FOSSIL\n",
      "1-ATICO by 1-ALTITUDE\n",
      "LOVE, BONITO\n",
      "GENTLE MONSTER\n",
      "SEN-RYO\n",
      "PARFUMS CHRISTIAN DIOR\n",
      "GRAF VON FABER-CASTELL\n",
      "GRAFF\n",
      "AMERICAN VINTAGE\n",
      "RIVE GAUCHE PATISSERIE\n",
      "SHU UEMURA\n",
      "LADY M\n",
      "HEYTEA 喜茶\n",
      "HUGO\n",
      "IMPERIAL TREASURE FINE TEOCHEW CUISINE\n",
      "ELEMIS LONDON\n",
      "IMPERIAL TREASURE STEAMBOAT RESTAURANT\n",
      "IN GOOD COMPANY\n",
      "HAWKERS' STREET\n",
      "BEYOND THE VINES\n",
      "ITACHO SUSHI\n",
      "NATURELAND SPA. PREMIUM\n",
      "IWC SCHAFFHAUSEN\n",
      "TEMPURA TENDON TENYA\n",
      "JENNIFER GREEN\n",
      "JO MALONE LONDON\n",
      "THE HISTORY OF WHOO\n",
      "KATE SPADE\n",
      "KIEHL'S SINCE 1851\n",
      "HUBLOT\n",
      "SU:M37\n",
      "L'ATELIER BY THE HOUR GLASS\n",
      "GLANCE\n",
      "L'OCCITANE\n",
      "SAMSUNG\n",
      "LAB SERIES\n",
      "SWEATY BETTY LONDON\n",
      "SUPERGURL\n",
      "RISIS\n",
      "LEGO\n",
      "SPACIO TCM WELLNESS\n",
      "LA PRAIRIE\n",
      "LEVI'S\n",
      "MONICA VINADER\n",
      "DIOR PRESTIGE LA SUITE\n",
      "MONSTER CURRY\n",
      "PARIS BAGUETTE\n",
      "CHALLENGER\n",
      "LAVENDER\n",
      "CATH KIDSTON\n",
      "BY INVITE ONLY\n",
      "NIKU KAPPO BY WATAMI\n",
      "PANERAI\n",
      "OMEGA\n",
      "OPERA GALLERY\n",
      "ORIGINS\n",
      "SUNDAY FOLKS\n",
      "PEDRO\n",
      "TWO LIPS\n",
      "PENHALIGON'S\n",
      "KEE WAH BAKERY\n",
      "PEZZO\n",
      "SANOOK KITCHEN\n",
      "PICOTA NAIL SPA\n",
      "KLARRA\n",
      "PINKO\n",
      "FIVE GUYS\n",
      "PLAIN VANILLA\n",
      "FRANCK MULLER\n",
      "PUTIEN\n",
      "Citibank ATM\n",
      "COULISSE HEIR\n",
      "RATIO CAFE + GASTROBAR\n",
      "DOLCE & GABBANA\n",
      "LUCKY DUMPLINGS\n",
      "GUERLAIN\n",
      "& OTHER STORIES\n",
      "SALLY'S\n",
      "YUMMY TUMMY\n",
      "COCOMI\n",
      "TUMI\n",
      "SULWHASOO\n",
      "YUN NANS RESTAURANT\n",
      "SPA ELEMENTS\n",
      "VENCHI\n",
      "COTTON ON\n",
      "FRED PERRY\n",
      "2XU\n",
      "G2000\n",
      "SUNNY STEP\n",
      "JERIC SALON\n",
      "CHANEL PRIVE\n",
      "NB 1906\n",
      "GUCCI BEAUTY\n",
      "VACHERON CONSTANTIN\n",
      "ROYAL SELANGOR\n",
      "CARTIER\n",
      "STELLAR\n",
      "MIZU AESTHETIC CLINIC + AERAS\n",
      "FILA\n",
      "NAILZ TREATS\n",
      "PUZZLE COFFEE\n",
      "THE ONITSUKA\n",
      "LEICA\n",
      "NESPRESSO\n",
      "G-SHOCK CASIO\n",
      "CLARKS\n",
      "MASSIMO DUTTI\n",
      "GOLDHEART JEWEL GALLERIA\n",
      "SURREY HILLS GROCER\n",
      "GUZMAN Y GOMEZ MEXICAN KITCHEN\n",
      "HAIR ATELIER BY SHUNJI MATSUO\n",
      "LA SENZA\n",
      "LETAO / TOKYO MILK CHEESE FACTORY\n",
      "BIMBA Y LOLA\n",
      "DESIGUAL\n",
      "V-ZUG\n",
      "H&M\n",
      "FITFLOP\n",
      "APM MONACO\n",
      "DIOR 01-06\n",
      "SINGAPORE POST\n",
      "I-PRIMO\n",
      "CROCS KIOSK\n",
      "SMILE MARTABAK\n",
      "JD SPORTS\n",
      "LE MATIN PATISSERIE\n",
      "AESOP\n",
      "OWNDAYS\n",
      "DEVIALET\n",
      "7-ELEVEN\n",
      "A. LANGE & SOHNE\n",
      "ADIDAS\n",
      "AESTHETIC BAY\n",
      "BENGAWAN SOLO\n",
      "BVLGARI\n",
      "CALVIN KLEIN\n",
      "CHRISTIAN DIOR PARIS\n",
      "CLARINS\n",
      "COS\n",
      "COTTON ON BODY\n",
      "CROCS\n",
      "DAISO\n",
      "DIOR HOMME\n",
      "EACH A CUP\n",
      "ECCO\n",
      "FENDI\n",
      "FRESH\n",
      "GIORGIO ARMANI BEAUTY\n",
      "LAC\n",
      "GUARDIAN B4\n",
      "GUCCI\n",
      "HARRY WINSTON\n",
      "HOSHINO COFFEE\n",
      "HYSSES\n",
      "ISLAND CREAMERY\n",
      "JUMBO SEAFOOD\n",
      "KANSHOKU RAMEN BAR\n",
      "KOI THE\n",
      "LEE HWA DIAMOND PROMENADE\n",
      "CHANEL\n",
      "LORO PIANA, id: 17aqlCOM5LXnhrzeYwc-0VUTalUmrq_SF6CRbUsZRUKU\n",
      "title: continue for the rest of the list starting from swatch, id: 1QCbI0YLVI68SWYTEQO61NyQQlNJg5B-88a0B_HjxZVY\n",
      "title: experiment-626, id: 1QPnZd_TuTMK5I94mQteBvP36K_Ylu5WP\n",
      "title: venv tide, id: 1GVCmPDhYTcHAzN9-_ABkRzQr2-Fz_6Kp\n",
      "title: Transcripts minutes, id: 14-NyPOHZUM6HX0u7j1JbGZCTUjfEMs1j\n",
      "title: Order, id: 193w2Vtz-WRHPUQrbqwv2Hd3FY0GGtq6zXpR8RVgCDJU\n",
      "title: bukhari.csv, id: 1g1t__9v4PUv5WbPWgM2R2uIe-I9GfQGN\n",
      "title: 16748027758207361413127095574762.jpg, id: 1Rovz1trxzgSdm86RtlZeg7JhAet0TOHv\n",
      "title: 16742714644162374234821918815532.jpg, id: 1DDs6eAfb_VUN3KDyif5NTNJixv-bqCnS\n",
      "title: 20230121_112405.jpg, id: 1zcuTBC80cSaGDYG0B9WGlWRiBMQl6CIF\n",
      "title: BTC_1m_2018_dollar.parq, id: 1Ris2qfXbXkLde3k69KNqsMXAVHI69Opv\n",
      "title: Copy of Elden Ring Weapon Calculator (1.05), id: 1D8ww7UF2vYcjI7wAnazVWimukv_sKtey8q5QsXLkpbo\n",
      "title: Copy of Elden Ring Weapon Calculator (1.05), id: 1e-BgI2vrTLpFKQ8WGR5iwerzUNfPqUAFOGjxJXXiyJo\n",
      "title: .ipynb_checkpoints, id: 1-2HPoImI4xE5ez1luFUHfdWunE0y8MZU\n",
      "title: Colab Notebooks, id: 1u0ZGJQHmWDTdLmzEeaEYgEbEKSEXhbQ7\n",
      "title: tide, id: 1av4zKzp-VAdl6WAbsGUubN5p0LO334wr\n",
      "title: myapp.crt, id: 1SybbrhrzQWxLUJSKjARkcRf94obUiA0D\n",
      "title: Prices, id: 1dDvdzfQ28yfaDipCHbPg0BCQq1a1f40_\n",
      "title: T3G1 Analysis of Iceland and Ireland Banking Crisis Presentation.pptx, id: 0B9FxKgcQD7V8Tlp1SWhQQUlXVVk\n",
      "title: Le Critique T3G1.pptx, id: 0B9FxKgcQD7V8VG5HLWxtd2FKSG8\n",
      "title: Draft 13.1.docx, id: 1HSdQYgH0hqz8B0JVNavLpTYvDSb8P3F5VO-E_krssvo\n",
      "title: Draft 13.1.docx, id: 0B9FxKgcQD7V8MUQ5RThXUDdKUUk\n",
      "title: Currency Peg Analysis.docx, id: 0B9FxKgcQD7V8VnlNUG5BMGw2SDA\n",
      "title: Draft 5 (Comparisons completed! =D ).docx, id: 0B9FxKgcQD7V8Rm5lbGp2U2JoSXM\n",
      "title: HE2006 report 1.docx, id: 1uJFydrKpPxDY8h4P9d0qTkXisYO0rBJXflxdW-ZaZy4\n",
      "title: Comparisons of Policy Responses.docx, id: 0B9FxKgcQD7V8SFVkODZQMHBQT2M\n",
      "title: Comparison of Economic Performances (post-crisis).docx, id: 0B9FxKgcQD7V8M2dwZl82TFNna0k\n",
      "title: Icelandic Banking Crisis Solutions.docx, id: 1mVJFaIfV9HUF5CdSKLqUvZ6CNHv1Twe8yPWt_oChhiA\n"
     ]
    }
   ],
   "source": [
    "# Auto-iterate through all files that matches this query\n",
    "file_list = drive.ListFile({'q': \"'root' in parents and trashed=false\"}).GetList()\n",
    "for file1 in file_list:\n",
    "  print('title: %s, id: %s' % (file1['title'], file1['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GoogleDriveFile({'kind': 'drive#file', 'userPermission': {'id': 'me', 'type': 'user', 'role': 'writer', 'kind': 'drive#permission', 'selfLink': 'https://www.googleapis.com/drive/v2/files/1YE66HuLqo89aB78_LLrX9PWerbDcQklz/permissions/me', 'etag': '\"n-T8ntPzwoU9-xUAJjSVjGQ3Vo8\"', 'pendingOwner': False}, 'selfLink': 'https://www.googleapis.com/drive/v2/files/1YE66HuLqo89aB78_LLrX9PWerbDcQklz', 'ownerNames': ['hellosmartworldtt'], 'lastModifyingUserName': 'hellosmartworldtt', 'editable': True, 'writersCanShare': True, 'mimeType': 'application/vnd.google-apps.folder', 'parents': [{'selfLink': 'https://www.googleapis.com/drive/v2/files/1YE66HuLqo89aB78_LLrX9PWerbDcQklz/parents/1ua92mVSsUFOGXgv6efBRoLHh3eASmssw', 'id': '1ua92mVSsUFOGXgv6efBRoLHh3eASmssw', 'isRoot': False, 'kind': 'drive#parentReference', 'parentLink': 'https://www.googleapis.com/drive/v2/files/1ua92mVSsUFOGXgv6efBRoLHh3eASmssw'}], 'appDataContents': False, 'iconLink': 'https://drive-thirdparty.googleusercontent.com/16/type/application/vnd.google-apps.folder+shared', 'shared': True, 'lastModifyingUser': {'displayName': 'hellosmartworldtt', 'kind': 'drive#user', 'isAuthenticatedUser': False, 'permissionId': '13444253390036453415', 'emailAddress': 'hellosmartworldtt@gmail.com', 'picture': {'url': 'https://lh3.googleusercontent.com/a-/ALV-UjWT3VU1oxcRJcZKn6VmUueBU7TN_FbO6xLsJQsz_u3i886q8A=s64'}}, 'owners': [{'displayName': 'hellosmartworldtt', 'kind': 'drive#user', 'isAuthenticatedUser': False, 'permissionId': '13444253390036453415', 'emailAddress': 'hellosmartworldtt@gmail.com', 'picture': {'url': 'https://lh3.googleusercontent.com/a-/ALV-UjWT3VU1oxcRJcZKn6VmUueBU7TN_FbO6xLsJQsz_u3i886q8A=s64'}}], 'copyable': False, 'etag': '\"MTcxNjE3NzA4MjMwNQ\"', 'alternateLink': 'https://drive.google.com/drive/folders/1YE66HuLqo89aB78_LLrX9PWerbDcQklz', 'embedLink': 'https://drive.google.com/embeddedfolderview?id=1YE66HuLqo89aB78_LLrX9PWerbDcQklz', 'copyRequiresWriterPermission': False, 'spaces': ['drive'], 'id': '1YE66HuLqo89aB78_LLrX9PWerbDcQklz', 'title': 'SWTT', 'labels': {'viewed': False, 'restricted': False, 'starred': False, 'hidden': False, 'trashed': False}, 'explicitlyTrashed': False, 'createdDate': '2024-05-20T03:32:16.788Z', 'modifiedDate': '2024-05-20T03:51:22.305Z', 'markedViewedByMeDate': '1970-01-01T00:00:00.000Z', 'quotaBytesUsed': '0', 'version': '3', 'capabilities': {'canEdit': True, 'canCopy': False}})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydrive2.drive import GoogleDrive\n",
    "# Create GoogleDrive instance with authenticated GoogleAuth instance.\n",
    "drive = GoogleDrive(gauth)\n",
    "filename = 'SWTT'\n",
    "mimetype = 'application/vnd.google-apps.folder'\n",
    "# Query\n",
    "query = {'q': f\"title = '{filename}' and mimeType='{mimetype}'\"}\n",
    "# Get list of files that match against the query\n",
    "files = drive.ListFile(query).GetList()\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GoogleDriveFile({'kind': 'drive#file', 'userPermission': {'id': 'me', 'type': 'user', 'role': 'writer', 'kind': 'drive#permission', 'selfLink': 'https://www.googleapis.com/drive/v2/files/1YE66HuLqo89aB78_LLrX9PWerbDcQklz/permissions/me', 'etag': '\"n-T8ntPzwoU9-xUAJjSVjGQ3Vo8\"', 'pendingOwner': False}, 'selfLink': 'https://www.googleapis.com/drive/v2/files/1YE66HuLqo89aB78_LLrX9PWerbDcQklz', 'ownerNames': ['hellosmartworldtt'], 'lastModifyingUserName': 'hellosmartworldtt', 'editable': True, 'writersCanShare': True, 'mimeType': 'application/vnd.google-apps.folder', 'parents': [{'selfLink': 'https://www.googleapis.com/drive/v2/files/1YE66HuLqo89aB78_LLrX9PWerbDcQklz/parents/1ua92mVSsUFOGXgv6efBRoLHh3eASmssw', 'id': '1ua92mVSsUFOGXgv6efBRoLHh3eASmssw', 'isRoot': False, 'kind': 'drive#parentReference', 'parentLink': 'https://www.googleapis.com/drive/v2/files/1ua92mVSsUFOGXgv6efBRoLHh3eASmssw'}], 'appDataContents': False, 'iconLink': 'https://drive-thirdparty.googleusercontent.com/16/type/application/vnd.google-apps.folder+shared', 'shared': True, 'lastModifyingUser': {'displayName': 'hellosmartworldtt', 'kind': 'drive#user', 'isAuthenticatedUser': False, 'permissionId': '13444253390036453415', 'emailAddress': 'hellosmartworldtt@gmail.com', 'picture': {'url': 'https://lh3.googleusercontent.com/a-/ALV-UjWT3VU1oxcRJcZKn6VmUueBU7TN_FbO6xLsJQsz_u3i886q8A=s64'}}, 'owners': [{'displayName': 'hellosmartworldtt', 'kind': 'drive#user', 'isAuthenticatedUser': False, 'permissionId': '13444253390036453415', 'emailAddress': 'hellosmartworldtt@gmail.com', 'picture': {'url': 'https://lh3.googleusercontent.com/a-/ALV-UjWT3VU1oxcRJcZKn6VmUueBU7TN_FbO6xLsJQsz_u3i886q8A=s64'}}], 'copyable': False, 'etag': '\"MTcxNjE3NzA4MjMwNQ\"', 'alternateLink': 'https://drive.google.com/drive/folders/1YE66HuLqo89aB78_LLrX9PWerbDcQklz', 'embedLink': 'https://drive.google.com/embeddedfolderview?id=1YE66HuLqo89aB78_LLrX9PWerbDcQklz', 'copyRequiresWriterPermission': False, 'spaces': ['drive'], 'id': '1YE66HuLqo89aB78_LLrX9PWerbDcQklz', 'title': 'SWTT', 'labels': {'viewed': False, 'restricted': False, 'starred': False, 'hidden': False, 'trashed': False}, 'explicitlyTrashed': False, 'createdDate': '2024-05-20T03:32:16.788Z', 'modifiedDate': '2024-05-20T03:51:22.305Z', 'markedViewedByMeDate': '1970-01-01T00:00:00.000Z', 'quotaBytesUsed': '0', 'version': '3', 'capabilities': {'canEdit': True, 'canCopy': False}})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) service account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydrive2.auth import GoogleAuth\n",
    "from pydrive2.drive import GoogleDrive\n",
    "\n",
    "def login_with_service_account():\n",
    "    \"\"\"\n",
    "    Google Drive service with a service account.\n",
    "    note: for the service account to work, you need to share the folder or\n",
    "    files with the service account email.\n",
    "\n",
    "    :return: google auth\n",
    "    \"\"\"\n",
    "    # Define the settings dict to use a service account\n",
    "    # We also can use all options available for the settings dict like\n",
    "    # oauth_scope,save_credentials,etc.\n",
    "    settings = {\n",
    "                \"client_config_backend\": \"service\",\n",
    "                \"service_config\": {\n",
    "                    \"client_json_file_path\": \"service-secrets.json\",\n",
    "                }\n",
    "            }\n",
    "    # Create instance of GoogleAuth\n",
    "    gauth = GoogleAuth(settings=settings)\n",
    "    # Authenticate\n",
    "    gauth.ServiceAuth()\n",
    "    return gauth\n",
    "\n",
    "gauth = login_with_service_account()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-iterate through all files that match this query[^2^][2]\n",
    "file_list = drive.ListFile({'q': \"'root' in parents and trashed=false\"}).GetList()\n",
    "for file1 in file_list:\n",
    "    # Check if the file is a folder; folders have a specific MIME type\n",
    "    if file1['mimeType'] == 'application/vnd.google-apps.folder':\n",
    "        print('title: %s, id: %s' % (file1['title'], file1['id']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_cohere\n",
      "  Downloading langchain_cohere-0.1.5-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting cohere<6.0,>=5.5 (from langchain_cohere)\n",
      "  Downloading cohere-5.5.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.42 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from langchain_cohere) (0.1.43)\n",
      "Collecting boto3<2.0.0,>=1.34.0 (from cohere<6.0,>=5.5->langchain_cohere)\n",
      "  Downloading boto3-1.34.111-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from cohere<6.0,>=5.5->langchain_cohere) (1.9.4)\n",
      "Requirement already satisfied: httpx>=0.21.2 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from cohere<6.0,>=5.5->langchain_cohere) (0.27.0)\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0 (from cohere<6.0,>=5.5->langchain_cohere)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: pydantic>=1.9.2 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from cohere<6.0,>=5.5->langchain_cohere) (2.7.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from cohere<6.0,>=5.5->langchain_cohere) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from cohere<6.0,>=5.5->langchain_cohere)\n",
      "  Downloading tokenizers-0.19.1-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from cohere<6.0,>=5.5->langchain_cohere) (2.31.0.20240406)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from cohere<6.0,>=5.5->langchain_cohere) (4.11.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.42->langchain_cohere) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.42->langchain_cohere) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.42->langchain_cohere) (0.1.48)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.42->langchain_cohere) (23.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.42->langchain_cohere) (8.2.3)\n",
      "Collecting botocore<1.35.0,>=1.34.111 (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5->langchain_cohere)\n",
      "  Downloading botocore-1.34.111-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5->langchain_cohere)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5->langchain_cohere)\n",
      "  Downloading s3transfer-0.10.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: anyio in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from httpx>=0.21.2->cohere<6.0,>=5.5->langchain_cohere) (4.3.0)\n",
      "Requirement already satisfied: certifi in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from httpx>=0.21.2->cohere<6.0,>=5.5->langchain_cohere) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from httpx>=0.21.2->cohere<6.0,>=5.5->langchain_cohere) (1.0.5)\n",
      "Requirement already satisfied: idna in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from httpx>=0.21.2->cohere<6.0,>=5.5->langchain_cohere) (3.7)\n",
      "Requirement already satisfied: sniffio in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from httpx>=0.21.2->cohere<6.0,>=5.5->langchain_cohere) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.21.2->cohere<6.0,>=5.5->langchain_cohere) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.42->langchain_cohere) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.42->langchain_cohere) (3.10.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from pydantic>=1.9.2->cohere<6.0,>=5.5->langchain_cohere) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from pydantic>=1.9.2->cohere<6.0,>=5.5->langchain_cohere) (2.18.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->cohere<6.0,>=5.5->langchain_cohere) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->cohere<6.0,>=5.5->langchain_cohere) (1.26.18)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from tokenizers<0.20,>=0.19->cohere<6.0,>=5.5->langchain_cohere) (0.22.2)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.0.0->cohere<6.0,>=5.5->langchain_cohere)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from botocore<1.35.0,>=1.34.111->boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5->langchain_cohere) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20,>=0.19->cohere<6.0,>=5.5->langchain_cohere) (3.13.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20,>=0.19->cohere<6.0,>=5.5->langchain_cohere) (2024.3.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20,>=0.19->cohere<6.0,>=5.5->langchain_cohere) (4.66.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.111->boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5->langchain_cohere) (1.16.0)\n",
      "Requirement already satisfied: colorama in d:\\pycharmprojects\\smart-travels\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers<0.20,>=0.19->cohere<6.0,>=5.5->langchain_cohere) (0.4.6)\n",
      "Downloading langchain_cohere-0.1.5-py3-none-any.whl (30 kB)\n",
      "Downloading cohere-5.5.1-py3-none-any.whl (166 kB)\n",
      "   ---------------------------------------- 0.0/166.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 166.1/166.1 kB 9.7 MB/s eta 0:00:00\n",
      "Downloading boto3-1.34.111-py3-none-any.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 139.3/139.3 kB 8.6 MB/s eta 0:00:00\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading tokenizers-0.19.1-cp312-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 41.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 35.6 MB/s eta 0:00:00\n",
      "Downloading botocore-1.34.111-py3-none-any.whl (12.3 MB)\n",
      "   ---------------------------------------- 0.0/12.3 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 2.2/12.3 MB 69.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 4.6/12.3 MB 58.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 7.0/12.3 MB 56.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.3/12.3 MB 54.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.7/12.3 MB 50.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.3/12.3 MB 43.5 MB/s eta 0:00:00\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
      "   ---------------------------------------- 0.0/82.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 82.2/82.2 kB ? eta 0:00:00\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: urllib3, jmespath, httpx-sse, botocore, s3transfer, tokenizers, boto3, cohere, langchain_cohere\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.18\n",
      "    Uninstalling urllib3-1.26.18:\n",
      "      Successfully uninstalled urllib3-1.26.18\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.2\n",
      "    Uninstalling tokenizers-0.15.2:\n",
      "      Successfully uninstalled tokenizers-0.15.2\n",
      "  Attempting uninstall: cohere\n",
      "    Found existing installation: cohere 5.3.0\n",
      "    Uninstalling cohere-5.3.0:\n",
      "      Successfully uninstalled cohere-5.3.0\n",
      "Successfully installed boto3-1.34.111 botocore-1.34.111 cohere-5.5.1 httpx-sse-0.4.0 jmespath-1.0.1 langchain_cohere-0.1.5 s3transfer-0.10.1 tokenizers-0.19.1 urllib3-2.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\PycharmProjects\\smart-travels\\.venv\\Lib\\site-packages\\~okenizers'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gpsoauth 1.1.0 requires urllib3<2.0, but you have urllib3 2.2.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Relevant documents:'\n",
      "[Document(page_content=\"Introducing Command R+: Our new, most powerful model in the Command R family. Learn More\\n\\nWe’re building the future of language AI\\n\\nCohere empowers every developer and enterprise to build amazing products and capture true business value with language AI.\\n\\nWe’re building the future of language AI\\n\\nCohere empowers every developer and enterprise to build amazing products and capture true business value with language AI.\\n\\nWe’re driven by cutting-edge research\\n\\nAt Cohere, we believe that the union of research and product will realize a world where technology commands language in a way that’s as compelling and coherent as ourselves. We live at the forefront of ML/AI research to bring the latest advancements in language AI to our platform.\\n\\nexplore our research lab\\n\\nPioneering the future of language AI for business\\n\\nCohere’s cutting-edge large language models are built on Transformer architecture and trained on supercomputers, providing NLP solutions that don’t need expensive ML development. With a world-class team of experts, we're dedicated to helping companies revolutionize their operations and maximize potential in real-world business applications.\\n\\nWe’re a collaborative team of experts\\n\\nWe are ML/AI engineers, thinkers, and champions who are passionate about exploring the potential of language AI to make our world a better place. With diverse experience and perspectives, we work together to bring advancements in language AI to developers everywhere.\\n\\nWe’re driven by cutting-edge research\\n\\nAt Cohere, we believe that the union of research and product will realize a world where technology commands language in a way that’s as compelling and coherent as ourselves. We live at the forefront of ML/AI research to bring the latest advancements in language AI to our platform.\\n\\nexplore our research lab\\n\\nPioneering the future of language AI for business\\n\\nCohere’s cutting-edge large language models are built on Transformer architecture and trained on supercomputers, providing NLP solutions that don’t need expensive ML development. With a world-class team of experts, we're dedicated to helping companies revolutionize their operations and maximize potential in real-world business applications.\\n\\nWe’re a collaborative team of experts\\n\\nWe are ML/AI engineers, thinkers, and champions who are passionate about exploring the potential of language AI to make our world a better place. With diverse experience and perspectives, we work together to bring advancements in language AI to developers everywhere.\\n\\nVP people operations\\n\\nChief Product Officer\\n\\nCHIEF SCIENTIST, SVP GENERATIVE MODELING\\n\\n“Very large language models are now giving computers a much better understanding of human communication. The team at Cohere is building technology that will make this revolution in natural language understanding much more widely available.”\\n\\nGeoffrey Hinton — Emeritus Prof. Comp Sci, U.Toronto\\n\\nCheck out the latest news featuring Cohere\", metadata={'id': 'web-search_0', 'snippet': \"Introducing Command R+: Our new, most powerful model in the Command R family. Learn More\\n\\nWe’re building the future of language AI\\n\\nCohere empowers every developer and enterprise to build amazing products and capture true business value with language AI.\\n\\nWe’re building the future of language AI\\n\\nCohere empowers every developer and enterprise to build amazing products and capture true business value with language AI.\\n\\nWe’re driven by cutting-edge research\\n\\nAt Cohere, we believe that the union of research and product will realize a world where technology commands language in a way that’s as compelling and coherent as ourselves. We live at the forefront of ML/AI research to bring the latest advancements in language AI to our platform.\\n\\nexplore our research lab\\n\\nPioneering the future of language AI for business\\n\\nCohere’s cutting-edge large language models are built on Transformer architecture and trained on supercomputers, providing NLP solutions that don’t need expensive ML development. With a world-class team of experts, we're dedicated to helping companies revolutionize their operations and maximize potential in real-world business applications.\\n\\nWe’re a collaborative team of experts\\n\\nWe are ML/AI engineers, thinkers, and champions who are passionate about exploring the potential of language AI to make our world a better place. With diverse experience and perspectives, we work together to bring advancements in language AI to developers everywhere.\\n\\nWe’re driven by cutting-edge research\\n\\nAt Cohere, we believe that the union of research and product will realize a world where technology commands language in a way that’s as compelling and coherent as ourselves. We live at the forefront of ML/AI research to bring the latest advancements in language AI to our platform.\\n\\nexplore our research lab\\n\\nPioneering the future of language AI for business\\n\\nCohere’s cutting-edge large language models are built on Transformer architecture and trained on supercomputers, providing NLP solutions that don’t need expensive ML development. With a world-class team of experts, we're dedicated to helping companies revolutionize their operations and maximize potential in real-world business applications.\\n\\nWe’re a collaborative team of experts\\n\\nWe are ML/AI engineers, thinkers, and champions who are passionate about exploring the potential of language AI to make our world a better place. With diverse experience and perspectives, we work together to bring advancements in language AI to developers everywhere.\\n\\nVP people operations\\n\\nChief Product Officer\\n\\nCHIEF SCIENTIST, SVP GENERATIVE MODELING\\n\\n“Very large language models are now giving computers a much better understanding of human communication. The team at Cohere is building technology that will make this revolution in natural language understanding much more widely available.”\\n\\nGeoffrey Hinton — Emeritus Prof. Comp Sci, U.Toronto\\n\\nCheck out the latest news featuring Cohere\", 'timestamp': '2024-05-17T18:23:53', 'title': 'About | Cohere', 'url': 'https://cohere.com/about'}),\n",
      " Document(page_content=\"Introducing Command R+: Our new, most powerful model in the Command R family. Learn More\\n\\nThe Leading Enterprise AI Platform\\n\\nBuilt on the language of business\\n\\nOptimized for enterprise generative AI, search and discovery, and advanced retrieval.\\n\\nOur models are designed to augment and elevate the global workforce, so businesses can thrive and stay competitive in the AI era.\\n\\nCohere Command, powering innovation with enterprise GenAI\\n\\nCommand models are used by companies to build production-ready, scalable and efficient AI-powered applications.\\n\\nCohere Embed, unlocking the full potential of your enterprise data\\n\\nWe have trained our models on the language of business to enable the most accurate and efficient solution. Scale your Enterprise AI strategy starting with the highest performing embedding model, which supports over 100 languages.\\n\\nCohere Rerank, surfacing the industry’s most accurate responses.\\n\\nThe combination of Rerank and Embed delivers the most reliable and up-to-date responses for your application, grounding retrieval-augmented generation (RAG) in your data.\\n\\nBuild with Advanced Retrieval\\n\\nOur Generative AI and Advanced Retrieval models work seamlessly together for applications requiring RAG. It's easy to connect and retrieve information from documents and enterprise data sources to build more powerful AI applications.\\n\\nEnterprise-grade AI deployment on any cloud or on-premises\\n\\nOnly Cohere provides flexible and secure deployment options. Bring our models to your data.\\n\\nExplore how the latest AI advancements can apply to your business\\n\\nGet the competitive edge in generative AI with courses from LLM University\\n\\nStart integrating Cohere’s AI models into your apps and workflows\\n\\nDo whatever it takes\\n\\nto scale intelligence\\n\\nDo whatever it takes\\n\\nWe work tirelessly towards our goals.\\n\\nTO scale intelligence\\n\\nWe want to make intelligence abundant, affordable, and accessible.\\n\\nWe build our technology to benefit people and positively impact the world.\", metadata={'id': 'web-search_1', 'snippet': \"Introducing Command R+: Our new, most powerful model in the Command R family. Learn More\\n\\nThe Leading Enterprise AI Platform\\n\\nBuilt on the language of business\\n\\nOptimized for enterprise generative AI, search and discovery, and advanced retrieval.\\n\\nOur models are designed to augment and elevate the global workforce, so businesses can thrive and stay competitive in the AI era.\\n\\nCohere Command, powering innovation with enterprise GenAI\\n\\nCommand models are used by companies to build production-ready, scalable and efficient AI-powered applications.\\n\\nCohere Embed, unlocking the full potential of your enterprise data\\n\\nWe have trained our models on the language of business to enable the most accurate and efficient solution. Scale your Enterprise AI strategy starting with the highest performing embedding model, which supports over 100 languages.\\n\\nCohere Rerank, surfacing the industry’s most accurate responses.\\n\\nThe combination of Rerank and Embed delivers the most reliable and up-to-date responses for your application, grounding retrieval-augmented generation (RAG) in your data.\\n\\nBuild with Advanced Retrieval\\n\\nOur Generative AI and Advanced Retrieval models work seamlessly together for applications requiring RAG. It's easy to connect and retrieve information from documents and enterprise data sources to build more powerful AI applications.\\n\\nEnterprise-grade AI deployment on any cloud or on-premises\\n\\nOnly Cohere provides flexible and secure deployment options. Bring our models to your data.\\n\\nExplore how the latest AI advancements can apply to your business\\n\\nGet the competitive edge in generative AI with courses from LLM University\\n\\nStart integrating Cohere’s AI models into your apps and workflows\\n\\nDo whatever it takes\\n\\nto scale intelligence\\n\\nDo whatever it takes\\n\\nWe work tirelessly towards our goals.\\n\\nTO scale intelligence\\n\\nWe want to make intelligence abundant, affordable, and accessible.\\n\\nWe build our technology to benefit people and positively impact the world.\", 'timestamp': '2024-05-18T19:12:42', 'title': 'Cohere | The leading AI platform for enterprise', 'url': 'https://cohere.com/'}),\n",
      " Document(page_content=\"From the Editor: Notable and Noteworthy\\n\\nFounders Aidan Gomez, Nick Frosst, and Ivan Zhang have deep ties to Google Brain, one of the search giant's early initiatives to create software that could independently learn from the data fed to it. At Google, Gomez was also a coauthor on the research paper that invented transformers, a key technical advancement that gave rise to large language models. Cohere is now building its own AI models catered towards enterprise businesses, as well as applications to put them to use. It was last valued by investors at $2.2 billion after a $270 million funding round in June 2023. The following month, it... Read More\\n\\nFounders Aidan Gomez, Nick Frosst, and Ivan Zhang have deep ties to Google Brain, one of the search giant's early initiatives to create software that could independently learn from the data fed to it. At Google, Gomez was also a coauthor on the research paper that invented transformers, a key technical advancement that gave rise to large language models. Cohere is now building its own AI models catered towards enterprise businesses, as well as applications to put them to use. It was last valued by investors at $2.2 billion after a $270 million funding round in June 2023. The following month, it launched Coral, a ChatGPT-style bot that lets users search through their company's proprietary data to find relevant information. In March 2024, it released Command-R, its latest model, which is designed for large-scale data requests in the workplace. Read Less\\n\\nCohere Company Stats\\n\\nCanada's Best Startup Employers (2024)\\n\\nRelated People & Companies\\n\\nRelated by Industry: AI model developerView Profile\\n\\nRelated by Industry: AI model developerView Profile\\n\\nLocated in CanadaView Profile\\n\\n$150 Million And Counting: How Much Trump’s Attorneys Have Paid For Trying To Overturn The 2020 Election—As Giuliani Ordered To Post Bond\\n\\nGiuliani pleaded not guilty Tuesday to nine state felony charges in Arizona.\\n\\nByAlison DurkeeForbes Staff\\n\\nSeveral Killed As Powerful Tornado Hits Greenfield, Iowa\\n\\nAfter devastating the small Iowa town of Greenfield, the storms moved towards Wisconsin and Illinois, knocking out power supply to thousands of homes.\\n\\nBySiladitya RayForbes Staff\\n\\nVfB Stuttgart Set New Hospitality Benchmark With Porsche Tunnel Club\\n\\nWith the opening of the new Porsche Tunnel Club, VfB Stuttgart has set a new innovative benchmark off the pitch.\\n\\nByManuel VethContributor\\n\\nIran’s President Dies In A Strange Copter Crash: What Now?\\n\\nThe death of Iran's President Ebrahim Raisi and in a helicopter crash recently has spurred all manner of excitable speculation.\\n\\nByMelik KaylanContributor\\n\\nTin Matches Copper’s 38% Rise, With A Blowout Possible\\n\\nCopper is the hot metal for investors thanks to a 38% price rise over the last 12 months, an increase matched by tin which could soon outperform copper.\\n\\nByTim TreadgoldContributor\\n\\nIndia At Cannes 2024: AR Rahman’s documentary’s First Look Launched\\n\\nCannes Film Festival 2024: A look at all the new launches for Indian films at the ongoing film festival. Two major Indian films have also been sold at Marche du Cannes.\\n\\nBySweta KaushalContributor\\n\\nSean ‘Diddy’ Combs Faces New Lawsuit Alleging He Sexually Assaulted Former Model\\n\\nFormer model Crystal McKinney alleges Combs drugged and sexually assaulted her at his New York studio in 2003.\\n\\nBySiladitya RayForbes Staff\\n\\nThe Biggie Experience: T’yanna Wallace Knows What To Do With Her Father’s Legacy\\n\\nT'yanna Wallace discussed her role as Biggie Smalls' daughter and vision for The Biggie Experience.\\n\\nByIme EkpoContributor\\n\\nConverting Food Tradition Into Science With The Periodic Table Of Food\\n\\nThe Periodic Table of Food Initiative provides a lens through which the secrets of ancient dietary wisdom can finally be scientifically validated.\\n\\nByDaphne Ewing-ChowSenior Contributor\\n\\nRicky Rudd, Carl Edwards And Ralph Moody Selected To Nascar Hall Of Fame Class Of 2025\\n\\nTwo Nascar drivers who never win a Cup Series championship are now Nascar Hall of Fame members. Ricky Rudd and Carl Edwards were elected with Ralph Moody.\\n\\nByJoseph WolkinContributor\\n\\nMatt Damon Movie Dud Falls Off Netflix\\ufeff Top 10 Global Chart After 1 Week\\n\\nNetflix viewers have apparently seen enough of a Matt Damon movie bust from 2017.\\n\\nByTim LammersContributor\\n\\nHow The Thunder Can Become Championship Favorites By Next Season\\n\\nThree offseason moves can help catapult the Oklahoma City Thunder into becoming championship favorites by next season.\\n\\nByMorten Stig JensenContributor\\n\\nTrump Prosecutor Fani Willis Easily Wins Her Democratic Primary, While Judge In Georgia Case Reelected\\n\\nWillis still needs to win the general election in November to keep her seat.\\n\\nByAntonio Pequeño IVForbes Staff\\n\\nLouisiana House Passes Bill Making Abortion Pills Controlled Substances—A First In The Country\\n\\nMedication abortions were used for more than half of all U.S. abortions in 2023.\\n\\nByAntonio Pequeño IVForbes Staff\\n\\nThese Are The Top Cities Where Gen Zs And Young Millennials Are Buying Their Homes—And Where They’re Not\\n\\nYoung homebuyers are flocking to affordable cities like Pittsburgh and Cincinnati, while making up the smallest share of new mortgages in wealthy retirement hotspots.\\n\\nByHyunsoo RimContributor\\n\\nToday’s ‘Wordle’ #1068 Hints, Clues And Answer For Wednesday, May 22nd\\n\\nHints, clues and commentary to help you solve today's Wordle and sharpen your guessing game.\\n\\nByErik KainSenior Contributor\\n\\nCapitalize on your AchievementClaim This Profile\\n\\n$150 Million And Counting: How Much Trump’s Attorneys Have Paid For Trying To Overturn The 2020 Election—As Giuliani Ordered To Post Bond\\n\\nSeveral Killed As Powerful Tornado Hits Greenfield, Iowa\\n\\nVfB Stuttgart Set New Hospitality Benchmark With Porsche Tunnel Club\\n\\nUse Forbes logos and quotes in your marketing.\\n\\nLogo LicensingReprints/PlaquesForbes Profile\", metadata={'id': 'web-search_3', 'snippet': \"From the Editor: Notable and Noteworthy\\n\\nFounders Aidan Gomez, Nick Frosst, and Ivan Zhang have deep ties to Google Brain, one of the search giant's early initiatives to create software that could independently learn from the data fed to it. At Google, Gomez was also a coauthor on the research paper that invented transformers, a key technical advancement that gave rise to large language models. Cohere is now building its own AI models catered towards enterprise businesses, as well as applications to put them to use. It was last valued by investors at $2.2 billion after a $270 million funding round in June 2023. The following month, it... Read More\\n\\nFounders Aidan Gomez, Nick Frosst, and Ivan Zhang have deep ties to Google Brain, one of the search giant's early initiatives to create software that could independently learn from the data fed to it. At Google, Gomez was also a coauthor on the research paper that invented transformers, a key technical advancement that gave rise to large language models. Cohere is now building its own AI models catered towards enterprise businesses, as well as applications to put them to use. It was last valued by investors at $2.2 billion after a $270 million funding round in June 2023. The following month, it launched Coral, a ChatGPT-style bot that lets users search through their company's proprietary data to find relevant information. In March 2024, it released Command-R, its latest model, which is designed for large-scale data requests in the workplace. Read Less\\n\\nCohere Company Stats\\n\\nCanada's Best Startup Employers (2024)\\n\\nRelated People & Companies\\n\\nRelated by Industry: AI model developerView Profile\\n\\nRelated by Industry: AI model developerView Profile\\n\\nLocated in CanadaView Profile\\n\\n$150 Million And Counting: How Much Trump’s Attorneys Have Paid For Trying To Overturn The 2020 Election—As Giuliani Ordered To Post Bond\\n\\nGiuliani pleaded not guilty Tuesday to nine state felony charges in Arizona.\\n\\nByAlison DurkeeForbes Staff\\n\\nSeveral Killed As Powerful Tornado Hits Greenfield, Iowa\\n\\nAfter devastating the small Iowa town of Greenfield, the storms moved towards Wisconsin and Illinois, knocking out power supply to thousands of homes.\\n\\nBySiladitya RayForbes Staff\\n\\nVfB Stuttgart Set New Hospitality Benchmark With Porsche Tunnel Club\\n\\nWith the opening of the new Porsche Tunnel Club, VfB Stuttgart has set a new innovative benchmark off the pitch.\\n\\nByManuel VethContributor\\n\\nIran’s President Dies In A Strange Copter Crash: What Now?\\n\\nThe death of Iran's President Ebrahim Raisi and in a helicopter crash recently has spurred all manner of excitable speculation.\\n\\nByMelik KaylanContributor\\n\\nTin Matches Copper’s 38% Rise, With A Blowout Possible\\n\\nCopper is the hot metal for investors thanks to a 38% price rise over the last 12 months, an increase matched by tin which could soon outperform copper.\\n\\nByTim TreadgoldContributor\\n\\nIndia At Cannes 2024: AR Rahman’s documentary’s First Look Launched\\n\\nCannes Film Festival 2024: A look at all the new launches for Indian films at the ongoing film festival. Two major Indian films have also been sold at Marche du Cannes.\\n\\nBySweta KaushalContributor\\n\\nSean ‘Diddy’ Combs Faces New Lawsuit Alleging He Sexually Assaulted Former Model\\n\\nFormer model Crystal McKinney alleges Combs drugged and sexually assaulted her at his New York studio in 2003.\\n\\nBySiladitya RayForbes Staff\\n\\nThe Biggie Experience: T’yanna Wallace Knows What To Do With Her Father’s Legacy\\n\\nT'yanna Wallace discussed her role as Biggie Smalls' daughter and vision for The Biggie Experience.\\n\\nByIme EkpoContributor\\n\\nConverting Food Tradition Into Science With The Periodic Table Of Food\\n\\nThe Periodic Table of Food Initiative provides a lens through which the secrets of ancient dietary wisdom can finally be scientifically validated.\\n\\nByDaphne Ewing-ChowSenior Contributor\\n\\nRicky Rudd, Carl Edwards And Ralph Moody Selected To Nascar Hall Of Fame Class Of 2025\\n\\nTwo Nascar drivers who never win a Cup Series championship are now Nascar Hall of Fame members. Ricky Rudd and Carl Edwards were elected with Ralph Moody.\\n\\nByJoseph WolkinContributor\\n\\nMatt Damon Movie Dud Falls Off Netflix\\ufeff Top 10 Global Chart After 1 Week\\n\\nNetflix viewers have apparently seen enough of a Matt Damon movie bust from 2017.\\n\\nByTim LammersContributor\\n\\nHow The Thunder Can Become Championship Favorites By Next Season\\n\\nThree offseason moves can help catapult the Oklahoma City Thunder into becoming championship favorites by next season.\\n\\nByMorten Stig JensenContributor\\n\\nTrump Prosecutor Fani Willis Easily Wins Her Democratic Primary, While Judge In Georgia Case Reelected\\n\\nWillis still needs to win the general election in November to keep her seat.\\n\\nByAntonio Pequeño IVForbes Staff\\n\\nLouisiana House Passes Bill Making Abortion Pills Controlled Substances—A First In The Country\\n\\nMedication abortions were used for more than half of all U.S. abortions in 2023.\\n\\nByAntonio Pequeño IVForbes Staff\\n\\nThese Are The Top Cities Where Gen Zs And Young Millennials Are Buying Their Homes—And Where They’re Not\\n\\nYoung homebuyers are flocking to affordable cities like Pittsburgh and Cincinnati, while making up the smallest share of new mortgages in wealthy retirement hotspots.\\n\\nByHyunsoo RimContributor\\n\\nToday’s ‘Wordle’ #1068 Hints, Clues And Answer For Wednesday, May 22nd\\n\\nHints, clues and commentary to help you solve today's Wordle and sharpen your guessing game.\\n\\nByErik KainSenior Contributor\\n\\nCapitalize on your AchievementClaim This Profile\\n\\n$150 Million And Counting: How Much Trump’s Attorneys Have Paid For Trying To Overturn The 2020 Election—As Giuliani Ordered To Post Bond\\n\\nSeveral Killed As Powerful Tornado Hits Greenfield, Iowa\\n\\nVfB Stuttgart Set New Hospitality Benchmark With Porsche Tunnel Club\\n\\nUse Forbes logos and quotes in your marketing.\\n\\nLogo LicensingReprints/PlaquesForbes Profile\", 'timestamp': '2024-05-22T09:33:02', 'title': 'Cohere | Company Overview & News', 'url': 'https://www.forbes.com/companies/cohere/'}),\n",
      " Document(page_content='Introducing Command R+: Our new, most powerful model in the Command R family. Learn More\\n\\nBuild a future without a language barrier between humans and machines\\n\\nCohere helps businesses explore, generate, search for, and act upon information in a new way that’s more intuitive and more natural than ever before.\\n\\nMeet the enterprise conversational AI agent\\n\\nPower an enterprise chat agent that answers questions grounded in your company’s knowledge, and that can take actions and drive processes.\\n\\nWhich products are gaining the most traction in France?\\n\\nOur top selling item was Product A, with 4.5m EUR monthly revenue. However, Product B is growing 40% year over year, with 2.3m EUR monthly revenue\\n\\nsource: EMEA_Q1_2023_QBR_final.ppt\\n\\nWhat is the sales outlook for Product A in France?\\n\\nCustomer A may sign a 6m EUR annual contract, however, there are issues with support and compliance requirements, so this is a 30% probability. Also, Customer B may move production to Slovakia in Q3, which risks 0.2m EUR in monthly revenue.\\n\\nsource: FR_202304_Pipeline.doc\\n\\nCan you summarize the most recent internal communications about Customer A?\\n\\nSure. Customer A’s support volume has risen by 40% in the last quarter, the customer is asking for a new certification process in Q2, but manufacturing does not believe it can support this until Q4. The relevant report is here:\\n\\nsource: Conversation Summary from #CustomerAChannel\\n\\nPlease schedule a 30 minute call with the France regional manager, topic “Customer A pipeline”\\n\\nDone. Scheduled for 8:30am ET on Thursday 26th (2 days from now)\\n\\nWhich products are gaining the most traction in France?\\n\\nWhich products are gaining the most traction in France?\\n\\nCohere’s AI tools are being used broadly across multiple functions today. Don’t see your use case? Reach out and we can discuss.\\n\\nTop Performing language models\\n\\nOur top performing Generative models (HELM benchmarks) are updated weekly to improve performance. Our state-of-the-art multilingual embedding model enables semantic search, classification and sentiment analysis across 109 languages.\\n\\nSecure, works in your cloud\\n\\nOur language models can be deployed to cloud platforms (AWS, Oracle, Google) and VPCs, offering you flexibility, control over your data, and the highest levels of data security and privacy.\\n\\nEase of customization\\n\\nOur language models are customizable for your unique use case, and can be easily integrated with your applications. No scarce ML experience needed.\\n\\nFor many customers, language AI is new. Cohere and its growing partner base is dedicated to helping you build solutions that find business value.\\n\\nTop Performing language models\\n\\nOur top performing Generative models (HELM benchmarks) are updated weekly to improve performance. Our state-of-the-art multilingual embedding model enables semantic search, classification and sentiment analysis across 109 languages.\\n\\nSecure, works in your cloud\\n\\nOur language models can be deployed to cloud platforms (AWS, Oracle, Google) and VPCs, offering you flexibility, control over your data, and the highest levels of data security and privacy.\\n\\nEase of customization\\n\\nOur language models are customizable for your unique use case, and can be easily integrated with your applications. No scarce ML experience needed.\\n\\nFor many customers, language AI is new. Cohere and its growing partner base is dedicated to helping you build solutions that find business value.\\n\\nHow Cohere is being used today\\n\\nCohere’s AI tools are being used broadly across multiple functions today. Don’t see your use case? Reach out and we can discuss\\n\\nWrite product descriptions, blog posts, articles, and marketing copy with scalable, affordable generative AI tools.\\n\\nExtract concise, accurate summaries of articles, emails, and documents.\\n\\nBuild accurate, high-performance semantic text search across any document type in English or in 100+ languages.\\n\\nRun text classification for customer support routing, intent recognition, sentiment analysis, and more.\\n\\nAccess a managed embedding model that outperforms OSS and works in both English and 100+ languages to develop your own capabilities.', metadata={'id': 'web-search_4', 'snippet': 'Introducing Command R+: Our new, most powerful model in the Command R family. Learn More\\n\\nBuild a future without a language barrier between humans and machines\\n\\nCohere helps businesses explore, generate, search for, and act upon information in a new way that’s more intuitive and more natural than ever before.\\n\\nMeet the enterprise conversational AI agent\\n\\nPower an enterprise chat agent that answers questions grounded in your company’s knowledge, and that can take actions and drive processes.\\n\\nWhich products are gaining the most traction in France?\\n\\nOur top selling item was Product A, with 4.5m EUR monthly revenue. However, Product B is growing 40% year over year, with 2.3m EUR monthly revenue\\n\\nsource: EMEA_Q1_2023_QBR_final.ppt\\n\\nWhat is the sales outlook for Product A in France?\\n\\nCustomer A may sign a 6m EUR annual contract, however, there are issues with support and compliance requirements, so this is a 30% probability. Also, Customer B may move production to Slovakia in Q3, which risks 0.2m EUR in monthly revenue.\\n\\nsource: FR_202304_Pipeline.doc\\n\\nCan you summarize the most recent internal communications about Customer A?\\n\\nSure. Customer A’s support volume has risen by 40% in the last quarter, the customer is asking for a new certification process in Q2, but manufacturing does not believe it can support this until Q4. The relevant report is here:\\n\\nsource: Conversation Summary from #CustomerAChannel\\n\\nPlease schedule a 30 minute call with the France regional manager, topic “Customer A pipeline”\\n\\nDone. Scheduled for 8:30am ET on Thursday 26th (2 days from now)\\n\\nWhich products are gaining the most traction in France?\\n\\nWhich products are gaining the most traction in France?\\n\\nCohere’s AI tools are being used broadly across multiple functions today. Don’t see your use case? Reach out and we can discuss.\\n\\nTop Performing language models\\n\\nOur top performing Generative models (HELM benchmarks) are updated weekly to improve performance. Our state-of-the-art multilingual embedding model enables semantic search, classification and sentiment analysis across 109 languages.\\n\\nSecure, works in your cloud\\n\\nOur language models can be deployed to cloud platforms (AWS, Oracle, Google) and VPCs, offering you flexibility, control over your data, and the highest levels of data security and privacy.\\n\\nEase of customization\\n\\nOur language models are customizable for your unique use case, and can be easily integrated with your applications. No scarce ML experience needed.\\n\\nFor many customers, language AI is new. Cohere and its growing partner base is dedicated to helping you build solutions that find business value.\\n\\nTop Performing language models\\n\\nOur top performing Generative models (HELM benchmarks) are updated weekly to improve performance. Our state-of-the-art multilingual embedding model enables semantic search, classification and sentiment analysis across 109 languages.\\n\\nSecure, works in your cloud\\n\\nOur language models can be deployed to cloud platforms (AWS, Oracle, Google) and VPCs, offering you flexibility, control over your data, and the highest levels of data security and privacy.\\n\\nEase of customization\\n\\nOur language models are customizable for your unique use case, and can be easily integrated with your applications. No scarce ML experience needed.\\n\\nFor many customers, language AI is new. Cohere and its growing partner base is dedicated to helping you build solutions that find business value.\\n\\nHow Cohere is being used today\\n\\nCohere’s AI tools are being used broadly across multiple functions today. Don’t see your use case? Reach out and we can discuss\\n\\nWrite product descriptions, blog posts, articles, and marketing copy with scalable, affordable generative AI tools.\\n\\nExtract concise, accurate summaries of articles, emails, and documents.\\n\\nBuild accurate, high-performance semantic text search across any document type in English or in 100+ languages.\\n\\nRun text classification for customer support routing, intent recognition, sentiment analysis, and more.\\n\\nAccess a managed embedding model that outperforms OSS and works in both English and 100+ languages to develop your own capabilities.', 'timestamp': '2024-05-22T09:33:03', 'title': 'Enterprise Solutions - Advanced NLP for Large Organizations | Cohere', 'url': 'https://cohere.com/business'})]\n",
      "'Question: Who are Cohere?'\n",
      "'Answer:'\n",
      "('Cohere is an AI company that builds language models to help businesses. '\n",
      " 'Their models are designed to augment and elevate the global workforce, so '\n",
      " \"businesses can thrive and stay competitive in the AI era. Cohere's founders \"\n",
      " 'Aidan Gomez, Nick Frosst, and Ivan Zhang have deep ties to Google Brain, one '\n",
      " \"of the search giant's early initiatives to create software that could \"\n",
      " 'independently learn from the data fed to it.')\n",
      "[ChatCitation(start=13, end=23, text='AI company', document_ids=['web-search_0', 'web-search_1', 'web-search_3', 'web-search_4']),\n",
      " ChatCitation(start=36, end=51, text='language models', document_ids=['web-search_0', 'web-search_1', 'web-search_3', 'web-search_4']),\n",
      " ChatCitation(start=60, end=71, text='businesses.', document_ids=['web-search_0', 'web-search_1', 'web-search_3', 'web-search_4']),\n",
      " ChatCitation(start=101, end=141, text='augment and elevate the global workforce', document_ids=['web-search_1']),\n",
      " ChatCitation(start=161, end=203, text='thrive and stay competitive in the AI era.', document_ids=['web-search_1']),\n",
      " ChatCitation(start=213, end=262, text='founders Aidan Gomez, Nick Frosst, and Ivan Zhang', document_ids=['web-search_3']),\n",
      " ChatCitation(start=268, end=293, text='deep ties to Google Brain', document_ids=['web-search_3']),\n",
      " ChatCitation(start=306, end=338, text=\"search giant's early initiatives\", document_ids=['web-search_3']),\n",
      " ChatCitation(start=349, end=413, text='software that could independently learn from the data fed to it.', document_ids=['web-search_3'])]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from langchain_cohere import ChatCohere, CohereRagRetriever\n",
    "\n",
    "# User query we will use for the generation\n",
    "user_query = \"Who are Cohere?\"\n",
    "\n",
    "\n",
    "# Use Cohere's RAG retriever with Cohere Connectors to generate an answer.\n",
    "# Cohere provides exact citations for the sources it used.\n",
    "llm = ChatCohere()\n",
    "rag = CohereRagRetriever(llm=llm, connectors=[{\"id\": \"web-search\"}])\n",
    "docs = rag.get_relevant_documents(user_query)\n",
    "answer = docs.pop()\n",
    "\n",
    "pprint(\"Relevant documents:\")\n",
    "pprint(docs)\n",
    "\n",
    "pprint(f\"Question: {user_query}\")\n",
    "pprint(\"Answer:\")\n",
    "pprint(answer.page_content)\n",
    "pprint(answer.metadata[\"citations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Relevant documents:'\n",
      "[Document(page_content='LangChain supports Cohere RAG!', metadata={'id': 'doc-0', 'text': 'LangChain supports Cohere RAG!'})]\n",
      "'Question: Does LangChain support Cohere RAG?'\n",
      "'Answer:'\n",
      "'Yes, LangChain supports Cohere RAG.'\n",
      "[ChatCitation(start=0, end=3, text='Yes', document_ids=['doc-0'])]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from langchain_cohere import ChatCohere, CohereRagRetriever\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# User query we will use for the generation\n",
    "user_query = \"Does LangChain support Cohere RAG?\"\n",
    "\n",
    "# Use Cohere's RAG retriever in document mode to generate an answer.\n",
    "# Cohere provides exact citations for the sources it used.\n",
    "llm = ChatCohere()\n",
    "rag = CohereRagRetriever(llm=llm, connectors=[])\n",
    "docs = rag.get_relevant_documents(\n",
    "    user_query,\n",
    "    documents=[\n",
    "        Document(page_content=\"LangChain supports Cohere RAG!\"),\n",
    "        Document(page_content=\"The sky is blue!\"),\n",
    "    ],\n",
    ")\n",
    "answer = docs.pop()\n",
    "\n",
    "pprint(\"Relevant documents:\")\n",
    "pprint(docs)\n",
    "\n",
    "pprint(f\"Question: {user_query}\")\n",
    "pprint(\"Answer:\")\n",
    "pprint(answer.page_content)\n",
    "pprint(answer.metadata[\"citations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for chroma-hnswlib (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [5 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_ext\n",
      "      building 'hnswlib' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for chroma-hnswlib\n",
      "ERROR: Could not build wheels for chroma-hnswlib, which is required to install pyproject.toml-based projects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langflow\n",
      "  Downloading langflow-1.0.0a36-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting assemblyai<0.24.0,>=0.23.1 (from langflow)\n",
      "  Downloading assemblyai-0.23.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting beautifulsoup4<5.0.0,>=4.12.2 (from langflow)\n",
      "  Downloading beautifulsoup4-4.13.0b2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting boto3<2.0.0,>=1.34.0 (from langflow)\n",
      "  Using cached boto3-1.34.111-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting certifi<2024.0.0,>=2023.11.17 (from langflow)\n",
      "  Downloading certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting chromadb<0.5.0,>=0.4.24 (from langflow)\n",
      "  Downloading chromadb-0.4.24-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting cohere<6.0.0,>=5.1.7 (from langflow)\n",
      "  Using cached cohere-5.5.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dspy-ai<3.0.0,>=2.4.0 (from langflow)\n",
      "  Downloading dspy_ai-2.4.9-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting elasticsearch<9.0.0,>=8.12.0 (from langflow)\n",
      "  Downloading elasticsearch-8.13.1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting extract-msg<0.48.0,>=0.47.0 (from langflow)\n",
      "  Downloading extract_msg-0.47.0-py2.py3-none-any.whl.metadata (15 kB)\n",
      "Collecting faiss-cpu<2.0.0,>=1.7.4 (from langflow)\n",
      "  Downloading faiss_cpu-1.8.0-cp312-cp312-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting fake-useragent<2.0.0,>=1.4.0 (from langflow)\n",
      "  Downloading fake_useragent-1.5.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting fastavro<2.0.0,>=1.8.0 (from langflow)\n",
      "  Using cached fastavro-1.9.4-cp312-cp312-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting google-api-python-client<3.0.0,>=2.118.0 (from langflow)\n",
      "  Downloading google_api_python_client-2.130.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-search-results<3.0.0,>=2.4.1 (from langflow)\n",
      "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting html2text<2025.0.0,>=2024.2.26 (from langflow)\n",
      "  Downloading html2text-2024.2.26.tar.gz (56 kB)\n",
      "     ---------------------------------------- 0.0/56.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 56.5/56.5 kB ? eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting huggingface-hub<0.21.0,>=0.20.0 (from huggingface-hub[inference]<0.21.0,>=0.20.0->langflow)\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting langchain-anthropic<0.2.0,>=0.1.6 (from langflow)\n",
      "  Downloading langchain_anthropic-0.1.13-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting langchain-astradb<0.2.0,>=0.1.0 (from langflow)\n",
      "  Downloading langchain_astradb-0.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting langchain-cohere<0.2.0,>=0.1.0rc1 (from langflow)\n",
      "  Using cached langchain_cohere-0.1.5-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting langchain-google-genai<2.0.0,>=1.0.1 (from langflow)\n",
      "  Downloading langchain_google_genai-1.0.5-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting langchain-google-vertexai<2.0.0,>=1.0.3 (from langflow)\n",
      "  Downloading langchain_google_vertexai-1.0.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting langchain-groq<0.2.0,>=0.1.3 (from langflow)\n",
      "  Downloading langchain_groq-0.1.4-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting langchain-mistralai<0.2.0,>=0.1.6 (from langflow)\n",
      "  Downloading langchain_mistralai-0.1.7-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting langchain-openai<0.2.0,>=0.1.1 (from langflow)\n",
      "  Downloading langchain_openai-0.1.7-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting langchain-pinecone<0.2.0,>=0.1.0 (from langflow)\n",
      "  Downloading langchain_pinecone-0.1.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting langflow-base<0.0.48,>=0.0.47 (from langflow)\n",
      "  Downloading langflow_base-0.0.47-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langfuse<3.0.0,>=2.9.0 (from langflow)\n",
      "  Downloading langfuse-2.33.1a0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting litellm<2.0.0,>=1.34.22 (from langflow)\n",
      "  Downloading litellm-1.38.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting llama-index<0.11.0,>=0.10.13 (from langflow)\n",
      "  Downloading llama_index-0.10.38-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting markupsafe<3.0.0,>=2.1.3 (from langflow)\n",
      "  Using cached MarkupSafe-2.1.5-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting metal-sdk<3.0.0,>=2.5.0 (from langflow)\n",
      "  Downloading metal_sdk-2.5.1-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting metaphor-python<0.2.0,>=0.1.11 (from langflow)\n",
      "  Downloading metaphor_python-0.1.23-py3-none-any.whl.metadata (636 bytes)\n",
      "Collecting networkx<4.0,>=3.1 (from langflow)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.6 (from langflow)\n",
      "  Downloading numexpr-2.10.0-cp312-cp312-win_amd64.whl.metadata (8.1 kB)\n",
      "Collecting pgvector<0.3.0,>=0.2.3 (from langflow)\n",
      "  Downloading pgvector-0.2.5-py2.py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting pinecone-client<4.0.0,>=3.0.3 (from langflow)\n",
      "  Using cached pinecone_client-3.2.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting psycopg<4.0.0,>=3.1.9 (from langflow)\n",
      "  Downloading psycopg-3.1.19-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting psycopg-binary<4.0.0,>=3.1.9 (from langflow)\n",
      "  Downloading psycopg_binary-3.1.19-cp312-cp312-win_amd64.whl.metadata (2.9 kB)\n",
      "Collecting psycopg2-binary<3.0.0,>=2.9.6 (from langflow)\n",
      "  Downloading psycopg2_binary-2.9.9-cp312-cp312-win_amd64.whl.metadata (4.6 kB)\n",
      "Collecting pyarrow<15.0.0,>=14.0.0 (from langflow)\n",
      "  Downloading pyarrow-14.0.2-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting pyautogen<0.3.0,>=0.2.0 (from langflow)\n",
      "  Downloading pyautogen-0.2.27-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting pymongo<5.0.0,>=4.6.0 (from langflow)\n",
      "  Downloading pymongo-4.7.2-cp312-cp312-win_amd64.whl.metadata (22 kB)\n",
      "Collecting pysrt<2.0.0,>=1.1.2 (from langflow)\n",
      "  Downloading pysrt-1.1.2.tar.gz (104 kB)\n",
      "     ---------------------------------------- 0.0/104.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 104.4/104.4 kB 5.9 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pytube<16.0.0,>=15.0.0 (from langflow)\n",
      "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting pywin32<307,>=306 (from langflow)\n",
      "  Using cached pywin32-306-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting qdrant-client<2.0.0,>=1.7.0 (from langflow)\n",
      "  Downloading qdrant_client-1.9.1-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting qianfan==0.3.5 (from langflow)\n",
      "  Downloading qianfan-0.3.5-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting supabase<3.0.0,>=2.3.0 (from langflow)\n",
      "  Downloading supabase-2.4.6-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting types-cachetools<6.0.0.0,>=5.3.0.5 (from langflow)\n",
      "  Downloading types_cachetools-5.3.0.7-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting weaviate-client (from langflow)\n",
      "  Downloading weaviate_client-4.7.0a0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting wikipedia<2.0.0,>=1.4.0 (from langflow)\n",
      "  Using cached wikipedia-1.4.0-py3-none-any.whl\n",
      "Collecting zep-python<3.0.0,>=2.0.0rc5 (from langflow)\n",
      "  Downloading zep_python-2.0.0rc6-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting aiohttp>=3.7.0 (from qianfan==0.3.5->langflow)\n",
      "  Using cached aiohttp-3.9.5-cp312-cp312-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting aiolimiter>=1.1.0 (from qianfan==0.3.5->langflow)\n",
      "  Downloading aiolimiter-1.1.0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting bce-python-sdk>=0.8.79 (from qianfan==0.3.5->langflow)\n",
      "  Downloading bce_python_sdk-0.9.10-py3-none-any.whl.metadata (319 bytes)\n",
      "Collecting clevercsv (from qianfan==0.3.5->langflow)\n",
      "  Downloading clevercsv-0.8.2-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting ijson (from qianfan==0.3.5->langflow)\n",
      "  Downloading ijson-3.2.3-cp312-cp312-win_amd64.whl.metadata (21 kB)\n",
      "Collecting multiprocess (from qianfan==0.3.5->langflow)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting numpy>=1.22.0 (from qianfan==0.3.5->langflow)\n",
      "  Downloading numpy-2.0.0rc2-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.9/60.9 kB ? eta 0:00:00\n",
      "Collecting prompt-toolkit>=3.0.38 (from qianfan==0.3.5->langflow)\n",
      "  Using cached prompt_toolkit-3.0.43-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pydantic (from qianfan==0.3.5->langflow)\n",
      "  Downloading pydantic-2.7.1-py3-none-any.whl.metadata (107 kB)\n",
      "     ---------------------------------------- 0.0/107.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 107.3/107.3 kB ? eta 0:00:00\n",
      "Collecting python-dateutil<3.0.0,>=2.8.2 (from qianfan==0.3.5->langflow)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting python-dotenv>=1.0 (from qianfan==0.3.5->langflow)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting pyyaml<7.0.0,>=6.0.1 (from qianfan==0.3.5->langflow)\n",
      "  Using cached PyYAML-6.0.1-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting requests>=2.24 (from qianfan==0.3.5->langflow)\n",
      "  Downloading requests-2.32.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting rich>=13.0.0 (from qianfan==0.3.5->langflow)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tenacity<9.0.0,>=8.2.3 (from qianfan==0.3.5->langflow)\n",
      "  Downloading tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting typer>=0.9.0 (from qianfan==0.3.5->langflow)\n",
      "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting httpx>=0.19.0 (from assemblyai<0.24.0,>=0.23.1->langflow)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting typing-extensions>=3.7 (from assemblyai<0.24.0,>=0.23.1->langflow)\n",
      "  Downloading typing_extensions-4.12.0rc1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting websockets>=11.0 (from assemblyai<0.24.0,>=0.23.1->langflow)\n",
      "  Downloading websockets-12.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.12.2->langflow)\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting botocore<1.35.0,>=1.34.111 (from boto3<2.0.0,>=1.34.0->langflow)\n",
      "  Using cached botocore-1.34.111-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->langflow)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->langflow)\n",
      "  Using cached s3transfer-0.10.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting build>=1.0.3 (from chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading build-1.2.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting chroma-hnswlib==0.7.3 (from chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading chroma-hnswlib-0.7.3.tar.gz (31 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting fastapi>=0.95.2 (from chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading fastapi-0.111.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading uvicorn-0.29.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting pulsar-client>=3.1.0 (from chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading pulsar_client-3.5.0-cp312-cp312-win_amd64.whl.metadata (1.0 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading onnxruntime-1.18.0-cp312-cp312-win_amd64.whl.metadata (4.4 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading opentelemetry_api-1.24.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Using cached tokenizers-0.19.1-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "     ---------------------------------------- 0.0/67.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 67.3/67.3 kB ? eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting tqdm>=4.65.0 (from chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB ? eta 0:00:00\n",
      "Collecting overrides>=7.3.1 (from chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading grpcio-1.64.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading bcrypt-4.1.3-cp39-abi3-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading kubernetes-30.1.0a1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading mmh3-4.1.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting orjson>=3.9.12 (from chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading orjson-3.10.3-cp312-none-win_amd64.whl.metadata (50 kB)\n",
      "     ---------------------------------------- 0.0/50.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.9/50.9 kB ? eta 0:00:00\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0 (from cohere<6.0.0,>=5.1.7->langflow)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0.0,>=5.1.7->langflow)\n",
      "  Downloading types_requests-2.32.0.20240523-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting backoff~=2.2.1 (from dspy-ai<3.0.0,>=2.4.0->langflow)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting joblib~=1.3.2 (from dspy-ai<3.0.0,>=2.4.0->langflow)\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting openai<2.0.0,>=0.28.1 (from dspy-ai<3.0.0,>=2.4.0->langflow)\n",
      "  Using cached openai-1.30.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting pandas (from dspy-ai<3.0.0,>=2.4.0->langflow)\n",
      "  Using cached pandas-2.2.2-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting regex (from dspy-ai<3.0.0,>=2.4.0->langflow)\n",
      "  Downloading regex-2024.5.15-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB ? eta 0:00:00\n",
      "Collecting ujson (from dspy-ai<3.0.0,>=2.4.0->langflow)\n",
      "  Downloading ujson-5.10.0-cp312-cp312-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting datasets<3.0.0,~=2.14.6 (from dspy-ai<3.0.0,>=2.4.0->langflow)\n",
      "  Downloading datasets-2.14.7-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting optuna (from dspy-ai<3.0.0,>=2.4.0->langflow)\n",
      "  Downloading optuna-3.6.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting structlog (from dspy-ai<3.0.0,>=2.4.0->langflow)\n",
      "  Downloading structlog-24.1.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting elastic-transport<9,>=8.13 (from elasticsearch<9.0.0,>=8.12.0->langflow)\n",
      "  Downloading elastic_transport-8.13.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting olefile==0.47 (from extract-msg<0.48.0,>=0.47.0->langflow)\n",
      "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tzlocal<6,>=4.2 (from extract-msg<0.48.0,>=0.47.0->langflow)\n",
      "  Using cached tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting compressed-rtf<2,>=1.0.6 (from extract-msg<0.48.0,>=0.47.0->langflow)\n",
      "  Downloading compressed_rtf-1.0.6.tar.gz (5.8 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting ebcdic<2,>=1.1.1 (from extract-msg<0.48.0,>=0.47.0->langflow)\n",
      "  Downloading ebcdic-1.1.1-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting beautifulsoup4<5.0.0,>=4.12.2 (from langflow)\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting RTFDE<0.2,>=0.1.1 (from extract-msg<0.48.0,>=0.47.0->langflow)\n",
      "  Downloading RTFDE-0.1.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting red-black-tree-mod==1.20 (from extract-msg<0.48.0,>=0.47.0->langflow)\n",
      "  Downloading red-black-tree-mod-1.20.tar.gz (28 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client<3.0.0,>=2.118.0->langflow)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 (from google-api-python-client<3.0.0,>=2.118.0->langflow)\n",
      "  Using cached google_auth-2.29.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client<3.0.0,>=2.118.0->langflow)\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client<3.0.0,>=2.118.0->langflow)\n",
      "  Downloading google_api_core-2.19.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client<3.0.0,>=2.118.0->langflow)\n",
      "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting filelock (from huggingface-hub<0.21.0,>=0.20.0->huggingface-hub[inference]<0.21.0,>=0.20.0->langflow)\n",
      "  Downloading filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<0.21.0,>=0.20.0->huggingface-hub[inference]<0.21.0,>=0.20.0->langflow)\n",
      "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting packaging>=20.9 (from huggingface-hub<0.21.0,>=0.20.0->huggingface-hub[inference]<0.21.0,>=0.20.0->langflow)\n",
      "  Using cached packaging-24.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting anthropic<1,>=0.26.0 (from langchain-anthropic<0.2.0,>=0.1.6->langflow)\n",
      "  Downloading anthropic-0.26.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting defusedxml<0.8.0,>=0.7.1 (from langchain-anthropic<0.2.0,>=0.1.6->langflow)\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting langchain-core<0.3,>=0.1.43 (from langchain-anthropic<0.2.0,>=0.1.6->langflow)\n",
      "  Downloading langchain_core-0.2.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting astrapy<0.8.0,>=0.7.7 (from langchain-astradb<0.2.0,>=0.1.0->langflow)\n",
      "  Downloading astrapy-0.7.7-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting langchain-core<0.3,>=0.1.43 (from langchain-anthropic<0.2.0,>=0.1.6->langflow)\n",
      "  Downloading langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting numpy>=1.22.0 (from qianfan==0.3.5->langflow)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Collecting google-generativeai<0.6.0,>=0.5.2 (from langchain-google-genai<2.0.0,>=1.0.1->langflow)\n",
      "  Downloading google_generativeai-0.5.4-py3-none-any.whl.metadata (3.9 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-google-genai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-google-genai<2.0.0,>=1.0.1 (from langflow)\n",
      "  Downloading langchain_google_genai-1.0.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting google-cloud-aiplatform<2.0.0,>=1.47.0 (from langchain-google-vertexai<2.0.0,>=1.0.3->langflow)\n",
      "  Downloading google_cloud_aiplatform-1.52.0-py2.py3-none-any.whl.metadata (30 kB)\n",
      "Collecting google-cloud-storage<3.0.0,>=2.14.0 (from langchain-google-vertexai<2.0.0,>=1.0.3->langflow)\n",
      "  Using cached google_cloud_storage-2.16.0-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting groq<1,>=0.4.1 (from langchain-groq<0.2.0,>=0.1.3->langflow)\n",
      "  Downloading groq-0.8.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai<0.2.0,>=0.1.1->langflow)\n",
      "  Downloading tiktoken-0.7.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting alembic<2.0.0,>=1.13.0 (from langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading alembic-1.13.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting asyncer<0.0.6,>=0.0.5 (from langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading asyncer-0.0.5-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading bcrypt-4.0.1-cp36-abi3-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting cachetools<6.0.0,>=5.3.1 (from langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Using cached cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting cryptography<43.0.0,>=42.0.5 (from langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Using cached cryptography-42.0.7-cp39-abi3-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting docstring-parser<0.16,>=0.15 (from langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading docstring_parser-0.15-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting duckdb<0.11.0,>=0.10.2 (from langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading duckdb-0.10.4.dev2-cp312-cp312-win_amd64.whl.metadata (787 bytes)\n",
      "Collecting emoji<3.0.0,>=2.11.0 (from langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading emoji-2.12.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting fastapi>=0.95.2 (from chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading fastapi-0.110.3-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting gunicorn<23.0.0,>=22.0.0 (from langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading gunicorn-22.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting langchain<0.2.0,>=0.1.16 (from langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading langchain-0.1.20-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langchain-experimental (from langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading langchain_experimental-0.0.59-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting langchainhub<0.2.0,>=0.1.15 (from langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading langchainhub-0.1.15-py3-none-any.whl.metadata (621 bytes)\n",
      "Collecting loguru<0.8.0,>=0.7.1 (from langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting nest-asyncio<2.0.0,>=1.6.0 (from langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Using cached nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting orjson>=3.9.12 (from chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading orjson-3.10.0-cp312-none-win_amd64.whl.metadata (50 kB)\n",
      "     ---------------------------------------- 0.0/50.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.7/50.7 kB ? eta 0:00:00\n",
      "Collecting pandas (from dspy-ai<3.0.0,>=2.4.0->langflow)\n",
      "  Downloading pandas-2.2.0-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting passlib<2.0.0,>=1.7.4 (from langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading passlib-1.7.4-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting pillow<11.0.0,>=10.2.0 (from langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Using cached pillow-10.3.0-cp312-cp312-win_amd64.whl.metadata (9.4 kB)\n",
      "Collecting platformdirs<5.0.0,>=4.2.0 (from langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading platformdirs-4.2.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.1.0 (from langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading pydantic_settings-2.2.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting pypdf<5.0.0,>=4.1.0 (from langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Using cached pypdf-4.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting python-docx<2.0.0,>=1.1.0 (from langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting python-jose<4.0.0,>=3.3.0 (from langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading python_jose-3.3.0-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting python-multipart<0.0.8,>=0.0.7 (from langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading python_multipart-0.0.7-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting python-socketio<6.0.0,>=5.11.0 (from langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading python_socketio-5.11.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting sqlmodel<0.0.17,>=0.0.16 (from langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading sqlmodel-0.0.16-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->dspy-ai<3.0.0,>=2.4.0->langflow)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->dspy-ai<3.0.0,>=2.4.0->langflow)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting idna<4.0,>=3.7 (from langfuse<3.0.0,>=2.9.0->langflow)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting packaging>=20.9 (from huggingface-hub<0.21.0,>=0.20.0->huggingface-hub[inference]<0.21.0,>=0.20.0->langflow)\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting wrapt<2.0,>=1.14 (from langfuse<3.0.0,>=2.9.0->langflow)\n",
      "  Using cached wrapt-1.16.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting click (from litellm<2.0.0,>=1.34.22->langflow)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting importlib-metadata>=6.8.0 (from litellm<2.0.0,>=1.34.22->langflow)\n",
      "  Downloading importlib_metadata-7.1.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting jinja2<4.0.0,>=3.1.2 (from litellm<2.0.0,>=1.34.22->langflow)\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index<0.11.0,>=0.10.13->langflow)\n",
      "  Using cached llama_index_agent_openai-0.2.5-py3-none-any.whl.metadata (678 bytes)\n",
      "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index<0.11.0,>=0.10.13->langflow)\n",
      "  Using cached llama_index_cli-0.1.12-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.11.0,>=0.10.38 (from llama-index<0.11.0,>=0.10.13->langflow)\n",
      "  Downloading llama_index_core-0.10.38.post2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index<0.11.0,>=0.10.13->langflow)\n",
      "  Downloading llama_index_embeddings_openai-0.1.10-py3-none-any.whl.metadata (604 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama-index<0.11.0,>=0.10.13->langflow)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index<0.11.0,>=0.10.13->langflow)\n",
      "  Using cached llama_index_legacy-0.9.48-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index<0.11.0,>=0.10.13->langflow)\n",
      "  Downloading llama_index_llms_openai-0.1.20-py3-none-any.whl.metadata (559 bytes)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index<0.11.0,>=0.10.13->langflow)\n",
      "  Using cached llama_index_multi_modal_llms_openai-0.1.6-py3-none-any.whl.metadata (677 bytes)\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index<0.11.0,>=0.10.13->langflow)\n",
      "  Using cached llama_index_program_openai-0.1.6-py3-none-any.whl.metadata (715 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index<0.11.0,>=0.10.13->langflow)\n",
      "  Using cached llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index<0.11.0,>=0.10.13->langflow)\n",
      "  Using cached llama_index_readers_file-0.1.22-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index<0.11.0,>=0.10.13->langflow)\n",
      "  Using cached llama_index_readers_llama_parse-0.1.4-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting urllib3>=1.26.5 (from pinecone-client<4.0.0,>=3.0.3->langflow)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting diskcache (from pyautogen<0.3.0,>=0.2.0->langflow)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting docker (from pyautogen<0.3.0,>=0.2.0->langflow)\n",
      "  Downloading docker-7.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting flaml (from pyautogen<0.3.0,>=0.2.0->langflow)\n",
      "  Downloading FLAML-2.1.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting termcolor (from pyautogen<0.3.0,>=0.2.0->langflow)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=4.6.0->langflow)\n",
      "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting chardet (from pysrt<2.0.0,>=1.1.2->langflow)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting grpcio-tools>=1.41.0 (from qdrant-client<2.0.0,>=1.7.0->langflow)\n",
      "  Downloading grpcio_tools-1.64.0-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client<2.0.0,>=1.7.0->langflow)\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting gotrue<3.0,>=1.3 (from supabase<3.0.0,>=2.3.0->langflow)\n",
      "  Downloading gotrue-2.4.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting postgrest<0.17.0,>=0.14 (from supabase<3.0.0,>=2.3.0->langflow)\n",
      "  Downloading postgrest-0.16.4-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting realtime<2.0.0,>=1.0.0 (from supabase<3.0.0,>=2.3.0->langflow)\n",
      "  Downloading realtime-1.0.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting storage3<0.8.0,>=0.5.3 (from supabase<3.0.0,>=2.3.0->langflow)\n",
      "  Downloading storage3-0.7.4-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting supafunc<0.5.0,>=0.3.1 (from supabase<3.0.0,>=2.3.0->langflow)\n",
      "  Downloading supafunc-0.4.5-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting validators==0.28.1 (from weaviate-client->langflow)\n",
      "  Downloading validators-0.28.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting authlib<2.0.0,>=1.2.1 (from weaviate-client->langflow)\n",
      "  Downloading Authlib-1.3.0-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting grpcio-health-checking<2.0.0,>=1.57.0 (from weaviate-client->langflow)\n",
      "  Downloading grpcio_health_checking-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp>=3.7.0->qianfan==0.3.5->langflow)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp>=3.7.0->qianfan==0.3.5->langflow)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp>=3.7.0->qianfan==0.3.5->langflow)\n",
      "  Using cached frozenlist-1.4.1-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.7.0->qianfan==0.3.5->langflow)\n",
      "  Using cached multidict-6.0.5-cp312-cp312-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp>=3.7.0->qianfan==0.3.5->langflow)\n",
      "  Using cached yarl-1.9.4-cp312-cp312-win_amd64.whl.metadata (32 kB)\n",
      "Collecting SQLAlchemy>=1.3.0 (from alembic<2.0.0,>=1.13.0->langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Using cached SQLAlchemy-2.0.30-cp312-cp312-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting Mako (from alembic<2.0.0,>=1.13.0->langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from anthropic<1,>=0.26.0->langchain-anthropic<0.2.0,>=0.1.6->langflow)\n",
      "  Using cached anyio-4.3.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from anthropic<1,>=0.26.0->langchain-anthropic<0.2.0,>=0.1.6->langflow)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.1.0 (from anthropic<1,>=0.26.0->langchain-anthropic<0.2.0,>=0.1.6->langflow)\n",
      "  Downloading jiter-0.4.0-cp312-none-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting sniffio (from anthropic<1,>=0.26.0->langchain-anthropic<0.2.0,>=0.1.6->langflow)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting cassio<0.2.0,>=0.1.4 (from astrapy<0.8.0,>=0.7.7->langchain-astradb<0.2.0,>=0.1.0->langflow)\n",
      "  Downloading cassio-0.1.7-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting deprecation<2.2.0,>=2.1.0 (from astrapy<0.8.0,>=0.7.7->langchain-astradb<0.2.0,>=0.1.0->langflow)\n",
      "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting toml<0.11.0,>=0.10.2 (from astrapy<0.8.0,>=0.7.7->langchain-astradb<0.2.0,>=0.1.0->langflow)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pycryptodome>=3.8.0 (from bce-python-sdk>=0.8.79->qianfan==0.3.5->langflow)\n",
      "  Downloading pycryptodome-3.20.0-cp35-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting future>=0.6.0 (from bce-python-sdk>=0.8.79->qianfan==0.3.5->langflow)\n",
      "  Using cached future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting six>=1.4.0 (from bce-python-sdk>=0.8.79->qianfan==0.3.5->langflow)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading pyproject_hooks-1.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting colorama (from build>=1.0.3->chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting cffi>=1.12 (from cryptography<43.0.0,>=42.0.5->langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Using cached cffi-1.16.0-cp312-cp312-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting pyarrow-hotfix (from datasets<3.0.0,~=2.14.6->dspy-ai<3.0.0,>=2.4.0->langflow)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets<3.0.0,~=2.14.6->dspy-ai<3.0.0,>=2.4.0->langflow)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting xxhash (from datasets<3.0.0,~=2.14.6->dspy-ai<3.0.0,>=2.4.0->langflow)\n",
      "  Downloading xxhash-3.4.1-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<0.21.0,>=0.20.0->huggingface-hub[inference]<0.21.0,>=0.20.0->langflow)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client<3.0.0,>=2.118.0->langflow)\n",
      "  Using cached googleapis_common_protos-1.63.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client<3.0.0,>=2.118.0->langflow)\n",
      "  Using cached protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client<3.0.0,>=2.118.0->langflow)\n",
      "  Downloading proto_plus-1.24.0.dev1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client<3.0.0,>=2.118.0->langflow)\n",
      "  Using cached pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client<3.0.0,>=2.118.0->langflow)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 (from google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai<2.0.0,>=1.0.3->langflow)\n",
      "  Downloading google_cloud_bigquery-3.23.1-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai<2.0.0,>=1.0.3->langflow)\n",
      "  Using cached google_cloud_resource_manager-1.12.3-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting shapely<3.0.0dev (from google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai<2.0.0,>=1.0.3->langflow)\n",
      "  Using cached shapely-2.0.4-cp312-cp312-win_amd64.whl.metadata (7.2 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0 (from google-cloud-storage<3.0.0,>=2.14.0->langchain-google-vertexai<2.0.0,>=1.0.3->langflow)\n",
      "  Using cached google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media>=2.6.0 (from google-cloud-storage<3.0.0,>=2.14.0->langchain-google-vertexai<2.0.0,>=1.0.3->langflow)\n",
      "  Using cached google_resumable_media-2.7.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage<3.0.0,>=2.14.0->langchain-google-vertexai<2.0.0,>=1.0.3->langflow)\n",
      "  Using cached google_crc32c-1.5.0-py3-none-any.whl\n",
      "Collecting google-ai-generativelanguage==0.6.4 (from google-generativeai<0.6.0,>=0.5.2->langchain-google-genai<2.0.0,>=1.0.1->langflow)\n",
      "  Downloading google_ai_generativelanguage-0.6.4-py3-none-any.whl.metadata (5.6 kB)\n",
      "INFO: pip is looking at multiple versions of grpcio-health-checking to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-health-checking<2.0.0,>=1.57.0 (from weaviate-client->langflow)\n",
      "  Downloading grpcio_health_checking-1.64.0rc1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_health_checking-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_health_checking-1.63.0rc2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_health_checking-1.63.0rc1-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: pip is looking at multiple versions of grpcio-tools to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-tools>=1.41.0 (from qdrant-client<2.0.0,>=1.7.0->langflow)\n",
      "  Downloading grpcio_tools-1.64.0rc1-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "  Downloading grpcio_tools-1.63.0-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "  Downloading grpcio_tools-1.63.0rc2-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "  Downloading grpcio_tools-1.63.0rc1-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting setuptools (from grpcio-tools>=1.41.0->qdrant-client<2.0.0,>=1.7.0->langflow)\n",
      "  Using cached setuptools-70.0.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.dev0,>=0.19.0->google-api-python-client<3.0.0,>=2.118.0->langflow)\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.19.0->assemblyai<0.24.0,>=0.23.1->langflow)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.19.0->assemblyai<0.24.0,>=0.23.1->langflow)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.7.0->langflow)\n",
      "  Downloading h2-4.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata>=6.8.0->litellm<2.0.0,>=1.34.22->langflow)\n",
      "  Downloading zipp-3.18.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain<0.2.0,>=0.1.16->langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Using cached dataclasses_json-0.6.6-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.38 (from langchain<0.2.0,>=0.1.16->langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain<0.2.0,>=0.1.16->langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain<0.2.0,>=0.1.16->langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading langsmith-0.1.62-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3,>=0.1.43->langchain-anthropic<0.2.0,>=0.1.6->langflow)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.38->llama-index<0.11.0,>=0.10.13->langflow)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.38->llama-index<0.11.0,>=0.10.13->langflow)\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llamaindex-py-client<0.2.0,>=0.1.18 (from llama-index-core<0.11.0,>=0.10.38->llama-index<0.11.0,>=0.10.13->langflow)\n",
      "  Using cached llamaindex_py_client-0.1.19-py3-none-any.whl.metadata (760 bytes)\n",
      "Collecting nltk<4.0.0,>=3.8.1 (from llama-index-core<0.11.0,>=0.10.38->llama-index<0.11.0,>=0.10.13->langflow)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.38->llama-index<0.11.0,>=0.10.13->langflow)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index<0.11.0,>=0.10.13->langflow)\n",
      "  Using cached striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse<0.5.0,>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index<0.11.0,>=0.10.13->langflow)\n",
      "  Using cached llama_parse-0.4.3-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting win32-setctime>=1.0.0 (from loguru<0.8.0,>=0.7.1->langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading win32_setctime-1.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from qianfan==0.3.5->langflow)\n",
      "  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading sympy-1.12.1rc1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting importlib-metadata>=6.8.0 (from litellm<2.0.0,>=1.34.22->langflow)\n",
      "  Downloading importlib_metadata-7.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting opentelemetry-proto==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading opentelemetry_proto-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-instrumentation==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-util-http==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting strenum<0.5.0,>=0.4.9 (from postgrest<0.17.0,>=0.14->supabase<3.0.0,>=2.3.0->langflow)\n",
      "  Using cached StrEnum-0.4.15-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wcwidth (from prompt-toolkit>=3.0.38->qianfan==0.3.5->langflow)\n",
      "  Using cached wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic->qianfan==0.3.5->langflow)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.18.2 (from pydantic->qianfan==0.3.5->langflow)\n",
      "  Downloading pydantic_core-2.18.2-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting lxml>=3.1.0 (from python-docx<2.0.0,>=1.1.0->langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading lxml-5.2.2-cp312-cp312-win_amd64.whl.metadata (3.5 kB)\n",
      "Collecting ecdsa!=0.15 (from python-jose<4.0.0,>=3.3.0->langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading ecdsa-0.19.0-py2.py3-none-any.whl.metadata (29 kB)\n",
      "Collecting pyasn1 (from python-jose<4.0.0,>=3.3.0->langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Using cached pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting bidict>=0.21.0 (from python-socketio<6.0.0,>=5.11.0->langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting python-engineio>=4.8.0 (from python-socketio<6.0.0,>=5.11.0->langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading python_engineio-4.9.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.24->qianfan==0.3.5->langflow)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=13.0.0->qianfan==0.3.5->langflow)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich>=13.0.0->qianfan==0.3.5->langflow)\n",
      "  Downloading pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting lark==1.1.8 (from RTFDE<0.2,>=0.1.1->extract-msg<0.48.0,>=0.47.0->langflow)\n",
      "  Downloading lark-1.1.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting oletools>=0.56 (from RTFDE<0.2,>=0.1.1->extract-msg<0.48.0,>=0.47.0->langflow)\n",
      "  Downloading oletools-0.60.1-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->qianfan==0.3.5->langflow)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading httptools-0.6.1-cp312-cp312-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading watchfiles-0.21.0-cp312-none-win_amd64.whl.metadata (5.0 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-experimental to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-experimental (from langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading langchain_experimental-0.0.58-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting colorlog (from optuna->dspy-ai<3.0.0,>=2.4.0->langflow)\n",
      "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting cassandra-driver<4.0.0,>=3.28.0 (from cassio<0.2.0,>=0.1.4->astrapy<0.8.0,>=0.7.7->langchain-astradb<0.2.0,>=0.1.0->langflow)\n",
      "  Downloading cassandra_driver-3.29.1-cp312-cp312-win_amd64.whl.metadata (6.0 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography<43.0.0,>=42.0.5->langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.16->langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Using cached marshmallow-3.21.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai<2.0.0,>=1.0.3->langflow)\n",
      "  Downloading grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4 (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai<2.0.0,>=1.0.3->langflow)\n",
      "  Using cached grpc_google_iam_v1-0.13.0-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.7.0->langflow)\n",
      "  Downloading hyperframe-6.0.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.7.0->langflow)\n",
      "  Downloading hpack-4.0.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.43->langchain-anthropic<0.2.0,>=0.1.6->langflow)\n",
      "  Using cached jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=13.0.0->qianfan==0.3.5->langflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.dev0,>=0.19.0->google-api-python-client<3.0.0,>=2.118.0->langflow)\n",
      "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting easygui (from oletools>=0.56->RTFDE<0.2,>=0.1.1->extract-msg<0.48.0,>=0.47.0->langflow)\n",
      "  Downloading easygui-0.98.3-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting colorclass (from oletools>=0.56->RTFDE<0.2,>=0.1.1->extract-msg<0.48.0,>=0.47.0->langflow)\n",
      "  Downloading colorclass-2.2.2-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pcodedmp>=1.2.5 (from oletools>=0.56->RTFDE<0.2,>=0.1.1->extract-msg<0.48.0,>=0.47.0->langflow)\n",
      "  Downloading pcodedmp-1.2.6-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting msoffcrypto-tool (from oletools>=0.56->RTFDE<0.2,>=0.1.1->extract-msg<0.48.0,>=0.47.0->langflow)\n",
      "  Downloading msoffcrypto_tool-5.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting simple-websocket>=0.10.0 (from python-engineio>=4.8.0->python-socketio<6.0.0,>=5.11.0->langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading simple_websocket-1.0.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy>=1.3.0->alembic<2.0.0,>=1.13.0->langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Using cached greenlet-3.0.3-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.38->llama-index<0.11.0,>=0.10.13->langflow)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting mpmath<1.4.0,>=1.1.0 (from sympy->onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting geomet<0.3,>=0.1 (from cassandra-driver<4.0.0,>=3.28.0->cassio<0.2.0,>=0.1.4->astrapy<0.8.0,>=0.7.7->langchain-astradb<0.2.0,>=0.1.0->langflow)\n",
      "  Downloading geomet-0.2.1.post1-py3-none-any.whl.metadata (1.0 kB)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai<2.0.0,>=1.0.3->langflow)\n",
      "  Downloading grpcio_status-1.64.0rc1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.63.0rc2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.63.0rc1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.24->langflow)\n",
      "  Downloading pyreadline3-3.4.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting win-unicode-console (from pcodedmp>=1.2.5->oletools>=0.56->RTFDE<0.2,>=0.1.1->extract-msg<0.48.0,>=0.47.0->langflow)\n",
      "  Downloading win_unicode_console-0.5.zip (31 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting wsproto (from simple-websocket>=0.10.0->python-engineio>=4.8.0->python-socketio<6.0.0,>=5.11.0->langflow-base<0.0.48,>=0.0.47->langflow)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Downloading langflow-1.0.0a36-py3-none-any.whl (6.3 kB)\n",
      "Downloading qianfan-0.3.5-py3-none-any.whl (304 kB)\n",
      "   ---------------------------------------- 0.0/304.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 304.8/304.8 kB 19.6 MB/s eta 0:00:00\n",
      "Downloading assemblyai-0.23.1-py3-none-any.whl (66 kB)\n",
      "   ---------------------------------------- 0.0/66.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 66.2/66.2 kB ? eta 0:00:00\n",
      "Using cached boto3-1.34.111-py3-none-any.whl (139 kB)\n",
      "Downloading certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "   ---------------------------------------- 0.0/162.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 162.5/162.5 kB 10.2 MB/s eta 0:00:00\n",
      "Downloading chromadb-0.4.24-py3-none-any.whl (525 kB)\n",
      "   ---------------------------------------- 0.0/525.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 525.5/525.5 kB 32.2 MB/s eta 0:00:00\n",
      "Using cached cohere-5.5.1-py3-none-any.whl (166 kB)\n",
      "Downloading dspy_ai-2.4.9-py3-none-any.whl (220 kB)\n",
      "   ---------------------------------------- 0.0/220.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 220.4/220.4 kB 13.1 MB/s eta 0:00:00\n",
      "Downloading elasticsearch-8.13.1-py3-none-any.whl (477 kB)\n",
      "   ---------------------------------------- 0.0/477.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 477.5/477.5 kB 29.2 MB/s eta 0:00:00\n",
      "Downloading extract_msg-0.47.0-py2.py3-none-any.whl (328 kB)\n",
      "   ---------------------------------------- 0.0/328.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 328.2/328.2 kB ? eta 0:00:00\n",
      "Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
      "   ---------------------------------------- 0.0/114.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 114.6/114.6 kB ? eta 0:00:00\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Downloading faiss_cpu-1.8.0-cp312-cp312-win_amd64.whl (14.5 MB)\n",
      "   ---------------------------------------- 0.0/14.5 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.9/14.5 MB 61.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.9/14.5 MB 41.3 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 5.8/14.5 MB 41.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 7.7/14.5 MB 41.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 9.6/14.5 MB 40.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 11.5/14.5 MB 38.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 13.3/14.5 MB 38.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.5/14.5 MB 36.4 MB/s eta 0:00:00\n",
      "Downloading fake_useragent-1.5.1-py3-none-any.whl (17 kB)\n",
      "Using cached fastavro-1.9.4-cp312-cp312-win_amd64.whl (487 kB)\n",
      "Downloading google_api_python_client-2.130.0-py2.py3-none-any.whl (11.7 MB)\n",
      "   ---------------------------------------- 0.0/11.7 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.6/11.7 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.5/11.7 MB 37.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.4/11.7 MB 38.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.2/11.7 MB 38.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.2/11.7 MB 39.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.1/11.7 MB 40.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.7/11.7 MB 38.4 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "   ---------------------------------------- 0.0/330.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 330.1/330.1 kB 20.0 MB/s eta 0:00:00\n",
      "Downloading langchain_anthropic-0.1.13-py3-none-any.whl (16 kB)\n",
      "Downloading langchain_astradb-0.1.0-py3-none-any.whl (25 kB)\n",
      "Using cached langchain_cohere-0.1.5-py3-none-any.whl (30 kB)\n",
      "Downloading langchain_google_genai-1.0.4-py3-none-any.whl (33 kB)\n",
      "Downloading langchain_google_vertexai-1.0.4-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.7/57.7 kB ? eta 0:00:00\n",
      "Downloading langchain_groq-0.1.4-py3-none-any.whl (11 kB)\n",
      "Downloading langchain_mistralai-0.1.7-py3-none-any.whl (12 kB)\n",
      "Downloading langchain_openai-0.1.7-py3-none-any.whl (34 kB)\n",
      "Downloading langchain_pinecone-0.1.1-py3-none-any.whl (8.4 kB)\n",
      "Downloading langflow_base-0.0.47-py3-none-any.whl (4.9 MB)\n",
      "   ---------------------------------------- 0.0/4.9 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 1.5/4.9 MB 47.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 3.4/4.9 MB 43.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.9/4.9 MB 39.0 MB/s eta 0:00:00\n",
      "Downloading bcrypt-4.0.1-cp36-abi3-win_amd64.whl (152 kB)\n",
      "   ---------------------------------------- 0.0/152.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 152.9/152.9 kB 8.9 MB/s eta 0:00:00\n",
      "Downloading orjson-3.10.0-cp312-none-win_amd64.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 139.4/139.4 kB 8.1 MB/s eta 0:00:00\n",
      "Downloading pandas-2.2.0-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.7/11.5 MB 37.2 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 2.1/11.5 MB 33.5 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 2.1/11.5 MB 33.5 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 2.1/11.5 MB 33.5 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 2.1/11.5 MB 33.5 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.1/11.5 MB 10.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.9/11.5 MB 15.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.8/11.5 MB 18.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.7/11.5 MB 20.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.6/11.5 MB 22.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 21.8 MB/s eta 0:00:00\n",
      "Downloading langfuse-2.33.1a0-py3-none-any.whl (162 kB)\n",
      "   ---------------------------------------- 0.0/162.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 162.5/162.5 kB ? eta 0:00:00\n",
      "Downloading litellm-1.38.0-py3-none-any.whl (4.4 MB)\n",
      "   ---------------------------------------- 0.0/4.4 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 1.6/4.4 MB 51.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.5/4.4 MB 44.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.4/4.4 MB 40.7 MB/s eta 0:00:00\n",
      "Downloading llama_index-0.10.38-py3-none-any.whl (6.8 kB)\n",
      "Using cached MarkupSafe-2.1.5-cp312-cp312-win_amd64.whl (17 kB)\n",
      "Downloading metal_sdk-2.5.1-py3-none-any.whl (14 kB)\n",
      "Downloading metaphor_python-0.1.23-py3-none-any.whl (6.1 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Downloading numexpr-2.10.0-cp312-cp312-win_amd64.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.1/97.1 kB 5.8 MB/s eta 0:00:00\n",
      "Downloading pgvector-0.2.5-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached pinecone_client-3.2.2-py3-none-any.whl (215 kB)\n",
      "Downloading psycopg-3.1.19-py3-none-any.whl (179 kB)\n",
      "   ---------------------------------------- 0.0/179.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 179.4/179.4 kB ? eta 0:00:00\n",
      "Downloading psycopg_binary-3.1.19-cp312-cp312-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 1.9/2.9 MB 40.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 36.8 MB/s eta 0:00:00\n",
      "Downloading psycopg2_binary-2.9.9-cp312-cp312-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 36.1 MB/s eta 0:00:00\n",
      "Downloading pyarrow-14.0.2-cp312-cp312-win_amd64.whl (25.0 MB)\n",
      "   ---------------------------------------- 0.0/25.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.9/25.0 MB 40.3 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 3.9/25.0 MB 40.9 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 5.7/25.0 MB 40.3 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 7.6/25.0 MB 40.6 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 9.6/25.0 MB 40.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 11.5/25.0 MB 40.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 13.4/25.0 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 15.3/25.0 MB 40.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 17.1/25.0 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 18.9/25.0 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.8/25.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.0/25.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.7/25.0 MB 36.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.0/25.0 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.0/25.0 MB 32.8 MB/s eta 0:00:00\n",
      "Downloading pyautogen-0.2.27-py3-none-any.whl (273 kB)\n",
      "   ---------------------------------------- 0.0/273.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 273.8/273.8 kB 16.5 MB/s eta 0:00:00\n",
      "Downloading pymongo-4.7.2-cp312-cp312-win_amd64.whl (485 kB)\n",
      "   ---------------------------------------- 0.0/485.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 485.1/485.1 kB 31.7 MB/s eta 0:00:00\n",
      "Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.6/57.6 kB ? eta 0:00:00\n",
      "Using cached pywin32-306-cp312-cp312-win_amd64.whl (9.2 MB)\n",
      "Downloading qdrant_client-1.9.1-py3-none-any.whl (229 kB)\n",
      "   ---------------------------------------- 0.0/229.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 229.3/229.3 kB 13.7 MB/s eta 0:00:00\n",
      "Downloading supabase-2.4.6-py3-none-any.whl (15 kB)\n",
      "Downloading types_cachetools-5.3.0.7-py3-none-any.whl (3.8 kB)\n",
      "Downloading zep_python-2.0.0rc6-py3-none-any.whl (34 kB)\n",
      "Downloading weaviate_client-4.7.0a0-py3-none-any.whl (325 kB)\n",
      "   ---------------------------------------- 0.0/325.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 325.2/325.2 kB ? eta 0:00:00\n",
      "Downloading validators-0.28.1-py3-none-any.whl (39 kB)\n",
      "Using cached aiohttp-3.9.5-cp312-cp312-win_amd64.whl (369 kB)\n",
      "Downloading aiolimiter-1.1.0-py3-none-any.whl (7.2 kB)\n",
      "Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
      "   ---------------------------------------- 0.0/233.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 233.4/233.4 kB ? eta 0:00:00\n",
      "Downloading anthropic-0.26.1-py3-none-any.whl (877 kB)\n",
      "   ---------------------------------------- 0.0/877.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 877.6/877.6 kB 54.2 MB/s eta 0:00:00\n",
      "Downloading astrapy-0.7.7-py3-none-any.whl (32 kB)\n",
      "Downloading asyncer-0.0.5-py3-none-any.whl (8.4 kB)\n",
      "Downloading Authlib-1.3.0-py2.py3-none-any.whl (223 kB)\n",
      "   ---------------------------------------- 0.0/223.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 223.7/223.7 kB ? eta 0:00:00\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading bce_python_sdk-0.9.10-py3-none-any.whl (342 kB)\n",
      "   ---------------------------------------- 0.0/342.3 kB ? eta -:--:--\n",
      "   --------------------------------------  337.9/342.3 kB 20.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 342.3/342.3 kB 10.4 MB/s eta 0:00:00\n",
      "Using cached botocore-1.34.111-py3-none-any.whl (12.3 MB)\n",
      "Downloading build-1.2.1-py3-none-any.whl (21 kB)\n",
      "Using cached cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Using cached cryptography-42.0.7-cp39-abi3-win_amd64.whl (2.9 MB)\n",
      "Downloading datasets-2.14.7-py3-none-any.whl (520 kB)\n",
      "   ---------------------------------------- 0.0/520.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 520.4/520.4 kB 31.9 MB/s eta 0:00:00\n",
      "Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "   ---------------------------------------- 0.0/307.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 307.7/307.7 kB ? eta 0:00:00\n",
      "Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Downloading duckdb-0.10.4.dev2-cp312-cp312-win_amd64.whl (9.9 MB)\n",
      "   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.8/9.9 MB 56.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.7/9.9 MB 47.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.6/9.9 MB 40.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.5/9.9 MB 40.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.3/9.9 MB 39.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.9/9.9 MB 39.7 MB/s eta 0:00:00\n",
      "Downloading ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n",
      "   ---------------------------------------- 0.0/128.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 128.5/128.5 kB ? eta 0:00:00\n",
      "Downloading elastic_transport-8.13.0-py3-none-any.whl (64 kB)\n",
      "   ---------------------------------------- 0.0/64.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 64.3/64.3 kB ? eta 0:00:00\n",
      "Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
      "   ---------------------------------------- 0.0/431.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 431.4/431.4 kB ? eta 0:00:00\n",
      "Downloading fastapi-0.110.3-py3-none-any.whl (91 kB)\n",
      "   ---------------------------------------- 0.0/91.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 91.8/91.8 kB ? eta 0:00:00\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "   ---------------------------------------- 0.0/166.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 166.4/166.4 kB ? eta 0:00:00\n",
      "Downloading google_api_core-2.19.0-py3-none-any.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 139.0/139.0 kB ? eta 0:00:00\n",
      "Using cached google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading google_cloud_aiplatform-1.52.0-py2.py3-none-any.whl (5.0 MB)\n",
      "   ---------------------------------------- 0.0/5.0 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 1.5/5.0 MB 47.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.4/5.0 MB 44.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.0/5.0 MB 45.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.0/5.0 MB 40.1 MB/s eta 0:00:00\n",
      "Using cached google_cloud_storage-2.16.0-py2.py3-none-any.whl (125 kB)\n",
      "Downloading google_generativeai-0.5.4-py3-none-any.whl (150 kB)\n",
      "   ---------------------------------------- 0.0/150.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 150.7/150.7 kB 8.8 MB/s eta 0:00:00\n",
      "Downloading google_ai_generativelanguage-0.6.4-py3-none-any.whl (679 kB)\n",
      "   ---------------------------------------- 0.0/679.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 679.1/679.1 kB 41.8 MB/s eta 0:00:00\n",
      "Downloading gotrue-2.4.2-py3-none-any.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.3/44.3 kB ? eta 0:00:00\n",
      "Downloading groq-0.8.0-py3-none-any.whl (105 kB)\n",
      "   ---------------------------------------- 0.0/105.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 105.4/105.4 kB ? eta 0:00:00\n",
      "Downloading grpcio-1.64.0-cp312-cp312-win_amd64.whl (4.1 MB)\n",
      "   ---------------------------------------- 0.0/4.1 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 1.9/4.1 MB 40.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.7/4.1 MB 40.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.1/4.1 MB 37.3 MB/s eta 0:00:00\n",
      "Downloading grpcio_health_checking-1.63.0rc1-py3-none-any.whl (18 kB)\n",
      "Downloading grpcio_tools-1.63.0rc1-cp312-cp312-win_amd64.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.0/1.1 MB 33.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.0/1.1 MB 33.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.0/1.1 MB 33.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.0/1.1 MB 33.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.0/1.1 MB 33.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 4.2 MB/s eta 0:00:00\n",
      "Downloading gunicorn-22.0.0-py3-none-any.whl (84 kB)\n",
      "   ---------------------------------------- 0.0/84.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 84.4/84.4 kB ? eta 0:00:00\n",
      "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 133.3/133.3 kB ? eta 0:00:00\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Downloading kubernetes-30.1.0a1-py2.py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 1.5/1.7 MB 48.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 36.1 MB/s eta 0:00:00\n",
      "Downloading langchain-0.1.20-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 32.4 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
      "   ---------------------------------------- 0.0/302.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 302.9/302.9 kB 18.3 MB/s eta 0:00:00\n",
      "Downloading langchainhub-0.1.15-py3-none-any.whl (4.6 kB)\n",
      "Using cached llama_index_agent_openai-0.2.5-py3-none-any.whl (13 kB)\n",
      "Using cached llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\n",
      "Downloading llama_index_core-0.10.38.post2-py3-none-any.whl (15.4 MB)\n",
      "   ---------------------------------------- 0.0/15.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.7/15.4 MB 51.8 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 3.6/15.4 MB 46.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 5.5/15.4 MB 43.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 7.4/15.4 MB 43.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 9.3/15.4 MB 42.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.2/15.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.1/15.4 MB 40.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.0/15.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.4/15.4 MB 38.4 MB/s eta 0:00:00\n",
      "Downloading llama_index_embeddings_openai-0.1.10-py3-none-any.whl (6.2 kB)\n",
      "Using cached llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl (6.7 kB)\n",
      "Using cached llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
      "Downloading llama_index_llms_openai-0.1.20-py3-none-any.whl (11 kB)\n",
      "Using cached llama_index_multi_modal_llms_openai-0.1.6-py3-none-any.whl (5.8 kB)\n",
      "Using cached llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
      "Using cached llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
      "Using cached llama_index_readers_file-0.1.22-py3-none-any.whl (36 kB)\n",
      "Using cached llama_index_readers_llama_parse-0.1.4-py3-none-any.whl (2.5 kB)\n",
      "Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 0.0/62.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 62.5/62.5 kB ? eta 0:00:00\n",
      "Downloading mmh3-4.1.0-cp312-cp312-win_amd64.whl (31 kB)\n",
      "Downloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
      "   ---------------------------------------- 0.0/135.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 135.4/135.4 kB 7.8 MB/s eta 0:00:00\n",
      "Using cached nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Downloading onnxruntime-1.18.0-cp312-cp312-win_amd64.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 2.0/5.6 MB 43.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.0/5.6 MB 42.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 40.0 MB/s eta 0:00:00\n",
      "Using cached openai-1.30.1-py3-none-any.whl (320 kB)\n",
      "Downloading opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n",
      "   ---------------------------------------- 0.0/60.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 60.1/60.1 kB ? eta 0:00:00\n",
      "Downloading importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl (17 kB)\n",
      "Downloading opentelemetry_proto-1.24.0-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.8/50.8 kB ? eta 0:00:00\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl (11 kB)\n",
      "Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl (28 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl (14 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl (36 kB)\n",
      "Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl (6.9 kB)\n",
      "Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n",
      "   ---------------------------------------- 0.0/106.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 106.1/106.1 kB ? eta 0:00:00\n",
      "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Downloading passlib-1.7.4-py2.py3-none-any.whl (525 kB)\n",
      "   ---------------------------------------- 0.0/525.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 525.6/525.6 kB 34.4 MB/s eta 0:00:00\n",
      "Using cached pillow-10.3.0-cp312-cp312-win_amd64.whl (2.5 MB)\n",
      "Downloading platformdirs-4.2.2-py3-none-any.whl (18 kB)\n",
      "Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Downloading postgrest-0.16.4-py3-none-any.whl (20 kB)\n",
      "Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
      "   ---------------------------------------- 0.0/41.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 41.3/41.3 kB ? eta 0:00:00\n",
      "Using cached prompt_toolkit-3.0.43-py3-none-any.whl (386 kB)\n",
      "Downloading pulsar_client-3.5.0-cp312-cp312-win_amd64.whl (3.3 MB)\n",
      "   ---------------------------------------- 0.0/3.3 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 1.6/3.3 MB 34.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.3/3.3 MB 35.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.3/3.3 MB 35.1 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
      "   ---------------------------------------- 0.0/409.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 409.3/409.3 kB ? eta 0:00:00\n",
      "Downloading pydantic_core-2.18.2-cp312-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.9/1.9 MB 40.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 40.6 MB/s eta 0:00:00\n",
      "Downloading pydantic_settings-2.2.1-py3-none-any.whl (13 kB)\n",
      "Using cached pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "   ---------------------------------------- 0.0/244.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 244.3/244.3 kB ? eta 0:00:00\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading python_jose-3.3.0-py2.py3-none-any.whl (33 kB)\n",
      "Downloading python_multipart-0.0.7-py3-none-any.whl (22 kB)\n",
      "Downloading python_socketio-5.11.2-py3-none-any.whl (75 kB)\n",
      "   ---------------------------------------- 0.0/75.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 75.7/75.7 kB ? eta 0:00:00\n",
      "Using cached PyYAML-6.0.1-cp312-cp312-win_amd64.whl (138 kB)\n",
      "Downloading realtime-1.0.4-py3-none-any.whl (8.9 kB)\n",
      "Downloading requests-2.32.2-py3-none-any.whl (63 kB)\n",
      "   ---------------------------------------- 0.0/63.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 63.9/63.9 kB ? eta 0:00:00\n",
      "Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "   ---------------------------------------- 0.0/240.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 240.7/240.7 kB 15.4 MB/s eta 0:00:00\n",
      "Downloading RTFDE-0.1.1-py3-none-any.whl (36 kB)\n",
      "Downloading lark-1.1.8-py3-none-any.whl (111 kB)\n",
      "   ---------------------------------------- 0.0/111.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 111.6/111.6 kB 6.3 MB/s eta 0:00:00\n",
      "Using cached s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
      "Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Downloading sqlmodel-0.0.16-py3-none-any.whl (26 kB)\n",
      "Downloading storage3-0.7.4-py3-none-any.whl (15 kB)\n",
      "Downloading supafunc-0.4.5-py3-none-any.whl (6.1 kB)\n",
      "Downloading tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
      "Downloading tiktoken-0.7.0-cp312-cp312-win_amd64.whl (799 kB)\n",
      "   ---------------------------------------- 0.0/799.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 799.3/799.3 kB 49.3 MB/s eta 0:00:00\n",
      "Downloading regex-2024.5.15-cp312-cp312-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 268.5/268.5 kB 16.1 MB/s eta 0:00:00\n",
      "Using cached tokenizers-0.19.1-cp312-none-win_amd64.whl (2.2 MB)\n",
      "Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB ? eta 0:00:00\n",
      "Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/47.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 47.2/47.2 kB ? eta 0:00:00\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading types_requests-2.32.0.20240523-py3-none-any.whl (15 kB)\n",
      "Downloading typing_extensions-4.12.0rc1-py3-none-any.whl (37 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
      "   ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 60.8/60.8 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading websockets-12.0-cp312-cp312-win_amd64.whl (124 kB)\n",
      "   ---------------------------------------- 0.0/125.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 125.0/125.0 kB 7.2 MB/s eta 0:00:00\n",
      "Using cached wrapt-1.16.0-cp312-cp312-win_amd64.whl (37 kB)\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "   ---------------------------------------- 0.0/199.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 199.4/199.4 kB ? eta 0:00:00\n",
      "Downloading clevercsv-0.8.2-cp312-cp312-win_amd64.whl (82 kB)\n",
      "   ---------------------------------------- 0.0/82.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 82.4/82.4 kB ? eta 0:00:00\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.5/45.5 kB ? eta 0:00:00\n",
      "Downloading docker-7.0.0-py3-none-any.whl (147 kB)\n",
      "   ---------------------------------------- 0.0/147.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 147.6/147.6 kB 9.2 MB/s eta 0:00:00\n",
      "Downloading filelock-3.14.0-py3-none-any.whl (12 kB)\n",
      "Downloading FLAML-2.1.2-py3-none-any.whl (296 kB)\n",
      "   ---------------------------------------- 0.0/296.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 296.7/296.7 kB 17.9 MB/s eta 0:00:00\n",
      "Downloading ijson-3.2.3-cp312-cp312-win_amd64.whl (48 kB)\n",
      "   ---------------------------------------- 0.0/48.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 48.5/48.5 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Downloading langchain_experimental-0.0.58-py3-none-any.whl (199 kB)\n",
      "   ---------------------------------------- 0.0/199.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 199.4/199.4 kB ? eta 0:00:00\n",
      "Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
      "   ---------------------------------------- 0.0/380.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 380.1/380.1 kB 24.7 MB/s eta 0:00:00\n",
      "Downloading structlog-24.1.0-py3-none-any.whl (65 kB)\n",
      "   ---------------------------------------- 0.0/65.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 65.7/65.7 kB 3.5 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading ujson-5.10.0-cp312-cp312-win_amd64.whl (42 kB)\n",
      "   ---------------------------------------- 0.0/42.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 42.2/42.2 kB ? eta 0:00:00\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Downloading bidict-0.23.1-py3-none-any.whl (32 kB)\n",
      "Downloading cassio-0.1.7-py3-none-any.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.9/44.9 kB ? eta 0:00:00\n",
      "Using cached cffi-1.16.0-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl (100 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Using cached dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 115.3/115.3 kB 6.6 MB/s eta 0:00:00\n",
      "Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading ecdsa-0.19.0-py2.py3-none-any.whl (149 kB)\n",
      "   ---------------------------------------- 0.0/149.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 149.3/149.3 kB 9.3 MB/s eta 0:00:00\n",
      "Using cached frozenlist-1.4.1-cp312-cp312-win_amd64.whl (50 kB)\n",
      "Using cached future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Downloading google_cloud_bigquery-3.23.1-py2.py3-none-any.whl (237 kB)\n",
      "   ---------------------------------------- 0.0/237.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 237.3/237.3 kB ? eta 0:00:00\n",
      "Using cached google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "Using cached google_cloud_resource_manager-1.12.3-py2.py3-none-any.whl (333 kB)\n",
      "Using cached google_resumable_media-2.7.0-py2.py3-none-any.whl (80 kB)\n",
      "Using cached googleapis_common_protos-1.63.0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.5/57.5 kB ? eta 0:00:00\n",
      "Downloading httptools-0.6.1-cp312-cp312-win_amd64.whl (55 kB)\n",
      "   ---------------------------------------- 0.0/55.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 55.7/55.7 kB 2.8 MB/s eta 0:00:00\n",
      "Downloading jiter-0.4.0-cp312-none-win_amd64.whl (200 kB)\n",
      "   ---------------------------------------- 0.0/200.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 200.1/200.1 kB 12.7 MB/s eta 0:00:00\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------- ----- 1.8/2.0 MB 56.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 42.9 MB/s eta 0:00:00\n",
      "Downloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
      "Downloading langsmith-0.1.62-py3-none-any.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.3/122.3 kB 7.0 MB/s eta 0:00:00\n",
      "Using cached llama_parse-0.4.3-py3-none-any.whl (7.7 kB)\n",
      "Using cached llamaindex_py_client-0.1.19-py3-none-any.whl (141 kB)\n",
      "Downloading lxml-5.2.2-cp312-cp312-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 2.1/3.8 MB 43.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.8/3.8 MB 40.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 34.9 MB/s eta 0:00:00\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "   ---------------------------------------- 0.0/87.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 87.5/87.5 kB 4.8 MB/s eta 0:00:00\n",
      "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached multidict-6.0.5-cp312-cp312-win_amd64.whl (27 kB)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading oletools-0.60.1-py2.py3-none-any.whl (977 kB)\n",
      "   ---------------------------------------- 0.0/977.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 977.2/977.2 kB 64.5 MB/s eta 0:00:00\n",
      "Downloading proto_plus-1.24.0.dev1-py3-none-any.whl (49 kB)\n",
      "   ---------------------------------------- 0.0/49.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 49.7/49.7 kB ? eta 0:00:00\n",
      "Using cached protobuf-4.25.3-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Using cached pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "Using cached pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "Downloading pycryptodome-3.20.0-cp35-abi3-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   --------------------------------- ------ 1.5/1.8 MB 47.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 37.2 MB/s eta 0:00:00\n",
      "Downloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 38.6 MB/s eta 0:00:00\n",
      "Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "   ---------------------------------------- 0.0/67.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 67.8/67.8 kB ? eta 0:00:00\n",
      "Downloading python_engineio-4.9.1-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.7/57.7 kB ? eta 0:00:00\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached setuptools-70.0.0-py3-none-any.whl (863 kB)\n",
      "Using cached shapely-2.0.4-cp312-cp312-win_amd64.whl (1.4 MB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached SQLAlchemy-2.0.30-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "   ---------------------------------------- 0.0/71.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 71.9/71.9 kB ? eta 0:00:00\n",
      "Using cached StrEnum-0.4.15-py3-none-any.whl (8.9 kB)\n",
      "Using cached striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading watchfiles-0.21.0-cp312-none-win_amd64.whl (280 kB)\n",
      "   ---------------------------------------- 0.0/280.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 280.3/280.3 kB 16.9 MB/s eta 0:00:00\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.8/58.8 kB ? eta 0:00:00\n",
      "Downloading win32_setctime-1.1.0-py3-none-any.whl (3.6 kB)\n",
      "Using cached yarl-1.9.4-cp312-cp312-win_amd64.whl (76 kB)\n",
      "Downloading zipp-3.18.2-py3-none-any.whl (8.3 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "   ---------------------------------------- 0.0/46.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 46.0/46.0 kB ? eta 0:00:00\n",
      "Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.6/78.6 kB ? eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading pyproject_hooks-1.1.0-py3-none-any.whl (9.2 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading sympy-1.12.1rc1-py3-none-any.whl (5.7 MB)\n",
      "   ---------------------------------------- 0.0/5.7 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 1.6/5.7 MB 34.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.6/5.7 MB 37.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.5/5.7 MB 38.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.7/5.7 MB 36.7 MB/s eta 0:00:00\n",
      "Using cached wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Downloading xxhash-3.4.1-cp312-cp312-win_amd64.whl (29 kB)\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading cassandra_driver-3.29.1-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 1.0/2.6 MB 33.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 1.0/2.6 MB 33.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 1.0/2.6 MB 33.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 1.0/2.6 MB 33.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 1.0/2.6 MB 33.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.5/2.6 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.1/2.6 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.1/2.6 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.1/2.6 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.1/2.6 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.1/2.6 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 4.9 MB/s eta 0:00:00\n",
      "Using cached greenlet-3.0.3-cp312-cp312-win_amd64.whl (293 kB)\n",
      "Using cached grpc_google_iam_v1-0.13.0-py2.py3-none-any.whl (25 kB)\n",
      "Downloading grpcio_status-1.63.0rc1-py3-none-any.whl (14 kB)\n",
      "Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "   ---------------------------------------- 0.0/86.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 86.8/86.8 kB ? eta 0:00:00\n",
      "Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
      "Using cached jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Using cached marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading pcodedmp-1.2.6-py2.py3-none-any.whl (30 kB)\n",
      "Downloading simple_websocket-1.0.0-py3-none-any.whl (13 kB)\n",
      "Downloading colorclass-2.2.2-py2.py3-none-any.whl (18 kB)\n",
      "Downloading easygui-0.98.3-py2.py3-none-any.whl (92 kB)\n",
      "   ---------------------------------------- 0.0/92.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 92.7/92.7 kB 5.5 MB/s eta 0:00:00\n",
      "Downloading msoffcrypto_tool-5.4.0-py3-none-any.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/48.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 48.0/48.0 kB 2.4 MB/s eta 0:00:00\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Downloading geomet-0.2.1.post1-py3-none-any.whl (18 kB)\n",
      "Downloading pyreadline3-3.4.1-py3-none-any.whl (95 kB)\n",
      "   ---------------------------------------- 0.0/95.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 95.2/95.2 kB 5.7 MB/s eta 0:00:00\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: chroma-hnswlib, red-black-tree-mod, google-search-results, html2text, pysrt, compressed-rtf, pypika, win-unicode-console\n",
      "  Building wheel for chroma-hnswlib (pyproject.toml): started\n",
      "  Building wheel for chroma-hnswlib (pyproject.toml): finished with status 'error'\n",
      "  Building wheel for red-black-tree-mod (pyproject.toml): started\n",
      "  Building wheel for red-black-tree-mod (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for red-black-tree-mod: filename=red_black_tree_mod-1.20-py3-none-any.whl size=18630 sha256=d1a4285df4ced984f273368d3b5b66a5a6f727ced5167ca94c635697b1249a15\n",
      "  Stored in directory: c:\\users\\shaik\\appdata\\local\\pip\\cache\\wheels\\19\\c1\\bc\\0f4e67ece48705c00f80d47e6db5edcc78936da18909ea9c07\n",
      "  Building wheel for google-search-results (pyproject.toml): started\n",
      "  Building wheel for google-search-results (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32084 sha256=6060d4cd8a24decd7392e8d215e82c56cf8b4b3985a3673c73033983b4a32cec\n",
      "  Stored in directory: c:\\users\\shaik\\appdata\\local\\pip\\cache\\wheels\\0c\\47\\f5\\89b7e770ab2996baf8c910e7353d6391e373075a0ac213519e\n",
      "  Building wheel for html2text (pyproject.toml): started\n",
      "  Building wheel for html2text (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for html2text: filename=html2text-2024.2.26-py3-none-any.whl size=33131 sha256=6572714acdead9372160f8d83fbcccb53bcb74e5be3a4938d3385165eed9fc93\n",
      "  Stored in directory: c:\\users\\shaik\\appdata\\local\\pip\\cache\\wheels\\2b\\01\\23\\578505d65e2a97d78bf1fe3fc8256ecf37572dc1df598b0eaf\n",
      "  Building wheel for pysrt (pyproject.toml): started\n",
      "  Building wheel for pysrt (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pysrt: filename=pysrt-1.1.2-py3-none-any.whl size=13463 sha256=e6b8edad84ffab93d637e1b9685560a3022e14d1bb54ec37803b3f08741b3e31\n",
      "  Stored in directory: c:\\users\\shaik\\appdata\\local\\pip\\cache\\wheels\\6a\\36\\54\\2aa8dc961885dfa7b0ebd45a57505f25039d79b4ea0fd9f29d\n",
      "  Building wheel for compressed-rtf (pyproject.toml): started\n",
      "  Building wheel for compressed-rtf (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6194 sha256=5ad988f308001b5efe4567a0c5a389d8f3753f0b1aab7e9c8adf76fe3b5b928d\n",
      "  Stored in directory: c:\\users\\shaik\\appdata\\local\\pip\\cache\\wheels\\c8\\7b\\c1\\a697196f29903a8e064f12afe97da95800b4b2c02e0d52568b\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53835 sha256=a9f203a81eefbe45eb484b0b63d7d3101fd84cea9a38fc5a2d2012739c4aa0b0\n",
      "  Stored in directory: c:\\users\\shaik\\appdata\\local\\pip\\cache\\wheels\\d5\\3d\\69\\8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
      "  Building wheel for win-unicode-console (pyproject.toml): started\n",
      "  Building wheel for win-unicode-console (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for win-unicode-console: filename=win_unicode_console-0.5-py3-none-any.whl size=20194 sha256=dd835c3acc3a280bbf25d82fa940697c3410f45a64a751ec610bd372e69a603e\n",
      "  Stored in directory: c:\\users\\shaik\\appdata\\local\\pip\\cache\\wheels\\1f\\96\\b5\\b825aaf74347c04c7eda00ce28240279dc1b6582a7625dca56\n",
      "Successfully built red-black-tree-mod google-search-results html2text pysrt compressed-rtf pypika win-unicode-console\n",
      "Failed to build chroma-hnswlib\n"
     ]
    }
   ],
   "source": [
    "pip install langflow --pre --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m pip install langflow -U"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
